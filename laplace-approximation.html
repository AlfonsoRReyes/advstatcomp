<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Advanced Statistical Computing</title>
  <meta name="description" content="The book covers material taught in the Johns Hopkins Biostatistics Advanced Statistical Computing course.">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="Advanced Statistical Computing" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://github.com/rdpeng/advstatcomp" />
  <meta property="og:image" content="https://github.com/rdpeng/advstatcompcover-image.png" />
  <meta property="og:description" content="The book covers material taught in the Johns Hopkins Biostatistics Advanced Statistical Computing course." />
  <meta name="github-repo" content="rdpeng/advstatcomp" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Advanced Statistical Computing" />
  
  <meta name="twitter:description" content="The book covers material taught in the Johns Hopkins Biostatistics Advanced Statistical Computing course." />
  <meta name="twitter:image" content="https://github.com/rdpeng/advstatcompcover-image.png" />

<meta name="author" content="Roger D. Peng">


<meta name="date" content="2017-12-12">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="integration.html">
<link rel="next" href="variational-inference.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="css\style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Advanced Statistical Computing</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a><ul>
<li class="chapter" data-level="" data-path="stay-in-touch.html"><a href="stay-in-touch.html"><i class="fa fa-check"></i>Stay in Touch!</a></li>
<li class="chapter" data-level="" data-path="setup.html"><a href="setup.html"><i class="fa fa-check"></i>Setup</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="example-linear-models.html"><a href="example-linear-models.html"><i class="fa fa-check"></i><b>1.1</b> Example: Linear Models</a></li>
<li class="chapter" data-level="1.2" data-path="principle-of-optimization-transfer.html"><a href="principle-of-optimization-transfer.html"><i class="fa fa-check"></i><b>1.2</b> Principle of Optimization Transfer</a></li>
<li class="chapter" data-level="1.3" data-path="textbooks-vs-computers.html"><a href="textbooks-vs-computers.html"><i class="fa fa-check"></i><b>1.3</b> Textbooks vs. Computers</a><ul>
<li class="chapter" data-level="1.3.1" data-path="textbooks-vs-computers.html"><a href="textbooks-vs-computers.html#using-logarithms"><i class="fa fa-check"></i><b>1.3.1</b> Using Logarithms</a></li>
<li class="chapter" data-level="1.3.2" data-path="textbooks-vs-computers.html"><a href="textbooks-vs-computers.html#linear-regression"><i class="fa fa-check"></i><b>1.3.2</b> Linear Regression</a></li>
<li class="chapter" data-level="1.3.3" data-path="textbooks-vs-computers.html"><a href="textbooks-vs-computers.html#multivariate-normal-distribution"><i class="fa fa-check"></i><b>1.3.3</b> Multivariate Normal Distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="solving-nonlinear-equations.html"><a href="solving-nonlinear-equations.html"><i class="fa fa-check"></i><b>2</b> Solving Nonlinear Equations</a><ul>
<li class="chapter" data-level="2.1" data-path="bisection-algorithm.html"><a href="bisection-algorithm.html"><i class="fa fa-check"></i><b>2.1</b> Bisection Algorithm</a><ul>
<li class="chapter" data-level="2.1.1" data-path="bisection-algorithm.html"><a href="bisection-algorithm.html#example-quantiles"><i class="fa fa-check"></i><b>2.1.1</b> Example: Quantiles</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="rates-of-convergence.html"><a href="rates-of-convergence.html"><i class="fa fa-check"></i><b>2.2</b> Rates of Convergence</a><ul>
<li class="chapter" data-level="2.2.1" data-path="rates-of-convergence.html"><a href="rates-of-convergence.html#linear-convergence"><i class="fa fa-check"></i><b>2.2.1</b> Linear convergence</a></li>
<li class="chapter" data-level="2.2.2" data-path="rates-of-convergence.html"><a href="rates-of-convergence.html#superlinear-convergence"><i class="fa fa-check"></i><b>2.2.2</b> Superlinear Convergence</a></li>
<li class="chapter" data-level="2.2.3" data-path="rates-of-convergence.html"><a href="rates-of-convergence.html#quadratic-convergence"><i class="fa fa-check"></i><b>2.2.3</b> Quadratic Convergence</a></li>
<li class="chapter" data-level="2.2.4" data-path="rates-of-convergence.html"><a href="rates-of-convergence.html#example-bisection-algorithm"><i class="fa fa-check"></i><b>2.2.4</b> Example: Bisection Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="functional-iteration.html"><a href="functional-iteration.html"><i class="fa fa-check"></i><b>2.3</b> Functional Iteration</a><ul>
<li class="chapter" data-level="2.3.1" data-path="functional-iteration.html"><a href="functional-iteration.html#the-shrinking-lemma"><i class="fa fa-check"></i><b>2.3.1</b> The Shrinking Lemma</a></li>
<li class="chapter" data-level="2.3.2" data-path="functional-iteration.html"><a href="functional-iteration.html#convergence-rates-for-shrinking-maps"><i class="fa fa-check"></i><b>2.3.2</b> Convergence Rates for Shrinking Maps</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="newtons-method.html"><a href="newtons-method.html"><i class="fa fa-check"></i><b>2.4</b> Newton’s Method</a><ul>
<li class="chapter" data-level="2.4.1" data-path="newtons-method.html"><a href="newtons-method.html#proof-of-newtons-method"><i class="fa fa-check"></i><b>2.4.1</b> Proof of Newton’s Method</a></li>
<li class="chapter" data-level="2.4.2" data-path="newtons-method.html"><a href="newtons-method.html#convergence-rate-of-newtons-method"><i class="fa fa-check"></i><b>2.4.2</b> Convergence Rate of Newton’s Method</a></li>
<li class="chapter" data-level="2.4.3" data-path="newtons-method.html"><a href="newtons-method.html#newtons-method-for-maximum-likelihood-estimation"><i class="fa fa-check"></i><b>2.4.3</b> Newton’s Method for Maximum Likelihood Estimation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="general-optimization.html"><a href="general-optimization.html"><i class="fa fa-check"></i><b>3</b> General Optimization</a><ul>
<li class="chapter" data-level="3.1" data-path="steepest-descent.html"><a href="steepest-descent.html"><i class="fa fa-check"></i><b>3.1</b> Steepest Descent</a><ul>
<li class="chapter" data-level="3.1.1" data-path="steepest-descent.html"><a href="steepest-descent.html#example-multivariate-normal"><i class="fa fa-check"></i><b>3.1.1</b> Example: Multivariate Normal</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="the-newton-direction.html"><a href="the-newton-direction.html"><i class="fa fa-check"></i><b>3.2</b> The Newton Direction</a><ul>
<li class="chapter" data-level="3.2.1" data-path="the-newton-direction.html"><a href="the-newton-direction.html#generalized-linear-models"><i class="fa fa-check"></i><b>3.2.1</b> Generalized Linear Models</a></li>
<li class="chapter" data-level="3.2.2" data-path="the-newton-direction.html"><a href="the-newton-direction.html#newtons-method-in-r"><i class="fa fa-check"></i><b>3.2.2</b> Newton’s Method in R</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="quasi-newton.html"><a href="quasi-newton.html"><i class="fa fa-check"></i><b>3.3</b> Quasi-Newton</a><ul>
<li class="chapter" data-level="3.3.1" data-path="quasi-newton.html"><a href="quasi-newton.html#quasi-newton-methods-in-r"><i class="fa fa-check"></i><b>3.3.1</b> Quasi-Newton Methods in R</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="conjugate-gradient.html"><a href="conjugate-gradient.html"><i class="fa fa-check"></i><b>3.4</b> Conjugate Gradient</a></li>
<li class="chapter" data-level="3.5" data-path="coordinate-descent.html"><a href="coordinate-descent.html"><i class="fa fa-check"></i><b>3.5</b> Coordinate Descent</a><ul>
<li class="chapter" data-level="3.5.1" data-path="coordinate-descent.html"><a href="coordinate-descent.html#convergence-rates"><i class="fa fa-check"></i><b>3.5.1</b> Convergence Rates</a></li>
<li class="chapter" data-level="3.5.2" data-path="coordinate-descent.html"><a href="coordinate-descent.html#generalized-additive-models"><i class="fa fa-check"></i><b>3.5.2</b> Generalized Additive Models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="the-em-algorithm.html"><a href="the-em-algorithm.html"><i class="fa fa-check"></i><b>4</b> The EM Algorithm</a><ul>
<li class="chapter" data-level="4.1" data-path="em-algorithm-for-exponential-families.html"><a href="em-algorithm-for-exponential-families.html"><i class="fa fa-check"></i><b>4.1</b> EM Algorithm for Exponential Families</a></li>
<li class="chapter" data-level="4.2" data-path="canonical-examples.html"><a href="canonical-examples.html"><i class="fa fa-check"></i><b>4.2</b> Canonical Examples</a><ul>
<li class="chapter" data-level="4.2.1" data-path="canonical-examples.html"><a href="canonical-examples.html#two-part-normal-mixture-model"><i class="fa fa-check"></i><b>4.2.1</b> Two-Part Normal Mixture Model</a></li>
<li class="chapter" data-level="4.2.2" data-path="canonical-examples.html"><a href="canonical-examples.html#censored-exponential-data"><i class="fa fa-check"></i><b>4.2.2</b> Censored Exponential Data</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="a-minorizing-function.html"><a href="a-minorizing-function.html"><i class="fa fa-check"></i><b>4.3</b> A Minorizing Function</a><ul>
<li class="chapter" data-level="4.3.1" data-path="a-minorizing-function.html"><a href="a-minorizing-function.html#example-minorization-in-a-two-part-mixture-model"><i class="fa fa-check"></i><b>4.3.1</b> Example: Minorization in a Two-Part Mixture Model</a></li>
<li class="chapter" data-level="4.3.2" data-path="a-minorizing-function.html"><a href="a-minorizing-function.html#constrained-minimization-with-and-adaptive-barrier"><i class="fa fa-check"></i><b>4.3.2</b> Constrained Minimization With and Adaptive Barrier</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="missing-information-principle.html"><a href="missing-information-principle.html"><i class="fa fa-check"></i><b>4.4</b> Missing Information Principle</a></li>
<li class="chapter" data-level="4.5" data-path="acceleration-methods.html"><a href="acceleration-methods.html"><i class="fa fa-check"></i><b>4.5</b> Acceleration Methods</a><ul>
<li class="chapter" data-level="4.5.1" data-path="acceleration-methods.html"><a href="acceleration-methods.html#louiss-acceleration"><i class="fa fa-check"></i><b>4.5.1</b> Louis’s Acceleration</a></li>
<li class="chapter" data-level="4.5.2" data-path="acceleration-methods.html"><a href="acceleration-methods.html#squarem"><i class="fa fa-check"></i><b>4.5.2</b> SQUAREM</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="integration.html"><a href="integration.html"><i class="fa fa-check"></i><b>5</b> Integration</a><ul>
<li class="chapter" data-level="5.1" data-path="laplace-approximation.html"><a href="laplace-approximation.html"><i class="fa fa-check"></i><b>5.1</b> Laplace Approximation</a><ul>
<li class="chapter" data-level="5.1.1" data-path="laplace-approximation.html"><a href="laplace-approximation.html#computing-the-posterior-mean"><i class="fa fa-check"></i><b>5.1.1</b> Computing the Posterior Mean</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="variational-inference.html"><a href="variational-inference.html"><i class="fa fa-check"></i><b>5.2</b> Variational Inference</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="independent-monte-carlo.html"><a href="independent-monte-carlo.html"><i class="fa fa-check"></i><b>6</b> Independent Monte Carlo</a><ul>
<li class="chapter" data-level="6.1" data-path="random-number-generation.html"><a href="random-number-generation.html"><i class="fa fa-check"></i><b>6.1</b> Random Number Generation</a><ul>
<li class="chapter" data-level="6.1.1" data-path="random-number-generation.html"><a href="random-number-generation.html#pseudo-random-numbers"><i class="fa fa-check"></i><b>6.1.1</b> Pseudo-random Numbers</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="non-uniform-random-numbers.html"><a href="non-uniform-random-numbers.html"><i class="fa fa-check"></i><b>6.2</b> Non-Uniform Random Numbers</a><ul>
<li class="chapter" data-level="6.2.1" data-path="non-uniform-random-numbers.html"><a href="non-uniform-random-numbers.html#inverse-cdf-transformation"><i class="fa fa-check"></i><b>6.2.1</b> Inverse CDF Transformation</a></li>
<li class="chapter" data-level="6.2.2" data-path="non-uniform-random-numbers.html"><a href="non-uniform-random-numbers.html#other-transformations"><i class="fa fa-check"></i><b>6.2.2</b> Other Transformations</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="rejection-sampling.html"><a href="rejection-sampling.html"><i class="fa fa-check"></i><b>6.3</b> Rejection Sampling</a><ul>
<li class="chapter" data-level="6.3.1" data-path="rejection-sampling.html"><a href="rejection-sampling.html#the-algorithm"><i class="fa fa-check"></i><b>6.3.1</b> The Algorithm</a></li>
<li class="chapter" data-level="6.3.2" data-path="rejection-sampling.html"><a href="rejection-sampling.html#properties-of-rejection-sampling"><i class="fa fa-check"></i><b>6.3.2</b> Properties of Rejection Sampling</a></li>
<li class="chapter" data-level="6.3.3" data-path="rejection-sampling.html"><a href="rejection-sampling.html#empirical-supremum-rejection-sampling"><i class="fa fa-check"></i><b>6.3.3</b> Empirical Supremum Rejection Sampling</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="importance-sampling.html"><a href="importance-sampling.html"><i class="fa fa-check"></i><b>6.4</b> Importance Sampling</a><ul>
<li class="chapter" data-level="6.4.1" data-path="importance-sampling.html"><a href="importance-sampling.html#example-bayesian-sensitivity-analysis"><i class="fa fa-check"></i><b>6.4.1</b> Example: Bayesian Sensitivity Analysis</a></li>
<li class="chapter" data-level="6.4.2" data-path="importance-sampling.html"><a href="importance-sampling.html#example-calculating-marginal-likelihoods"><i class="fa fa-check"></i><b>6.4.2</b> Example: Calculating Marginal Likelihoods</a></li>
<li class="chapter" data-level="6.4.3" data-path="importance-sampling.html"><a href="importance-sampling.html#properties-of-the-importance-sampling-estimator"><i class="fa fa-check"></i><b>6.4.3</b> Properties of the Importance Sampling Estimator</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html"><i class="fa fa-check"></i><b>7</b> Markov Chain Monte Carlo</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Advanced Statistical Computing</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="laplace-approximation" class="section level2">
<h2><span class="header-section-number">5.1</span> Laplace Approximation</h2>
<p>The first technique that we will discuss is Laplace approximation. This technique can be used for reasonably well behaved functions that have most of their mass concentrated in a small area of their domain. Technically, it works for functions that are in the class of <span class="math inline">\(\mathfrak{L}^2\)</span>, meaning that <span class="math display">\[
\int g(x)^2\,dx &lt; \infty
\]</span> Such a function generally has very rapidly decreasing tails so that in the far reaches of the domain we would not expect to see large spikes.</p>
<p>Imagine a function that looks as follows</p>
<p><img src="integration_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>We can see that this function has most of its mass concentrated around the point <span class="math inline">\(x_0\)</span> and that we could probably approximate the area under the function with something like a step function.</p>
<p><img src="integration_files/figure-html/unnamed-chunk-3-1.png" width="672" /> The benefit of using something like a step function is that the area under a step function is trivial to compute. If we could find a principled and automatic way to find that approximating step function, and it were easier than just directly computing the integral in the first place, then we could have an alternative to computing the integral. In other words, we could perhaps say that <span class="math display">\[
\int g(x)\,dx \approx g(x_0)\varepsilon
\]</span> for some small value of <span class="math inline">\(\varepsilon\)</span>.</p>
<p>In reality, we actually have some more sophisticated functions that we can use besides step functions, and that’s how the Laplace approximation works. <strong>The general idea is to take a well-behaved uni-modal function and approximate it with a Normal density function</strong>, which is a very well-understood quantity.</p>
<p>Suppose we have a function <span class="math inline">\(g(x)\in\mathfrak{L}^2\)</span> which achieves its maximum at <span class="math inline">\(x_0\)</span>. We want to compute <span class="math display">\[
\int_a^b g(x)\, dx.
\]</span></p>
<p>Let <span class="math inline">\(h(x) = \log g(x)\)</span> so that we have <span class="math display">\[
\int_a^b g(x)\,dx = \int_a^b \exp(h(x))\,dx
\]</span></p>
<p>From here we can take a Taylor series approximation of <span class="math inline">\(h(x)\)</span> around the point <span class="math inline">\(x_0\)</span> to give us</p>
<p><span class="math display">\[
\int_a^b \exp(h(x))\,dx
 \approx 
\int_a^b\exp\left(h(x_0) + h^\prime(x_0)(x-x_0) 
+ \frac{1}{2}h^{\prime\prime}(x_0)(x-x_0)^2\right)\,dx
\]</span></p>
<p>Because we assumed <span class="math inline">\(h(x)\)</span> achieves its maximum at <span class="math inline">\(x_0\)</span>, we know <span class="math inline">\(h^\prime(x_0) = 0\)</span>. Therefore, we can simplify the above expression to be</p>
<p><span class="math display">\[
= 
\int_a^b\exp\left(h(x_0) + \frac{1}{2}h^{\prime\prime}(x_0)(x-x_0)^2\right)\,dx
\]</span> Given that <span class="math inline">\(h(x_0)\)</span> is a constant that doesn’t depend on <span class="math inline">\(x\)</span>, we can pull it outside the integral. In addition, we can rearrange some of the terms to give us <span class="math display">\[
=
\exp(h(x_0))
\int_a^b
\exp\left(-\frac{1}{2}\frac{(x-x_0)^2}{-h^{\prime\prime}(x_0)^{-1}}\right)\,dx
\]</span></p>
<p>Now that looks more like it, right? Inside the integral we have a quantity that is proportional to a Normal density with mean <span class="math inline">\(x_0\)</span> and variance <span class="math inline">\(-h^{\prime\prime}(x_0)^{-1}\)</span>. At this point we are just one call to the <code>pnorm()</code> function away from approximating our integral. All we need is to compute our normalizing constants.</p>
<p>If we let <span class="math inline">\(\Phi(x\mid \mu, \sigma^2)\)</span> be the cumulative distribution function for the Normal distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span> (and <span class="math inline">\(\varphi\)</span> is its density function), then we can write the above expression as</p>
<span class="math display">\[\begin{eqnarray*}
&amp; = &amp;
\exp(h(x_0))
\sqrt{\frac{2\pi}{-h^{\prime\prime}(x_0)}}
\int_a^b
\varphi(x\mid x_0,-h^{\prime\prime}(x_0)^{-1})\,dx\\
&amp; = &amp; 
\exp(h(x_0))
\sqrt{\frac{2\pi}{-h^{\prime\prime}(x_0)}}
\left[
\Phi\left(b\mid x_0,-h^{\prime\prime}(x_0)^{-1}\right)
- \Phi\left(a\mid x_0,-h^{\prime\prime}(x_0)^{-1}\right)
\right]
\end{eqnarray*}\]</span>
<p>Recall that <span class="math inline">\(\exp(h(x_0)) = g(x_0)\)</span>. If <span class="math inline">\(b=\infty\)</span> and <span class="math inline">\(a = -\infty\)</span>, as is commonly the case, then the term in the square brackets is equal to <span class="math inline">\(1\)</span>, making the Laplace approximation equal to the value of the function <span class="math inline">\(g(x)\)</span> at its mode multiplied by a constant that depends on the curvature of the function <span class="math inline">\(h\)</span>.</p>
<p>One final note about the Laplace approximation is that it replaces the problem of integrating a function with the problem of <em>maximizing</em> it. In order to compute the Laplace approximation, we have to compute the location of the mode, which is an optimization problem. Often, this problem is faster to solve using well-understood function optimizers than integrating the same function would be.</p>
<div id="computing-the-posterior-mean" class="section level3">
<h3><span class="header-section-number">5.1.1</span> Computing the Posterior Mean</h3>
<p>In Bayesian computations we often want to compute the posterior mean of a parameter given the observed data. If <span class="math inline">\(y\)</span> represents data we observe and <span class="math inline">\(y\)</span> comes from the distribution <span class="math inline">\(f(y\mid\theta)\)</span> with parameter <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\theta\)</span> has a prior distribution <span class="math inline">\(\pi(\theta)\)</span>, then we usually want to compute the posterior distribution <span class="math inline">\(p(\theta\mid y)\)</span> and its mean, <span class="math display">\[
\mathbb{E}_p[\theta]
=
\int \theta\, p(\theta\mid y)\,d\theta.
\]</span> We can then write</p>
<span class="math display">\[\begin{eqnarray*}
\int \theta\,p(\theta\mid y)\,dx
&amp; = &amp;
\frac{
\int\theta\,f(y\mid\theta)\pi(\theta)\,d\theta
}{
\int f(y\mid\theta)\pi(\theta)\,d\theta
}\\
&amp; = &amp;
\frac{
\int\theta\,\exp(\log f(y\mid\theta)\pi(\theta))\,d\theta
}{
\int\exp(\log f(y\mid\theta)\pi(\theta)\,d\theta)
}
\end{eqnarray*}\]</span>
<p>Here, we’ve used the age old trick of exponentiating and log-ging.</p>
<p>If we let <span class="math inline">\(h(\theta) = \log f(y\mid\theta)\pi(\theta)\)</span>, then we can use the same Laplace approximation procedure described in the previous section. However, in order to do that we must know where <span class="math inline">\(h(\theta)\)</span> achieves its maximum. Because <span class="math inline">\(h(\theta)\)</span> is simply a monotonic transformation of a function proportional to the posterior density, we know that <span class="math inline">\(h(\theta)\)</span> achieves its maximum at the <em>posterior mode</em>.</p>
<p>Let <span class="math inline">\(\hat{\theta}\)</span> be the posterior mode of <span class="math inline">\(p(\theta\mid y)\)</span>. Then we have</p>
<span class="math display">\[\begin{eqnarray*}
\int \theta\,p(\theta\mid y)\,dx
&amp; \approx &amp;
\frac{
\int\theta
\exp\left(
h(\hat{\theta})
+ \frac{1}{2}h^{\prime\prime}(\hat{\theta})(\theta-\hat{\theta})^2
\right)\,d\theta
}{
\int
\exp\left(
h(\hat{\theta})
+ \frac{1}{2}h^{\prime\prime}(\hat{\theta})(\theta-\hat{\theta})^2
\right)\,d\theta
}\\
&amp; = &amp;
\frac{
\int\theta
\exp\left(
\frac{1}{2}h^{\prime\prime}(\hat{\theta})(\theta-\hat{\theta})^2
\right)\,d\theta
}{
\int
\exp\left(
\frac{1}{2}h^{\prime\prime}(\hat{\theta})(\theta-\hat{\theta})^2
\right)\,d\theta
}\\
&amp; = &amp;
\frac{
\int
\theta
\sqrt{\frac{2\pi}{-h^{\prime\prime}(\hat{\theta})}}
\varphi\left(\theta\mid\hat{\theta},-h^{\prime\prime}(\hat{\theta})^{-1}\right)
\,d\theta
}{
\int
\sqrt{\frac{2\pi}{-h^{\prime\prime}(\hat{\theta})}}
\varphi\left(\theta\mid\hat{\theta},-h^{\prime\prime}(\hat{\theta})^{-1}\right)
\,d\theta
}\\
&amp; = &amp;
\hat{\theta}
\end{eqnarray*}\]</span>
<p>Hence, the Laplace approximation to the posterior mean is equal to the posterior mode. This approximation is likely to work well when the posterior is unimodal and relatively symmetric around the model. Furthermore, the more concentrated the posterior is around <span class="math inline">\(\hat{\theta}\)</span>, the better.</p>
<div id="example-poisson-data-with-a-gamma-prior" class="section level4">
<h4><span class="header-section-number">5.1.1.1</span> Example: Poisson Data with a Gamma Prior</h4>
<p>In this simple example, we will use data drawn from a Poisson distribution with a mean that has a Gamma prior distribution. The model is therefore</p>
<span class="math display">\[\begin{eqnarray*}
Y \mid \mu &amp; \sim &amp; \text{Poisson}(\mu)\\
\mu &amp; \sim &amp; \text{Gamma}(a, b)
\end{eqnarray*}\]</span>
<p>where the Gamma density is <span class="math display">\[
f(\mu)
=
\frac{1}{b^{a}\Gamma(a)}\mu^{a - 1}e^{-\mu/b}
\]</span></p>
<p>In this case, given an observation <span class="math inline">\(y\)</span>, the posterior distribution is simply a Gamma distribution with shape parameter <span class="math inline">\(y + a\)</span> and scale parameter <span class="math inline">\((1 + 1/b)\)</span>.</p>
<p>Suppose we observe <span class="math inline">\(y = 2\)</span>. We can draw the posterior distribution and prior distribution as follows.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">make_post &lt;-<span class="st"> </span><span class="cf">function</span>(y, shape, scale) {
        <span class="cf">function</span>(x) {
                <span class="kw">dgamma</span>(x, <span class="dt">shape =</span> y <span class="op">+</span><span class="st"> </span>shape,
                       <span class="dt">scale =</span> <span class="dv">1</span> <span class="op">/</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span>scale))
        }
}
<span class="kw">set.seed</span>(<span class="dv">2017</span><span class="op">-</span><span class="dv">11</span><span class="op">-</span><span class="dv">29</span>)
y &lt;-<span class="st"> </span><span class="dv">2</span>
prior.shape &lt;-<span class="st"> </span><span class="dv">3</span>
prior.scale &lt;-<span class="st"> </span><span class="dv">3</span>
p &lt;-<span class="st"> </span><span class="kw">make_post</span>(y, prior.shape, prior.scale)
<span class="kw">curve</span>(p, <span class="dv">0</span>, <span class="dv">12</span>, <span class="dt">n =</span> <span class="dv">1000</span>, <span class="dt">lwd =</span> <span class="dv">3</span>, <span class="dt">xlab =</span> <span class="kw">expression</span>(mu),
      <span class="dt">ylab =</span> <span class="kw">expression</span>(<span class="kw">paste</span>(<span class="st">&quot;p(&quot;</span>, mu, <span class="st">&quot; | y)&quot;</span>)))
<span class="kw">curve</span>(<span class="kw">dgamma</span>(x, <span class="dt">shape =</span> prior.shape, <span class="dt">scale =</span> prior.scale), <span class="dt">add =</span> <span class="ot">TRUE</span>,
      <span class="dt">lty =</span> <span class="dv">2</span>)
<span class="kw">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="dt">legend =</span> <span class="kw">c</span>(<span class="st">&quot;Posterior&quot;</span>, <span class="st">&quot;Prior&quot;</span>), <span class="dt">lty =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>), <span class="dt">lwd =</span> <span class="kw">c</span>(<span class="dv">3</span>, <span class="dv">1</span>), <span class="dt">bty =</span> <span class="st">&quot;n&quot;</span>)</code></pre></div>
<p><img src="integration_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Because this is a Gamma distribution, we can also compute the posterior mode in closed form.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pmode &lt;-<span class="st"> </span>(y <span class="op">+</span><span class="st"> </span>prior.shape <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">/</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span>prior.scale))
pmode</code></pre></div>
<pre><code>[1] 3</code></pre>
<p>We can also compute the mean.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pmean &lt;-<span class="st"> </span>(y <span class="op">+</span><span class="st"> </span>prior.shape) <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">/</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span>prior.scale))
pmean</code></pre></div>
<pre><code>[1] 3.75</code></pre>
<p>From the skewness in the figure above, it’s clear that the mean and the mode should not match.</p>
<p>We can now see what the Laplace approximation to the posterior looks like in this case. First, we can compute the gradient and Hessian of the Gamma density.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">a &lt;-<span class="st"> </span>prior.shape
b &lt;-<span class="st"> </span>prior.scale
fhat &lt;-<span class="st"> </span><span class="kw">deriv3</span>(<span class="op">~</span><span class="st"> </span>mu<span class="op">^</span>(y <span class="op">+</span><span class="st"> </span>a <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) <span class="op">*</span><span class="st"> </span><span class="kw">exp</span>(<span class="op">-</span>mu <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="dv">1</span><span class="op">/</span>b)) <span class="op">/</span><span class="st"> </span>((<span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span><span class="op">+</span><span class="dv">1</span><span class="op">/</span>b))<span class="op">^</span>(y<span class="op">+</span>a) <span class="op">*</span><span class="st"> </span><span class="kw">gamma</span>(y <span class="op">+</span><span class="st"> </span>a)), <span class="st">&quot;mu&quot;</span>, <span class="dt">function.arg =</span> <span class="ot">TRUE</span>)</code></pre></div>
<p>Then we can compute the quadratic approximation to the density via the <code>lapprox()</code> function below.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">post.shape &lt;-<span class="st"> </span>y <span class="op">+</span><span class="st"> </span>prior.shape <span class="op">-</span><span class="st"> </span><span class="dv">1</span>
post.scale &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span>(<span class="kw">length</span>(y) <span class="op">+</span><span class="st"> </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span>prior.scale)
lapprox &lt;-<span class="st"> </span><span class="kw">Vectorize</span>(<span class="cf">function</span>(mu, <span class="dt">mu0 =</span> pmode) {
        deriv &lt;-<span class="st"> </span><span class="kw">fhat</span>(mu0)
        grad &lt;-<span class="st"> </span><span class="kw">attr</span>(deriv, <span class="st">&quot;gradient&quot;</span>)
        hess &lt;-<span class="st"> </span><span class="kw">drop</span>(<span class="kw">attr</span>(deriv, <span class="st">&quot;hessian&quot;</span>))
        f &lt;-<span class="st"> </span><span class="cf">function</span>(x) <span class="kw">dgamma</span>(x, <span class="dt">shape =</span> post.shape, <span class="dt">scale =</span> post.scale)
        hpp &lt;-<span class="st"> </span>(hess <span class="op">*</span><span class="st"> </span><span class="kw">f</span>(mu0) <span class="op">-</span><span class="st"> </span>grad<span class="op">^</span><span class="dv">2</span>) <span class="op">/</span><span class="st"> </span><span class="kw">f</span>(mu0)<span class="op">^</span><span class="dv">2</span>
        <span class="kw">exp</span>(<span class="kw">log</span>(<span class="kw">f</span>(mu0)) <span class="op">+</span><span class="st"> </span><span class="fl">0.5</span> <span class="op">*</span><span class="st"> </span>hpp <span class="op">*</span><span class="st"> </span>(mu <span class="op">-</span><span class="st"> </span>mu0)<span class="op">^</span><span class="dv">2</span>)
}, <span class="st">&quot;mu&quot;</span>)</code></pre></div>
<p>Plotting the true posterior and the Laplace approximation gives us the following.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">curve</span>(p, <span class="dv">0</span>, <span class="dv">12</span>, <span class="dt">n =</span> <span class="dv">1000</span>, <span class="dt">lwd =</span> <span class="dv">3</span>, <span class="dt">xlab =</span> <span class="kw">expression</span>(mu),
      <span class="dt">ylab =</span> <span class="kw">expression</span>(<span class="kw">paste</span>(<span class="st">&quot;p(&quot;</span>, mu, <span class="st">&quot; | y)&quot;</span>)))
<span class="kw">curve</span>(<span class="kw">dgamma</span>(x, <span class="dt">shape =</span> prior.shape, <span class="dt">scale =</span> prior.scale), <span class="dt">add =</span> <span class="ot">TRUE</span>,
      <span class="dt">lty =</span> <span class="dv">2</span>)
<span class="kw">legend</span>(<span class="st">&quot;topright&quot;</span>, 
       <span class="dt">legend =</span> <span class="kw">c</span>(<span class="st">&quot;Posterior Density&quot;</span>, <span class="st">&quot;Prior Density&quot;</span>, <span class="st">&quot;Laplace Approx&quot;</span>), 
       <span class="dt">lty =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>), <span class="dt">lwd =</span> <span class="kw">c</span>(<span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">1</span>), <span class="dt">col =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>), <span class="dt">bty =</span> <span class="st">&quot;n&quot;</span>)
<span class="kw">curve</span>(lapprox, <span class="fl">0.001</span>, <span class="dv">12</span>, <span class="dt">n =</span> <span class="dv">1000</span>, <span class="dt">add =</span> <span class="ot">TRUE</span>, <span class="dt">col =</span> <span class="dv">2</span>, <span class="dt">lwd =</span> <span class="dv">2</span>)</code></pre></div>
<p><img src="integration_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>The solid red curve is the Laplace approximation and we can see that in the neighborhood of the mode, the approximation is reasonable. However, as we move farther away from the mode, the tail of the Gamma is heavier on the right.</p>
<p>Of course, this Laplace approximation is done with only a single observation. One would expect the approximation to improve as the sample size increases. In this case, with respect to the posterior mode as an approximation to the posterior mean, we can see that the difference between the two is simply</p>
<p><span class="math display">\[
\hat{\theta}_{\text{mean}} - \hat{\theta}_{\text{mode}} = \frac{1}{n + 1/b}
\]</span></p>
<p>which clearly goes to zero as <span class="math inline">\(n\rightarrow\infty\)</span>.</p>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="integration.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="variational-inference.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
