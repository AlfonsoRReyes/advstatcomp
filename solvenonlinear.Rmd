# Solving Nonlinear Equations

## Bisection Algorithm

The bisection algorithm is a simple method for finding the roots of one-dimensional functions. The goal is to find a root $x_0\in[a, b]$ such that $f(x_0)=0$. The algorithm starts with a large interval, known to contain $x_0$, and then successively reduces the size of the interval until it brackets the root. The theoretical underpinning of the algorithm is the [intermediate value theorem](https://en.wikipedia.org/wiki/Intermediate_value_theorem) which states that if a continuous function $f$ takes values $f(a)$ and $f(b)$ at the end points of the interval $[a, b]$, then $f$ must take all values between $f(a)$ and $f(b)$ somewhere in the interval. So if $f(a) < \gamma < f(b)$, then there exists a $c\in[a, b]$ such that $f(c)=\gamma$.

Using this information, we can present the bisection algorithm. First we must check that $\text{sign}(f(a)) \ne \text{sign}(f(b))$. Otherwise, the interval does not contain the root and might need to be widened. Then we can proceed:

1. Let $c = \frac{a + b}{2}$.

2. If $f(c) = 0$, stop and return $c$.

3. If $\text{sign}(f(a))\ne\text{sign}(f(c))$, then set $b\leftarrow c$. Else if $\text{sign}(f(b))\ne\text{sign}(f(c))$, then set $a\leftarrow c$.

4. Goto the beginning and repeat until convergence (see below).

After $n$ iterations, the size of the interval bracketing the root will be $2^{-n}(b-a)$.

The bisection algorithm is useful, conceptually simple, and is easy to implement. In particular, you do not need any special information about the function $f$ except the ability to evaluate it at various points in the interval. The downsides are that it is only useful in one dimension and its convergence is linear, which is the slowest rate of convergence for algorithms we will discuss (more on that later).

The bisection algorithm can run into problems in situations where the function $f$ is not well behaved. The ideal situation for the bisection algorithm looks something like this.

![Ideal setup for bisection algorithm.](image/bisection-ideal.png)

Here, $f(a)$ and $f(b)$ are of opposite signs and the root is clearly in between $a$ and $b$.

In the scenario below, the algorithm will not start because $f(a)>0$ and $f(b)>0$.

![Derivative of $f$ at the root is $0$.](image/bisection-tangent.png)


In this next scenario, there are two roots between $a$ and $b$, in addition to having $f(a)>0$ and $f(b)>0$. One would need to reduce the length of the starting interval in order to find either root.

![Two roots within the interval.](image/bisection-two.png)

In the scenario below, the algorithm will start because $f(a)$ and $f(b)$ are of opposite sign, but there is no root.

![Interval contains an asymptote but no root.](image/bisection-asymptote.png)

Convergence of the bisection algorithm can be determined by either having $|b-a|<\varepsilon$ for some small $\varepsilon$ or having $|f(b)-f(a)|<\varepsilon$. Which criterion you use will depend on the specific application and on what kinds of tolerances are required. 


### Example: Quantiles

Given a cumulative distribution function $F(x)$ and a number $p\in (0, 1)$, a quantile of $F$ is a number $x$ such that $F(x) = p$. The bisection algorithm can be used to find a quantile $x$ for a given $p$ by defining the function $g(x) = F(x) - p$ and solving for the value of $x$ that achieves $g(x) = 0$.

## Rates of Convergence

### Linear convergence

Suppose we have a sequence $\{x_n\}$ such that $x_n\rightarrow x_\infty$ in $\mathfrak{R}^k$. We say the convergence is *linear* if there exists $r\in(0, 1)$ such that 

\[
\frac{\|x_{n+1}-x_\infty\|}{\|x_n-x_\infty\|}\leq r
\]
for all $n$ sufficiently large.

#### Example

The simple sequence $x_n = 1 + \left(\frac{1}{2}\right)^n$ converges linearly to $x_\infty = 1$ because

\[
\frac{\|x_{n+1}-x_\infty\|}{\|x_n-x_\infty\|}
=
\frac{\left(\frac{1}{2}\right)^{n+1}}{\left(\frac{1}{2}\right)^n}
=
\frac{1}{2}
\]
which is always in $(0, 1)$.

### Superlinear Convergence

We say a sequence $\{x_n\}$ converges to $x_\infty$ *superlinearly* if we have
\[
\lim_{n\rightarrow\infty} \frac{\|x_{n+1}-x_\infty\|}{\|x_n-x_\infty\|}
=
0
\]

The sequence above does not converge superlinearly because the ratio is always constant, and so never can converge to zero as $n\rightarrow\infty$. However, the sequence $x_n = 1 + \left(\frac{1}{n}\right)^n$ converges superlinearly to $1$.


### Quadratic Convergence

Quadratic convergence is the fastest form of convergence that we will discuss here and is generally considered desirable if possible to achieve. We say the sequence converges at a *quadratic* rate if there exists some constant $0 < M < \infty$ such that 
\[
\frac{\|x_{n+1}-x_\infty\|}{\|x_n-x_\infty\|^2}\leq M
\]
for all $n$ sufficiently large.

Extending the examples from above, the sequence $x_n = 1 + \left(\frac{1}{n}\right)^{2^n}$ converges quadratically to $1$. With this sequence, we have

\[
\frac{\|x_{n+1}-x_\infty\|}{\|x_n-x_\infty\|^2}
=
\frac{\left(\frac{1}{n+1}\right)^{2^{n+1}}}{\left(\frac{1}{n}\right)^{(2^n)2}}
=
\left(\frac{n}{n+1}\right)^{2^{n+1}}
\leq
1
\]

### Example: Bisection Algorithm

For the bisection algorithm, the error that we make in estimating the root is $x_n = |b_n - a_n|$. However, we know that the size of the interval in the bisection algorithm decreases by a half at each iteration. Therefore, we can write $x_n = 2^{-n}|b_0 - a_0|$. Therefore, we can write the rate of convergence as

\[
\frac{\|x_{n+1}-x_\infty\|}{\|x_n-x_\infty\|}
=
\frac{x_{n+1}}{x_n}
=
\frac{2^{-(n+1)}(b_0-a_0)}{2^{-n}(b_0-a_0)}
=
\frac{1}{2}
\]
Therefore, the error of the bisection algorithm converges linearly to $0$.





## Functional Iteration

### Proof of the Shrinking Lemma

### Convergence Rates for Shrinking Maps


## Newton's Method

### Proof of Newton's Method

### Convergence Rate of Newton's Method







