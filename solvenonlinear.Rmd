# Solving Nonlinear Equations

## Bisection Algorithm

The bisection algorithm is a simple method for finding the roots of one-dimensional functions. The goal is to find a root $x_0\in[a, b]$ such that $f(x_0)=0$. The algorithm starts with a large interval, known to contain $x_0$, and then successively reduces the size of the interval until it brackets the root. The theoretical underpinning of the algorithm is the [intermediate value theorem](https://en.wikipedia.org/wiki/Intermediate_value_theorem) which states that if a continuous function $f$ takes values $f(a)$ and $f(b)$ at the end points of the interval $[a, b]$, then $f$ must take all values between $f(a)$ and $f(b)$ somewhere in the interval. So if $f(a) < \gamma < f(b)$, then there exists a $c\in[a, b]$ such that $f(c)=\gamma$.

Using this information, we can present the bisection algorithm. First we must check that $\text{sign}(f(a)) \ne \text{sign}(f(b))$. Otherwise, the interval does not contain the root and might need to be widened. Then we can proceed:

1. Let $c = \frac{a + b}{2}$.

2. If $f(c) = 0$, stop and return $c$.

3. If $\text{sign}(f(a))\ne\text{sign}(f(c))$, then set $b\leftarrow c$. Else if $\text{sign}(f(b))\ne\text{sign}(f(c))$, then set $a\leftarrow c$.

4. Goto the beginning and repeat until convergence (see below).

After $n$ iterations, the size of the interval bracketing the root will be $2^{-n}(b-a)$.

The bisection algorithm is useful, conceptually simple, and is easy to implement. In particular, you do not need any special information about the function $f$ except the ability to evaluate it at various points in the interval. The downsides are that it is only useful in one dimension and its convergence is linear, which is the slowest rate of convergence for algorithms we will discuss (more on that later).

The bisection algorithm can run into problems in situations where the function $f$ is not well behaved. The ideal situation for the bisection algorithm looks something like this.

![Ideal setup for bisection algorithm.](image/bisection-ideal.png)

Here, $f(a)$ and $f(b)$ are of opposite signs and the root is clearly in between $a$ and $b$.

In the scenario below, the algorithm will not start because $f(a)>0$ and $f(b)>0$.

![Derivative of $f$ at the root is $0$.](image/bisection-tangent.png)


In this next scenario, there are two roots between $a$ and $b$, in addition to having $f(a)>0$ and $f(b)>0$. One would need to reduce the length of the starting interval in order to find either root.

![Two roots within the interval.](image/bisection-two.png)

In the scenario below, the algorithm will start because $f(a)$ and $f(b)$ are of opposite sign, but there is no root.

![Interval contains an asymptote but no root.](image/bisection-asymptote.png)

Convergence of the bisection algorithm can be determined by either having $|b-a|<\varepsilon$ for some small $\varepsilon$ or having $|f(b)-f(a)|<\varepsilon$. Which criterion you use will depend on the specific application and on what kinds of tolerances are required. 


### Example: Quantiles

Given a cumulative distribution function $F(x)$ and a number $p\in (0, 1)$, a quantile of $F$ is a number $x$ such that $F(x) = p$. The bisection algorithm can be used to find a quantile $x$ for a given $p$ by defining the function $g(x) = F(x) - p$ and solving for the value of $x$ that achieves $g(x) = 0$.

Another way to put this is that we are inverting the CDF to compute $x = F^{-1}(p)$. So the bisection algorithm can be used to invert functions in these situations.




## Rates of Convergence

One of the ways in which algorithms will be compared is via their rates of convergence to some limiting value. Typically, we have an interative algorithm that is trying to find the maximum/minimum of a function and we want an estimate of how long it will take to reach that optimal value. There are three rates of convergence that we will focus on here---linear, superlinear, and quadratic---which are ordered from slowest to fastest.

In our context, rates of convergence are typically determined by how much information about the target function $f$ we use in the updating process of the algorithm.  Algorithms that use little information about $f$, such as the bisection algorithm, converge slowly. Algorithms that require more information about $f$, such as derivative information, typically converge more quickly. There is no free lunch! 

### Linear convergence

Suppose we have a sequence $\{x_n\}$ such that $x_n\rightarrow x_\infty$ in $\mathfrak{R}^k$. We say the convergence is *linear* if there exists $r\in(0, 1)$ such that 

\[
\frac{\|x_{n+1}-x_\infty\|}{\|x_n-x_\infty\|}\leq r
\]
for all $n$ sufficiently large.

#### Example

The simple sequence $x_n = 1 + \left(\frac{1}{2}\right)^n$ converges linearly to $x_\infty = 1$ because

\[
\frac{\|x_{n+1}-x_\infty\|}{\|x_n-x_\infty\|}
=
\frac{\left(\frac{1}{2}\right)^{n+1}}{\left(\frac{1}{2}\right)^n}
=
\frac{1}{2}
\]
which is always in $(0, 1)$.

### Superlinear Convergence

We say a sequence $\{x_n\}$ converges to $x_\infty$ *superlinearly* if we have
\[
\lim_{n\rightarrow\infty} \frac{\|x_{n+1}-x_\infty\|}{\|x_n-x_\infty\|}
=
0
\]

The sequence above does not converge superlinearly because the ratio is always constant, and so never can converge to zero as $n\rightarrow\infty$. However, the sequence $x_n = 1 + \left(\frac{1}{n}\right)^n$ converges superlinearly to $1$.


### Quadratic Convergence

Quadratic convergence is the fastest form of convergence that we will discuss here and is generally considered desirable if possible to achieve. We say the sequence converges at a *quadratic* rate if there exists some constant $0 < M < \infty$ such that 
\[
\frac{\|x_{n+1}-x_\infty\|}{\|x_n-x_\infty\|^2}\leq M
\]
for all $n$ sufficiently large.

Extending the examples from above, the sequence $x_n = 1 + \left(\frac{1}{n}\right)^{2^n}$ converges quadratically to $1$. With this sequence, we have

\[
\frac{\|x_{n+1}-x_\infty\|}{\|x_n-x_\infty\|^2}
=
\frac{\left(\frac{1}{n+1}\right)^{2^{n+1}}}{\left(\frac{1}{n}\right)^{(2^n)2}}
=
\left(\frac{n}{n+1}\right)^{2^{n+1}}
\leq
1
\]

### Example: Bisection Algorithm

For the bisection algorithm, the error that we make in estimating the root is $x_n = |b_n - a_n|$, where $a_n$ and $b_n$ represent the end points of the bracketing interval at iteration $n$. However, we know that the size of the interval in the bisection algorithm decreases by a half at each iteration. Therefore, we can write $x_n = 2^{-n}|b_0 - a_0|$ and we can write the rate of convergence as

\[
\frac{\|x_{n+1}-x_\infty\|}{\|x_n-x_\infty\|}
=
\frac{x_{n+1}}{x_n}
=
\frac{2^{-(n+1)}(b_0-a_0)}{2^{-n}(b_0-a_0)}
=
\frac{1}{2}
\]
Therefore, the error of the bisection algorithm converges linearly to $0$.





## Functional Iteration

We want to find a solution to the equation $f(x)=0$ for $f: \mathbb{R}^k\rightarrow \mathbb{R}$ and $x\in S\subset \mathbb{R}^k$. One approach to solving this problem is to characterize solutions as *fixed points* of other functions. For example, if $f(x_0) = 0$, then $x_0$ is a fixed point of the function $g(x)=f(x) + x$. Another such function might be $g(x) = x(f(x) + 1)$ for $x\ne 0$.

In some situations, we can construct a function $g$ and a sequence $x_n = g(x_{n-1})$ such that we have the sequence $x_n\rightarrow x_\infty$ where $g(x_\infty) = x_\infty$. In other words, the sequence of values $x_n$ converges to a fixed point of $g$. If this fixed point satisfies $f(x_\infty) = 0$, then we have found a solution to our original problem. 

When can such a functional iteration procedure work? The Shrinking Lemma gives us the conditions under which this type of sequence will converge.



### Proof of the Shrinking Lemma

The Shrinking Lemma gives conditions under which a sequence derived via functional iteration will converge to a fixed point. Let $M$ be a closed subset of a complete normed vector space and let $f: M\rightarrow M$ be a map. Assume that there exists a $K$, $0<K<1$, such that for all $x,y\in M$, 

\[
\|f(x)-f(y)\|\leq K\|x-y\|.
\]

Then $f$ has a unique fixed point, i.e. there is a unique point $x_0\in M$ such that $f(x_0) = x_0$.

*Proof*: The basic idea of the proof of this lemma is that for a give $x\in M$, we can construct a Cauchy sequence $\{f^n(x)\}$ that converges to $x_0$, where $f^n(x)$ represents the $n$th functional iteration of $x$, i.e. $f^2(x) = f(f(x))$.

Given $x\in M$, we can write

\[
\|f^2(x)-f(x)\| = \|f(f(x)) - f(x)\|\leq K\|f(x)-x\|.
\]

By induction, we can therefore write

\[
\|f(^{n+1}(x)-f^{n}(x)\|\leq K\|f^n(x)-f^{n-1}(x)\|\leq K^n\|f(x)-x\|.
\]

It then follows that

\begin{eqnarray*}
\|f^n(x)-x\| & \leq & \|f^{n}(x)-f^{n-1}(x)\| + \|f^{n-1}(x)-f^{n-2}(x)\| + \cdots + \|f(x)-x\|\\
& \leq & (K^{n-1} + K^{n-2} + \cdots + K)\|f(x)-x\|\\
& \leq & \frac{1}{1-K}\|f(x)-x\|.
\end{eqnarray*}

Given integers $m\geq 1$ and $k\geq 1$, we can write 

\begin{eqnarray*}
\|f^{m+k}(x)-f^k(x)\| & \leq & K^m\|f^k(x)-x\|\\
& \leq & K^m\frac{1}{1-K}\|f(x)-x\|
\end{eqnarray*}

Therefore, there exists some $N$ such that for all $m, n\geq N$ (say $n=m+k$), we have $\|f^n(x)-f^m(x)\|\leq\varepsilon$, because $K^m\rightarrow 0$ as $m\rightarrow\infty$. As a result, the sequence $\{f^n(x)\}$ is a Cauchy sequence, so let $x_0$ be its limit. 

Given $\varepsilon>0$, let $N$ be such that for all $n\geq N$, $\|f^n(x)-x_0\|\leq\varepsilon$. Then we can also say that for $n\geq N$,

\[
\|f(x_0) - f^{n+1}(x)\|\leq \|x_0-f^n(x)\|\leq\varepsilon
\]

So what we have is $\{f^n(x)\}\rightarrow x_0$ and we have $\{f^n(x)\}\rightarrow f(x_0)$. Therefore, $x_0$ is a fixed point of $f$, so that $f(x_0)=x_0$.

To show that $x_0$ is unique, suppose that $x_1$ is another fixed point of $f$. Then

\[
\|x_1-x_0\| = \|f(x_1)-f(x_0)\|\leq K\|x_1-x_0\|.
\]
Because $0<K<1$, we must have $x_0=x_1$.


### Convergence Rates for Shrinking Maps


### Newton's Method

### Proof of Newton's Method

### Convergence Rate of Newton's Method







