<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Advanced Statistical Computing</title>
  <meta name="description" content="The book covers material taught in the Johns Hopkins Biostatistics Advanced Statistical Computing course.">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="Advanced Statistical Computing" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://github.com/rdpeng/advstatcomp" />
  <meta property="og:image" content="https://github.com/rdpeng/advstatcompcover-image.png" />
  <meta property="og:description" content="The book covers material taught in the Johns Hopkins Biostatistics Advanced Statistical Computing course." />
  <meta name="github-repo" content="rdpeng/advstatcomp" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Advanced Statistical Computing" />
  
  <meta name="twitter:description" content="The book covers material taught in the Johns Hopkins Biostatistics Advanced Statistical Computing course." />
  <meta name="twitter:image" content="https://github.com/rdpeng/advstatcompcover-image.png" />

<meta name="author" content="Roger D. Peng">


<meta name="date" content="2017-12-12">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="principle-of-optimization-transfer.html">
<link rel="next" href="solving-nonlinear-equations.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="css\style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Advanced Statistical Computing</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a><ul>
<li class="chapter" data-level="" data-path="stay-in-touch.html"><a href="stay-in-touch.html"><i class="fa fa-check"></i>Stay in Touch!</a></li>
<li class="chapter" data-level="" data-path="setup.html"><a href="setup.html"><i class="fa fa-check"></i>Setup</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="example-linear-models.html"><a href="example-linear-models.html"><i class="fa fa-check"></i><b>1.1</b> Example: Linear Models</a></li>
<li class="chapter" data-level="1.2" data-path="principle-of-optimization-transfer.html"><a href="principle-of-optimization-transfer.html"><i class="fa fa-check"></i><b>1.2</b> Principle of Optimization Transfer</a></li>
<li class="chapter" data-level="1.3" data-path="textbooks-vs-computers.html"><a href="textbooks-vs-computers.html"><i class="fa fa-check"></i><b>1.3</b> Textbooks vs. Computers</a><ul>
<li class="chapter" data-level="1.3.1" data-path="textbooks-vs-computers.html"><a href="textbooks-vs-computers.html#using-logarithms"><i class="fa fa-check"></i><b>1.3.1</b> Using Logarithms</a></li>
<li class="chapter" data-level="1.3.2" data-path="textbooks-vs-computers.html"><a href="textbooks-vs-computers.html#linear-regression"><i class="fa fa-check"></i><b>1.3.2</b> Linear Regression</a></li>
<li class="chapter" data-level="1.3.3" data-path="textbooks-vs-computers.html"><a href="textbooks-vs-computers.html#multivariate-normal-distribution"><i class="fa fa-check"></i><b>1.3.3</b> Multivariate Normal Distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="solving-nonlinear-equations.html"><a href="solving-nonlinear-equations.html"><i class="fa fa-check"></i><b>2</b> Solving Nonlinear Equations</a><ul>
<li class="chapter" data-level="2.1" data-path="bisection-algorithm.html"><a href="bisection-algorithm.html"><i class="fa fa-check"></i><b>2.1</b> Bisection Algorithm</a><ul>
<li class="chapter" data-level="2.1.1" data-path="bisection-algorithm.html"><a href="bisection-algorithm.html#example-quantiles"><i class="fa fa-check"></i><b>2.1.1</b> Example: Quantiles</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="rates-of-convergence.html"><a href="rates-of-convergence.html"><i class="fa fa-check"></i><b>2.2</b> Rates of Convergence</a><ul>
<li class="chapter" data-level="2.2.1" data-path="rates-of-convergence.html"><a href="rates-of-convergence.html#linear-convergence"><i class="fa fa-check"></i><b>2.2.1</b> Linear convergence</a></li>
<li class="chapter" data-level="2.2.2" data-path="rates-of-convergence.html"><a href="rates-of-convergence.html#superlinear-convergence"><i class="fa fa-check"></i><b>2.2.2</b> Superlinear Convergence</a></li>
<li class="chapter" data-level="2.2.3" data-path="rates-of-convergence.html"><a href="rates-of-convergence.html#quadratic-convergence"><i class="fa fa-check"></i><b>2.2.3</b> Quadratic Convergence</a></li>
<li class="chapter" data-level="2.2.4" data-path="rates-of-convergence.html"><a href="rates-of-convergence.html#example-bisection-algorithm"><i class="fa fa-check"></i><b>2.2.4</b> Example: Bisection Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="functional-iteration.html"><a href="functional-iteration.html"><i class="fa fa-check"></i><b>2.3</b> Functional Iteration</a><ul>
<li class="chapter" data-level="2.3.1" data-path="functional-iteration.html"><a href="functional-iteration.html#the-shrinking-lemma"><i class="fa fa-check"></i><b>2.3.1</b> The Shrinking Lemma</a></li>
<li class="chapter" data-level="2.3.2" data-path="functional-iteration.html"><a href="functional-iteration.html#convergence-rates-for-shrinking-maps"><i class="fa fa-check"></i><b>2.3.2</b> Convergence Rates for Shrinking Maps</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="newtons-method.html"><a href="newtons-method.html"><i class="fa fa-check"></i><b>2.4</b> Newton’s Method</a><ul>
<li class="chapter" data-level="2.4.1" data-path="newtons-method.html"><a href="newtons-method.html#proof-of-newtons-method"><i class="fa fa-check"></i><b>2.4.1</b> Proof of Newton’s Method</a></li>
<li class="chapter" data-level="2.4.2" data-path="newtons-method.html"><a href="newtons-method.html#convergence-rate-of-newtons-method"><i class="fa fa-check"></i><b>2.4.2</b> Convergence Rate of Newton’s Method</a></li>
<li class="chapter" data-level="2.4.3" data-path="newtons-method.html"><a href="newtons-method.html#newtons-method-for-maximum-likelihood-estimation"><i class="fa fa-check"></i><b>2.4.3</b> Newton’s Method for Maximum Likelihood Estimation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="general-optimization.html"><a href="general-optimization.html"><i class="fa fa-check"></i><b>3</b> General Optimization</a><ul>
<li class="chapter" data-level="3.1" data-path="steepest-descent.html"><a href="steepest-descent.html"><i class="fa fa-check"></i><b>3.1</b> Steepest Descent</a><ul>
<li class="chapter" data-level="3.1.1" data-path="steepest-descent.html"><a href="steepest-descent.html#example-multivariate-normal"><i class="fa fa-check"></i><b>3.1.1</b> Example: Multivariate Normal</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="the-newton-direction.html"><a href="the-newton-direction.html"><i class="fa fa-check"></i><b>3.2</b> The Newton Direction</a><ul>
<li class="chapter" data-level="3.2.1" data-path="the-newton-direction.html"><a href="the-newton-direction.html#generalized-linear-models"><i class="fa fa-check"></i><b>3.2.1</b> Generalized Linear Models</a></li>
<li class="chapter" data-level="3.2.2" data-path="the-newton-direction.html"><a href="the-newton-direction.html#newtons-method-in-r"><i class="fa fa-check"></i><b>3.2.2</b> Newton’s Method in R</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="quasi-newton.html"><a href="quasi-newton.html"><i class="fa fa-check"></i><b>3.3</b> Quasi-Newton</a><ul>
<li class="chapter" data-level="3.3.1" data-path="quasi-newton.html"><a href="quasi-newton.html#quasi-newton-methods-in-r"><i class="fa fa-check"></i><b>3.3.1</b> Quasi-Newton Methods in R</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="conjugate-gradient.html"><a href="conjugate-gradient.html"><i class="fa fa-check"></i><b>3.4</b> Conjugate Gradient</a></li>
<li class="chapter" data-level="3.5" data-path="coordinate-descent.html"><a href="coordinate-descent.html"><i class="fa fa-check"></i><b>3.5</b> Coordinate Descent</a><ul>
<li class="chapter" data-level="3.5.1" data-path="coordinate-descent.html"><a href="coordinate-descent.html#convergence-rates"><i class="fa fa-check"></i><b>3.5.1</b> Convergence Rates</a></li>
<li class="chapter" data-level="3.5.2" data-path="coordinate-descent.html"><a href="coordinate-descent.html#generalized-additive-models"><i class="fa fa-check"></i><b>3.5.2</b> Generalized Additive Models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="the-em-algorithm.html"><a href="the-em-algorithm.html"><i class="fa fa-check"></i><b>4</b> The EM Algorithm</a><ul>
<li class="chapter" data-level="4.1" data-path="em-algorithm-for-exponential-families.html"><a href="em-algorithm-for-exponential-families.html"><i class="fa fa-check"></i><b>4.1</b> EM Algorithm for Exponential Families</a></li>
<li class="chapter" data-level="4.2" data-path="canonical-examples.html"><a href="canonical-examples.html"><i class="fa fa-check"></i><b>4.2</b> Canonical Examples</a><ul>
<li class="chapter" data-level="4.2.1" data-path="canonical-examples.html"><a href="canonical-examples.html#two-part-normal-mixture-model"><i class="fa fa-check"></i><b>4.2.1</b> Two-Part Normal Mixture Model</a></li>
<li class="chapter" data-level="4.2.2" data-path="canonical-examples.html"><a href="canonical-examples.html#censored-exponential-data"><i class="fa fa-check"></i><b>4.2.2</b> Censored Exponential Data</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="a-minorizing-function.html"><a href="a-minorizing-function.html"><i class="fa fa-check"></i><b>4.3</b> A Minorizing Function</a><ul>
<li class="chapter" data-level="4.3.1" data-path="a-minorizing-function.html"><a href="a-minorizing-function.html#example-minorization-in-a-two-part-mixture-model"><i class="fa fa-check"></i><b>4.3.1</b> Example: Minorization in a Two-Part Mixture Model</a></li>
<li class="chapter" data-level="4.3.2" data-path="a-minorizing-function.html"><a href="a-minorizing-function.html#constrained-minimization-with-and-adaptive-barrier"><i class="fa fa-check"></i><b>4.3.2</b> Constrained Minimization With and Adaptive Barrier</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="missing-information-principle.html"><a href="missing-information-principle.html"><i class="fa fa-check"></i><b>4.4</b> Missing Information Principle</a></li>
<li class="chapter" data-level="4.5" data-path="acceleration-methods.html"><a href="acceleration-methods.html"><i class="fa fa-check"></i><b>4.5</b> Acceleration Methods</a><ul>
<li class="chapter" data-level="4.5.1" data-path="acceleration-methods.html"><a href="acceleration-methods.html#louiss-acceleration"><i class="fa fa-check"></i><b>4.5.1</b> Louis’s Acceleration</a></li>
<li class="chapter" data-level="4.5.2" data-path="acceleration-methods.html"><a href="acceleration-methods.html#squarem"><i class="fa fa-check"></i><b>4.5.2</b> SQUAREM</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="integration.html"><a href="integration.html"><i class="fa fa-check"></i><b>5</b> Integration</a><ul>
<li class="chapter" data-level="5.1" data-path="laplace-approximation.html"><a href="laplace-approximation.html"><i class="fa fa-check"></i><b>5.1</b> Laplace Approximation</a><ul>
<li class="chapter" data-level="5.1.1" data-path="laplace-approximation.html"><a href="laplace-approximation.html#computing-the-posterior-mean"><i class="fa fa-check"></i><b>5.1.1</b> Computing the Posterior Mean</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="variational-inference.html"><a href="variational-inference.html"><i class="fa fa-check"></i><b>5.2</b> Variational Inference</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="independent-monte-carlo.html"><a href="independent-monte-carlo.html"><i class="fa fa-check"></i><b>6</b> Independent Monte Carlo</a><ul>
<li class="chapter" data-level="6.1" data-path="random-number-generation.html"><a href="random-number-generation.html"><i class="fa fa-check"></i><b>6.1</b> Random Number Generation</a><ul>
<li class="chapter" data-level="6.1.1" data-path="random-number-generation.html"><a href="random-number-generation.html#pseudo-random-numbers"><i class="fa fa-check"></i><b>6.1.1</b> Pseudo-random Numbers</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="non-uniform-random-numbers.html"><a href="non-uniform-random-numbers.html"><i class="fa fa-check"></i><b>6.2</b> Non-Uniform Random Numbers</a><ul>
<li class="chapter" data-level="6.2.1" data-path="non-uniform-random-numbers.html"><a href="non-uniform-random-numbers.html#inverse-cdf-transformation"><i class="fa fa-check"></i><b>6.2.1</b> Inverse CDF Transformation</a></li>
<li class="chapter" data-level="6.2.2" data-path="non-uniform-random-numbers.html"><a href="non-uniform-random-numbers.html#other-transformations"><i class="fa fa-check"></i><b>6.2.2</b> Other Transformations</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="rejection-sampling.html"><a href="rejection-sampling.html"><i class="fa fa-check"></i><b>6.3</b> Rejection Sampling</a><ul>
<li class="chapter" data-level="6.3.1" data-path="rejection-sampling.html"><a href="rejection-sampling.html#the-algorithm"><i class="fa fa-check"></i><b>6.3.1</b> The Algorithm</a></li>
<li class="chapter" data-level="6.3.2" data-path="rejection-sampling.html"><a href="rejection-sampling.html#properties-of-rejection-sampling"><i class="fa fa-check"></i><b>6.3.2</b> Properties of Rejection Sampling</a></li>
<li class="chapter" data-level="6.3.3" data-path="rejection-sampling.html"><a href="rejection-sampling.html#empirical-supremum-rejection-sampling"><i class="fa fa-check"></i><b>6.3.3</b> Empirical Supremum Rejection Sampling</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="importance-sampling.html"><a href="importance-sampling.html"><i class="fa fa-check"></i><b>6.4</b> Importance Sampling</a><ul>
<li class="chapter" data-level="6.4.1" data-path="importance-sampling.html"><a href="importance-sampling.html#example-bayesian-sensitivity-analysis"><i class="fa fa-check"></i><b>6.4.1</b> Example: Bayesian Sensitivity Analysis</a></li>
<li class="chapter" data-level="6.4.2" data-path="importance-sampling.html"><a href="importance-sampling.html#example-calculating-marginal-likelihoods"><i class="fa fa-check"></i><b>6.4.2</b> Example: Calculating Marginal Likelihoods</a></li>
<li class="chapter" data-level="6.4.3" data-path="importance-sampling.html"><a href="importance-sampling.html#properties-of-the-importance-sampling-estimator"><i class="fa fa-check"></i><b>6.4.3</b> Properties of the Importance Sampling Estimator</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html"><i class="fa fa-check"></i><b>7</b> Markov Chain Monte Carlo</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Advanced Statistical Computing</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="textbooks-vs.computers" class="section level2">
<h2><span class="header-section-number">1.3</span> Textbooks vs. Computers</h2>
<p>One confusing aspect of statistical computing is that often there is a disconnect between what is printed in a statistical computing textbook and what <em>should</em> be implemented on the computer. In textbooks, it is usually simpler to present solutions as convenient mathematical formulas whenever possible, in order to communicate basic ideas and to provide some insight. However, directly translating these formulas into computer code is usually not advisable because there are many problematic aspects of computers that are simply not relevant when writing things down on paper.</p>
<p>Some key issues to look for when implementing statistical or numerical solutions on the computer are</p>
<ol style="list-style-type: decimal">
<li><p>Overflow - When numbers get too big, they cannot be represented on a computer and so often <code>NA</code>s are produced instead;</p></li>
<li><p>Underflow - Similar to overflow, numbers can get too small for computers to represent, resulting in errors or warnings or inaccurate computation;</p></li>
<li><p>Near linear dependence - the existence of linear dependence in matrix computations depends on the precision of a machine. Because computers are finite precision, there are commonly situations where one might think there is no linear dependence but the computer cannot tell the difference.</p></li>
</ol>
<p>All three of the above problems arise from the finite precision nature of all computers. One must take care to use algorithms that do calculations in the computable range and that automatically handle things like near dependence.</p>
<p>Below, I highlight some common examples in statistics where the implementation diverges from what textbooks explain as the solution: Computing with logarithms, the least squares solution to the linear regression estimation problem, and the computation of the multivariate Normal density. Both problems, on paper, involve inverting a matrix, which is typically a warning sign in any linear algebra problem. While matrix inverses are commonly found in statistics textbooks, it’s rare in practice that you will ever want to directly compute them. This point bears repeating: <strong>If you find yourself computing the inverse of a matrix, there is usually a better way of doing whatever you are trying to do</strong>.</p>
<div id="using-logarithms" class="section level3">
<h3><span class="header-section-number">1.3.1</span> Using Logarithms</h3>
<p>Most textbooks write out functions, such as densities, in their natural form. For example, the univariate Normal distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span> is written <span class="math display">\[
f(x\mid\mu,\sigma^2)
=
\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2\sigma^2}(x-\mu)^2}
\]</span> and you can compute this value for any <span class="math inline">\(x\)</span>, <span class="math inline">\(\mu\)</span>, and <span class="math inline">\(\sigma\)</span> in R with the <code>dnorm()</code> function.</p>
<p>But in practice, you almost never have to compute this exact number. Usually you can get away with computing the log of this value (and with <code>dnorm()</code> you can set the option <code>log = TRUE</code>). In some situations, such as with <a href="importance-sampling.html#importance-sampling">importance sampling</a>, you do have to compute density values on the original scale, and that can be considered a disadvantage of that technique.</p>
<p>Computing densities with logarithms is much more numerically stable than computing densities without them. With the exponential function in the density, numbers can get very small quickly, to the point where they are too small for the machine to represent (underflow). In some situations, you may need to take the ratio of two densities and then you may end up with either underflow or overflow (if numbers get too big). Doing calculations on a log scale (and then exponentiating them later if needed) usually resolves problems of underflow or overflow.</p>
<p>In this book (and in any other), when you see expressions like <span class="math inline">\(f(x)/g(x)\)</span>, you should think that this means <span class="math inline">\(\exp(\log f(x) - \log(g(x)))\)</span>. The two are equivalent but the latter is likely more numerically stable. In fact, most of the time, you never have to re-exponentiate the values, in which case you can spend your entire time in log-land. For example, in the <a href="rejection-sampling.html#rejection-sampling">rejection sampling</a> algorithm, you need to determine if <span class="math inline">\(U\leq\frac{f(x)}{g(x)}\)</span>. However, taking the log of both sides allows you to do the exact same comparison in a much more numerically stable way.</p>
</div>
<div id="linear-regression" class="section level3">
<h3><span class="header-section-number">1.3.2</span> Linear Regression</h3>
<p>The typical linear regression model, written in matrix form, is represented as follows,</p>
<p><span class="math display">\[
y = X\beta + \varepsilon
\]</span> where <span class="math inline">\(y\)</span> is an <span class="math inline">\(n\times 1\)</span> observed response, <span class="math inline">\(X\)</span> is the <span class="math inline">\(n\times p\)</span> predictor matrix, <span class="math inline">\(\beta\)</span> is the <span class="math inline">\(p\times 1\)</span> coefficient vector, and <span class="math inline">\(\varepsilon\)</span> is <span class="math inline">\(n\times 1\)</span> error vector.</p>
<p>In most textbooks the solution for estimating <span class="math inline">\(\beta\)</span>, whether it be via maximum likelihood or least squares, is written as</p>
<p><span class="math display">\[
\hat{\beta} = (X^\prime X)^{-1}X^\prime y.
\]</span> And indeed, that <em>is</em> the solution. In R, this could be translated literally as</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">betahat &lt;-<span class="st"> </span><span class="kw">solve</span>(<span class="kw">t</span>(X) <span class="op">%*%</span><span class="st"> </span>X) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(X) <span class="op">%*%</span><span class="st"> </span>y</code></pre></div>
<p>where <code>solve()</code> is used to invert the cross product matrix <span class="math inline">\(X^\prime X\)</span>. However, one would never compute the actual value of <span class="math inline">\(\hat{\beta}\)</span> this way on the computer. The formula presented above is <em>only</em> computed in textbooks.</p>
<p>The primary reason is that computing the direct inverse of <span class="math inline">\(X^\prime X\)</span> is very expensive computationally and is a potentially unstable operation on a computer when there is high colinearity amongst the predictors. Furthermore, in computing <span class="math inline">\(\hat{\beta}\)</span> we do not actually need the inverse of <span class="math inline">\(X^\prime X\)</span>, so why compute it? A simpler approach would be to take the normal equations, <span class="math display">\[
X^\prime X\beta = X^\prime y
\]</span> and solve them directly. In R, we could write</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">solve</span>(<span class="kw">crossprod</span>(X), <span class="kw">crossprod</span>(X, y))</code></pre></div>
<p>Rather than compute the inverse of <span class="math inline">\(X^\prime X\)</span>, we directly compute <span class="math inline">\(\hat{\beta}\)</span> via <a href="https://en.wikipedia.org/wiki/LU_decomposition">Gaussian elimination</a>. This approach has the benefit of being more numerically stable and being <em>much</em> faster.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">2017</span><span class="op">-</span><span class="dv">07</span><span class="op">-</span><span class="dv">13</span>)
X &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(<span class="dv">5000</span> <span class="op">*</span><span class="st"> </span><span class="dv">100</span>), <span class="dv">5000</span>, <span class="dv">100</span>)
y &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">5000</span>)</code></pre></div>
<p>Here we benchmark the naive computation.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(microbenchmark)
<span class="kw">microbenchmark</span>(<span class="kw">solve</span>(<span class="kw">t</span>(X) <span class="op">%*%</span><span class="st"> </span>X) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(X) <span class="op">%*%</span><span class="st"> </span>y)</code></pre></div>
<pre><code>Unit: milliseconds
                             expr      min       lq     mean   median
 solve(t(X) %*% X) %*% t(X) %*% y 47.56642 51.34981 58.33341 54.22329
       uq     max neval
 58.83291 101.612   100</code></pre>
<p>The following timing uses the <code>solve()</code> function to compute <span class="math inline">\(\hat{\beta}\)</span> via Gaussian elimination.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">microbenchmark</span>(<span class="kw">solve</span>(<span class="kw">t</span>(X) <span class="op">%*%</span><span class="st"> </span>X) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(X) <span class="op">%*%</span><span class="st"> </span>y,
               <span class="kw">solve</span>(<span class="kw">crossprod</span>(X), <span class="kw">crossprod</span>(X, y)))</code></pre></div>
<pre><code>Unit: milliseconds
                                 expr      min       lq     mean   median
     solve(t(X) %*% X) %*% t(X) %*% y 45.96281 48.84230 53.48634 50.52306
 solve(crossprod(X), crossprod(X, y)) 22.24667 22.45159 22.94948 22.70641
       uq       max neval cld
 53.32787 148.98025   100   b
 23.12598  29.17004   100  a </code></pre>
<p>You can see that the betweeen the two approach there is a more than 5-fold difference in computation time, with the second approach being considerably faster.</p>
<p>However, this approach breaks down when there is any colinearity in the <span class="math inline">\(X\)</span> matrix. For example, we can tack on a column to <span class="math inline">\(X\)</span> that is very similar (but not identical) to the first column of <span class="math inline">\(X\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">W &lt;-<span class="st"> </span><span class="kw">cbind</span>(X, X[, <span class="dv">1</span>] <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">5000</span>, <span class="dt">sd =</span> <span class="fl">0.0000000001</span>))
<span class="kw">solve</span>(<span class="kw">crossprod</span>(W), <span class="kw">crossprod</span>(W, y))</code></pre></div>
<pre><code>                [,1]
  [1,] -1.009751e+03
  [2,] -8.039783e-03
  [3,]  4.684944e-03
  [4,]  2.700024e-02
  [5,]  6.355835e-03
  [6,]  8.693650e-03
  [7,]  1.440320e-02
  [8,] -1.433221e-02
  [9,] -9.035157e-03
 [10,] -2.170287e-02
 [11,] -1.737509e-02
 [12,]  1.681501e-02
 [13,]  2.223502e-03
 [14,] -1.373478e-02
 [15,]  1.167108e-03
 [16,] -5.869775e-03
 [17,] -6.319076e-04
 [18,]  2.556232e-03
 [19,] -4.057100e-03
 [20,] -6.623820e-04
 [21,] -3.384654e-03
 [22,]  1.509237e-02
 [23,]  5.172755e-04
 [24,] -1.656931e-02
 [25,] -1.321050e-02
 [26,] -6.083916e-05
 [27,]  1.027881e-02
 [28,]  4.948704e-03
 [29,]  1.012277e-02
 [30,]  4.484213e-03
 [31,] -1.133879e-02
 [32,]  1.723859e-03
 [33,]  3.625681e-03
 [34,]  7.376709e-03
 [35,]  1.794856e-02
 [36,]  1.547454e-02
 [37,] -2.234061e-02
 [38,]  1.443603e-02
 [39,] -2.052446e-02
 [40,] -2.553873e-03
 [41,]  2.351903e-02
 [42,]  2.261801e-02
 [43,]  2.021437e-02
 [44,]  6.262966e-03
 [45,]  7.463015e-03
 [46,] -1.977186e-02
 [47,] -6.093681e-02
 [48,] -8.966449e-03
 [49,] -8.477453e-03
 [50,] -1.863702e-02
 [51,] -3.808376e-03
 [52,]  7.120105e-03
 [53,]  2.219219e-03
 [54,]  4.041155e-03
 [55,]  1.878475e-02
 [56,] -3.440053e-02
 [57,] -6.169164e-03
 [58,]  1.496643e-03
 [59,] -8.046797e-03
 [60,] -7.373807e-03
 [61,]  3.833052e-03
 [62,]  2.784244e-03
 [63,]  8.004059e-03
 [64,] -1.676562e-04
 [65,] -1.035378e-02
 [66,] -3.802059e-03
 [67,]  1.025505e-02
 [68,]  9.116551e-03
 [69,] -7.395200e-03
 [70,] -1.505831e-02
 [71,]  1.617219e-02
 [72,]  2.888739e-02
 [73,] -2.593069e-02
 [74,]  2.479187e-02
 [75,] -7.660286e-03
 [76,]  1.043432e-02
 [77,]  7.832101e-03
 [78,] -1.891912e-02
 [79,]  7.746156e-03
 [80,] -2.321135e-03
 [81,]  7.036778e-03
 [82,]  1.592099e-02
 [83,] -1.004987e-02
 [84,] -7.116285e-03
 [85,]  3.188724e-03
 [86,] -1.686812e-02
 [87,] -3.881375e-03
 [88,]  7.061706e-03
 [89,]  7.478217e-03
 [90,]  9.793769e-03
 [91,] -2.878374e-02
 [92,]  1.151801e-02
 [93,] -6.219824e-03
 [94,] -2.377119e-02
 [95,]  1.127488e-02
 [96,] -5.908337e-03
 [97,]  2.185800e-02
 [98,] -1.256333e-02
 [99,]  1.826857e-02
[100,] -6.703255e-03
[101,]  1.009750e+03</code></pre>
<p>Now the approach doesn’t work because the cross product matrix <span class="math inline">\(W^\prime W\)</span> is singular. In practice, matrices like these can come up a lot in data analysis and it would be useful to have a way to deal with it automatically.</p>
<p>R takes a different approach to solving for the unknown coefficients in a linear model. R uses the <a href="https://en.wikipedia.org/wiki/QR_decomposition">QR decomposition</a>, which is not as fast, but has the added benefit of being able to automatically detect and handle colinear columns in the matrix.</p>
<p>Here, we use the fact that <span class="math inline">\(X\)</span> can be decomposed as <span class="math inline">\(X = QR\)</span>, where <span class="math inline">\(Q\)</span> is an orthonormal matrix and <span class="math inline">\(R\)</span> is an upper triangular matrix. Given that, we can write</p>
<span class="math display">\[
X^\prime X\beta = X^\prime y
\]</span> as
<span class="math display">\[\begin{eqnarray*}
R^\prime Q^\prime QR\beta &amp; = &amp; R^\prime Q^\prime y\\
R^\prime R\beta &amp; = &amp; R^\prime Q^\prime y\\
R\beta &amp; = &amp; Q^\prime y
\end{eqnarray*}\]</span>
<p>because <span class="math inline">\(Q^\prime Q = I\)</span>. At this point, we can solve for <span class="math inline">\(\beta\)</span> via Gaussian elimination, which is greatly simplified because <span class="math inline">\(R\)</span> is already upper triangular. The QR decomposition has the added benefit that we do not have to compute the cross product <span class="math inline">\(X^\prime X\)</span> at all, as this matrix can be numericaly unstable if it is not properly centered or scaled.</p>
<p>We can see in R code that even with our singular matrix <code>W</code> above, the QR decomposition continues without error.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Qw &lt;-<span class="st"> </span><span class="kw">qr</span>(W)
<span class="kw">str</span>(Qw)</code></pre></div>
<pre><code>List of 4
 $ qr   : num [1:5000, 1:101] 70.88664 -0.01277 0.01561 0.00158 0.02451 ...
 $ rank : int 100
 $ qraux: num [1:101] 1.01 1.03 1.01 1.02 1.02 ...
 $ pivot: int [1:101] 1 2 3 4 5 6 7 8 9 10 ...
 - attr(*, &quot;class&quot;)= chr &quot;qr&quot;</code></pre>
<p>Note that the output of <code>qr()</code> computes the rank of <span class="math inline">\(W\)</span> to be 100, not 101, because of the colinear column. From there, we can get <span class="math inline">\(\hat{\beta}\)</span> if we want using <code>qr.coef()</code>,</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">betahat &lt;-<span class="st"> </span><span class="kw">qr.coef</span>(Qw, y)</code></pre></div>
<p>We do not show it here, but the very last element of <code>betahat</code> is <code>NA</code> because a coefficient corresponding to the last column of <span class="math inline">\(W\)</span> (the collinear column) could not be calculated.</p>
<p>While the QR decomposition does handle colinearity, we do pay a price in speed.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ggplot2)
m &lt;-<span class="st"> </span><span class="kw">microbenchmark</span>(<span class="kw">solve</span>(<span class="kw">t</span>(X) <span class="op">%*%</span><span class="st"> </span>X) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(X) <span class="op">%*%</span><span class="st"> </span>y,
                    <span class="kw">solve</span>(<span class="kw">crossprod</span>(X), <span class="kw">crossprod</span>(X, y)),
                    <span class="kw">qr.coef</span>(<span class="kw">qr</span>(X), y))
<span class="kw">autoplot</span>(m)</code></pre></div>
<p><img src="image/unnamed-chunk-8-1.png" width="672" /></p>
<p>Compared to the approaches above, it is comparable to the naive approach but it is a much better and more stable method.</p>
<p>In practice, we do not use functions like <code>qr()</code> or <code>qr.coef()</code> directly because higher level functions like <code>lm()</code> do the work for us. However, for certain narrow, highly optimized cases, it may be fruitful to turn to another matrix decomposition to compute linear regression coefficients, particularly if this must be done repeatedly in a loop.</p>
</div>
<div id="multivariate-normal-distribution" class="section level3">
<h3><span class="header-section-number">1.3.3</span> Multivariate Normal Distribution</h3>
<p>Computing the multivariate normal density is a common problem in statistics, such as in fitting spatial statistical models or Gaussian process models. Because optimization procedures used to compute maximum likelihood estimates or likelihood ratios can be evaluated hundreds or thousands of times in a single run, it’s useful to have a highly efficient procedure for evaluating the multivariate Normal density.</p>
<p>The <span class="math inline">\(p\)</span>-dimensional multivariate Normal density is written as</p>
<p><span class="math display">\[
\varphi(x\mid\mu,\Sigma)
=
-\frac{p}{2}\log 2\pi-\frac{1}{2}\log|\Sigma| - \frac{1}{2}(x-\mu)^\prime\Sigma^{-1}(x-\mu)
\]</span></p>
<p>The critical, and most time-consuming, part of computing the multivariate Normal density is the quadratic form, <span class="math display">\[
(x-\mu)^\prime\Sigma^{-1}(x-\mu).
\]</span> We can simplify this problem a bit by focusing on the centered version of <span class="math inline">\(x\)</span> which we will refer to as <span class="math inline">\(z=x-\mu\)</span>. Hence, we are trying to compute <span class="math display">\[
z^\prime\Sigma^{-1}z
\]</span></p>
<p>Here, much like the linear regression example above, the key bottleneck is the inversion of the <span class="math inline">\(p\)</span>-dimensional covariance matrix <span class="math inline">\(\Sigma\)</span>. If we take <span class="math inline">\(z\)</span> to be a <span class="math inline">\(p\times 1\)</span> column vector, then a literal translation of the mathematics into R code might look something like this,</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">t</span>(z) <span class="op">%*%</span><span class="st"> </span><span class="kw">solve</span>(Sigma) <span class="op">%*%</span><span class="st"> </span>z</code></pre></div>
<p>But once again, we are taking on the difficult and unstable task of inverting <span class="math inline">\(\Sigma\)</span> when, at the end of the day, we do not need this inverse.</p>
<p>Instead of taking the textbook translation approach, we can make use of the Cholesky decomposition of <span class="math inline">\(\Sigma\)</span>. The Cholesky decomposition of a positive definite matrix provides</p>
<p><span class="math display">\[
\Sigma = R^\prime R
\]</span></p>
<p>where <span class="math inline">\(R\)</span> is an upper triangular matrix. <span class="math inline">\(R\)</span> is sometimes referred to as the “square root” of <span class="math inline">\(\Sigma\)</span> (although it is not unique). Using the Cholesky decomposition of <span class="math inline">\(\Sigma\)</span> and the rules of matrix algebra, we can then write</p>
<span class="math display">\[\begin{eqnarray*}
z^\prime\Sigma^{-1}z 
&amp; = &amp; 
z^\prime (R^\prime R)^{-1}z\\
&amp; = &amp; 
z^\prime R^{-1}R^{\prime -1}z\\
&amp; = &amp;
(R^{\prime -1}z)^\prime R^{\prime -1}z\\
&amp; = &amp;
v^\prime v
\end{eqnarray*}\]</span>
<p>where <span class="math inline">\(v = R^{\prime -1} z\)</span> and is a <span class="math inline">\(p\times 1\)</span> vector. Furthermore, we can avoid inverting <span class="math inline">\(R^\prime\)</span> by computing <span class="math inline">\(v\)</span> as the solution to the linear system <span class="math display">\[
R^\prime v = z
\]</span> Once we have computed <span class="math inline">\(v\)</span>, we can compute the quadratic form as <span class="math inline">\(v^\prime v\)</span>, which is simply the cross product of two <span class="math inline">\(p\)</span>-dimensional vectors!</p>
<p>Another benefit of the Cholesky decomposition is that it gives us a simple way to compute the log-determinant of <span class="math inline">\(\Sigma\)</span>. The log-determinant of <span class="math inline">\(\Sigma\)</span> is simply <span class="math inline">\(2\)</span> times the sum of the log of the diagonal elements of <span class="math inline">\(R\)</span>.</p>
<p>Here is an implementation of the naive approach to computing the quadratic form in the multivariate Normal.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">2017</span><span class="op">-</span><span class="dv">07</span><span class="op">-</span><span class="dv">13</span>)
z &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(<span class="dv">200</span> <span class="op">*</span><span class="st"> </span><span class="dv">100</span>), <span class="dv">200</span>, <span class="dv">100</span>)
S &lt;-<span class="st"> </span><span class="kw">cov</span>(z)
quad.naive &lt;-<span class="st"> </span><span class="cf">function</span>(z, S) {
        Sinv &lt;-<span class="st"> </span><span class="kw">solve</span>(S)
        <span class="kw">rowSums</span>((z <span class="op">%*%</span><span class="st"> </span>Sinv) <span class="op">*</span><span class="st"> </span>z)
}</code></pre></div>
<p>We can first take a look at the output that this function produces.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(dplyr)
<span class="kw">quad.naive</span>(z, S) <span class="op">%&gt;%</span><span class="st"> </span>summary</code></pre></div>
<pre><code>   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  73.57   93.54  100.59  100.34  106.39  129.12 </code></pre>
<p>The following is a version of the quadratic form function that uses the Cholesky decomposition.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">quad.chol &lt;-<span class="st"> </span><span class="cf">function</span>(z, S) {
        R &lt;-<span class="st"> </span><span class="kw">chol</span>(S)
        v &lt;-<span class="st"> </span><span class="kw">backsolve</span>(R, <span class="kw">t</span>(z), <span class="dt">transpose =</span> <span class="ot">TRUE</span>)
        <span class="kw">colSums</span>(v <span class="op">*</span><span class="st"> </span>v)
}</code></pre></div>
<p>We can verify that this function produces the same output as the naive version.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">quad.chol</span>(z, S) <span class="op">%&gt;%</span><span class="st"> </span>summary</code></pre></div>
<pre><code>   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  73.57   93.54  100.59  100.34  106.39  129.12 </code></pre>
<p>Now, we can time both procedures to see how they perform.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(microbenchmark)
<span class="kw">microbenchmark</span>(<span class="kw">quad.naive</span>(z, S), <span class="kw">quad.chol</span>(z, S))</code></pre></div>
<pre><code>Unit: microseconds
             expr      min       lq     mean   median      uq      max
 quad.naive(z, S) 1612.812 1736.153 2006.144 1824.102 1992.39 4435.319
  quad.chol(z, S)  936.118  977.704 1148.600 1042.648 1203.15 3799.679
 neval cld
   100   b
   100  a </code></pre>
<p>We can see that the version using the Cholesky decomposition takes about 60% of the time of the naive version. In a single evaluation, this may not amount to much time. However, over the course of potentially many iterations, these kinds of small savings can add up.</p>
<p>The key lesson here is that our use of the Cholesky decomposition takes advantage of the fact that we know that the covariance matrix in a multivariate Normal is symmetric and positive definite. The naive version of the algorithm that just blindly inverts the covariance matrix is not able to take advantage of this information.</p>

</div>
</div>
<!-- </div> -->
            </section>

          </div>
        </div>
      </div>
<a href="principle-of-optimization-transfer.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="solving-nonlinear-equations.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
