<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Advanced Statistical Computing</title>
  <meta name="description" content="The book covers material taught in the Johns Hopkins Biostatistics Advanced Statistical Computing course.">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="Advanced Statistical Computing" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://github.com/rdpeng/advstatcomp" />
  <meta property="og:image" content="https://github.com/rdpeng/advstatcompcover-image.png" />
  <meta property="og:description" content="The book covers material taught in the Johns Hopkins Biostatistics Advanced Statistical Computing course." />
  <meta name="github-repo" content="rdpeng/advstatcomp" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Advanced Statistical Computing" />
  
  <meta name="twitter:description" content="The book covers material taught in the Johns Hopkins Biostatistics Advanced Statistical Computing course." />
  <meta name="twitter:image" content="https://github.com/rdpeng/advstatcompcover-image.png" />

<meta name="author" content="Roger D. Peng">


<meta name="date" content="2017-12-12">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="non-uniform-random-numbers.html">
<link rel="next" href="importance-sampling.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="css\style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Advanced Statistical Computing</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a><ul>
<li class="chapter" data-level="" data-path="stay-in-touch.html"><a href="stay-in-touch.html"><i class="fa fa-check"></i>Stay in Touch!</a></li>
<li class="chapter" data-level="" data-path="setup.html"><a href="setup.html"><i class="fa fa-check"></i>Setup</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="example-linear-models.html"><a href="example-linear-models.html"><i class="fa fa-check"></i><b>1.1</b> Example: Linear Models</a></li>
<li class="chapter" data-level="1.2" data-path="principle-of-optimization-transfer.html"><a href="principle-of-optimization-transfer.html"><i class="fa fa-check"></i><b>1.2</b> Principle of Optimization Transfer</a></li>
<li class="chapter" data-level="1.3" data-path="textbooks-vs-computers.html"><a href="textbooks-vs-computers.html"><i class="fa fa-check"></i><b>1.3</b> Textbooks vs. Computers</a><ul>
<li class="chapter" data-level="1.3.1" data-path="textbooks-vs-computers.html"><a href="textbooks-vs-computers.html#using-logarithms"><i class="fa fa-check"></i><b>1.3.1</b> Using Logarithms</a></li>
<li class="chapter" data-level="1.3.2" data-path="textbooks-vs-computers.html"><a href="textbooks-vs-computers.html#linear-regression"><i class="fa fa-check"></i><b>1.3.2</b> Linear Regression</a></li>
<li class="chapter" data-level="1.3.3" data-path="textbooks-vs-computers.html"><a href="textbooks-vs-computers.html#multivariate-normal-distribution"><i class="fa fa-check"></i><b>1.3.3</b> Multivariate Normal Distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="solving-nonlinear-equations.html"><a href="solving-nonlinear-equations.html"><i class="fa fa-check"></i><b>2</b> Solving Nonlinear Equations</a><ul>
<li class="chapter" data-level="2.1" data-path="bisection-algorithm.html"><a href="bisection-algorithm.html"><i class="fa fa-check"></i><b>2.1</b> Bisection Algorithm</a><ul>
<li class="chapter" data-level="2.1.1" data-path="bisection-algorithm.html"><a href="bisection-algorithm.html#example-quantiles"><i class="fa fa-check"></i><b>2.1.1</b> Example: Quantiles</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="rates-of-convergence.html"><a href="rates-of-convergence.html"><i class="fa fa-check"></i><b>2.2</b> Rates of Convergence</a><ul>
<li class="chapter" data-level="2.2.1" data-path="rates-of-convergence.html"><a href="rates-of-convergence.html#linear-convergence"><i class="fa fa-check"></i><b>2.2.1</b> Linear convergence</a></li>
<li class="chapter" data-level="2.2.2" data-path="rates-of-convergence.html"><a href="rates-of-convergence.html#superlinear-convergence"><i class="fa fa-check"></i><b>2.2.2</b> Superlinear Convergence</a></li>
<li class="chapter" data-level="2.2.3" data-path="rates-of-convergence.html"><a href="rates-of-convergence.html#quadratic-convergence"><i class="fa fa-check"></i><b>2.2.3</b> Quadratic Convergence</a></li>
<li class="chapter" data-level="2.2.4" data-path="rates-of-convergence.html"><a href="rates-of-convergence.html#example-bisection-algorithm"><i class="fa fa-check"></i><b>2.2.4</b> Example: Bisection Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="functional-iteration.html"><a href="functional-iteration.html"><i class="fa fa-check"></i><b>2.3</b> Functional Iteration</a><ul>
<li class="chapter" data-level="2.3.1" data-path="functional-iteration.html"><a href="functional-iteration.html#the-shrinking-lemma"><i class="fa fa-check"></i><b>2.3.1</b> The Shrinking Lemma</a></li>
<li class="chapter" data-level="2.3.2" data-path="functional-iteration.html"><a href="functional-iteration.html#convergence-rates-for-shrinking-maps"><i class="fa fa-check"></i><b>2.3.2</b> Convergence Rates for Shrinking Maps</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="newtons-method.html"><a href="newtons-method.html"><i class="fa fa-check"></i><b>2.4</b> Newton’s Method</a><ul>
<li class="chapter" data-level="2.4.1" data-path="newtons-method.html"><a href="newtons-method.html#proof-of-newtons-method"><i class="fa fa-check"></i><b>2.4.1</b> Proof of Newton’s Method</a></li>
<li class="chapter" data-level="2.4.2" data-path="newtons-method.html"><a href="newtons-method.html#convergence-rate-of-newtons-method"><i class="fa fa-check"></i><b>2.4.2</b> Convergence Rate of Newton’s Method</a></li>
<li class="chapter" data-level="2.4.3" data-path="newtons-method.html"><a href="newtons-method.html#newtons-method-for-maximum-likelihood-estimation"><i class="fa fa-check"></i><b>2.4.3</b> Newton’s Method for Maximum Likelihood Estimation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="general-optimization.html"><a href="general-optimization.html"><i class="fa fa-check"></i><b>3</b> General Optimization</a><ul>
<li class="chapter" data-level="3.1" data-path="steepest-descent.html"><a href="steepest-descent.html"><i class="fa fa-check"></i><b>3.1</b> Steepest Descent</a><ul>
<li class="chapter" data-level="3.1.1" data-path="steepest-descent.html"><a href="steepest-descent.html#example-multivariate-normal"><i class="fa fa-check"></i><b>3.1.1</b> Example: Multivariate Normal</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="the-newton-direction.html"><a href="the-newton-direction.html"><i class="fa fa-check"></i><b>3.2</b> The Newton Direction</a><ul>
<li class="chapter" data-level="3.2.1" data-path="the-newton-direction.html"><a href="the-newton-direction.html#generalized-linear-models"><i class="fa fa-check"></i><b>3.2.1</b> Generalized Linear Models</a></li>
<li class="chapter" data-level="3.2.2" data-path="the-newton-direction.html"><a href="the-newton-direction.html#newtons-method-in-r"><i class="fa fa-check"></i><b>3.2.2</b> Newton’s Method in R</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="quasi-newton.html"><a href="quasi-newton.html"><i class="fa fa-check"></i><b>3.3</b> Quasi-Newton</a><ul>
<li class="chapter" data-level="3.3.1" data-path="quasi-newton.html"><a href="quasi-newton.html#quasi-newton-methods-in-r"><i class="fa fa-check"></i><b>3.3.1</b> Quasi-Newton Methods in R</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="conjugate-gradient.html"><a href="conjugate-gradient.html"><i class="fa fa-check"></i><b>3.4</b> Conjugate Gradient</a></li>
<li class="chapter" data-level="3.5" data-path="coordinate-descent.html"><a href="coordinate-descent.html"><i class="fa fa-check"></i><b>3.5</b> Coordinate Descent</a><ul>
<li class="chapter" data-level="3.5.1" data-path="coordinate-descent.html"><a href="coordinate-descent.html#convergence-rates"><i class="fa fa-check"></i><b>3.5.1</b> Convergence Rates</a></li>
<li class="chapter" data-level="3.5.2" data-path="coordinate-descent.html"><a href="coordinate-descent.html#generalized-additive-models"><i class="fa fa-check"></i><b>3.5.2</b> Generalized Additive Models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="the-em-algorithm.html"><a href="the-em-algorithm.html"><i class="fa fa-check"></i><b>4</b> The EM Algorithm</a><ul>
<li class="chapter" data-level="4.1" data-path="em-algorithm-for-exponential-families.html"><a href="em-algorithm-for-exponential-families.html"><i class="fa fa-check"></i><b>4.1</b> EM Algorithm for Exponential Families</a></li>
<li class="chapter" data-level="4.2" data-path="canonical-examples.html"><a href="canonical-examples.html"><i class="fa fa-check"></i><b>4.2</b> Canonical Examples</a><ul>
<li class="chapter" data-level="4.2.1" data-path="canonical-examples.html"><a href="canonical-examples.html#two-part-normal-mixture-model"><i class="fa fa-check"></i><b>4.2.1</b> Two-Part Normal Mixture Model</a></li>
<li class="chapter" data-level="4.2.2" data-path="canonical-examples.html"><a href="canonical-examples.html#censored-exponential-data"><i class="fa fa-check"></i><b>4.2.2</b> Censored Exponential Data</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="a-minorizing-function.html"><a href="a-minorizing-function.html"><i class="fa fa-check"></i><b>4.3</b> A Minorizing Function</a><ul>
<li class="chapter" data-level="4.3.1" data-path="a-minorizing-function.html"><a href="a-minorizing-function.html#example-minorization-in-a-two-part-mixture-model"><i class="fa fa-check"></i><b>4.3.1</b> Example: Minorization in a Two-Part Mixture Model</a></li>
<li class="chapter" data-level="4.3.2" data-path="a-minorizing-function.html"><a href="a-minorizing-function.html#constrained-minimization-with-and-adaptive-barrier"><i class="fa fa-check"></i><b>4.3.2</b> Constrained Minimization With and Adaptive Barrier</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="missing-information-principle.html"><a href="missing-information-principle.html"><i class="fa fa-check"></i><b>4.4</b> Missing Information Principle</a></li>
<li class="chapter" data-level="4.5" data-path="acceleration-methods.html"><a href="acceleration-methods.html"><i class="fa fa-check"></i><b>4.5</b> Acceleration Methods</a><ul>
<li class="chapter" data-level="4.5.1" data-path="acceleration-methods.html"><a href="acceleration-methods.html#louiss-acceleration"><i class="fa fa-check"></i><b>4.5.1</b> Louis’s Acceleration</a></li>
<li class="chapter" data-level="4.5.2" data-path="acceleration-methods.html"><a href="acceleration-methods.html#squarem"><i class="fa fa-check"></i><b>4.5.2</b> SQUAREM</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="integration.html"><a href="integration.html"><i class="fa fa-check"></i><b>5</b> Integration</a><ul>
<li class="chapter" data-level="5.1" data-path="laplace-approximation.html"><a href="laplace-approximation.html"><i class="fa fa-check"></i><b>5.1</b> Laplace Approximation</a><ul>
<li class="chapter" data-level="5.1.1" data-path="laplace-approximation.html"><a href="laplace-approximation.html#computing-the-posterior-mean"><i class="fa fa-check"></i><b>5.1.1</b> Computing the Posterior Mean</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="variational-inference.html"><a href="variational-inference.html"><i class="fa fa-check"></i><b>5.2</b> Variational Inference</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="independent-monte-carlo.html"><a href="independent-monte-carlo.html"><i class="fa fa-check"></i><b>6</b> Independent Monte Carlo</a><ul>
<li class="chapter" data-level="6.1" data-path="random-number-generation.html"><a href="random-number-generation.html"><i class="fa fa-check"></i><b>6.1</b> Random Number Generation</a><ul>
<li class="chapter" data-level="6.1.1" data-path="random-number-generation.html"><a href="random-number-generation.html#pseudo-random-numbers"><i class="fa fa-check"></i><b>6.1.1</b> Pseudo-random Numbers</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="non-uniform-random-numbers.html"><a href="non-uniform-random-numbers.html"><i class="fa fa-check"></i><b>6.2</b> Non-Uniform Random Numbers</a><ul>
<li class="chapter" data-level="6.2.1" data-path="non-uniform-random-numbers.html"><a href="non-uniform-random-numbers.html#inverse-cdf-transformation"><i class="fa fa-check"></i><b>6.2.1</b> Inverse CDF Transformation</a></li>
<li class="chapter" data-level="6.2.2" data-path="non-uniform-random-numbers.html"><a href="non-uniform-random-numbers.html#other-transformations"><i class="fa fa-check"></i><b>6.2.2</b> Other Transformations</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="rejection-sampling.html"><a href="rejection-sampling.html"><i class="fa fa-check"></i><b>6.3</b> Rejection Sampling</a><ul>
<li class="chapter" data-level="6.3.1" data-path="rejection-sampling.html"><a href="rejection-sampling.html#the-algorithm"><i class="fa fa-check"></i><b>6.3.1</b> The Algorithm</a></li>
<li class="chapter" data-level="6.3.2" data-path="rejection-sampling.html"><a href="rejection-sampling.html#properties-of-rejection-sampling"><i class="fa fa-check"></i><b>6.3.2</b> Properties of Rejection Sampling</a></li>
<li class="chapter" data-level="6.3.3" data-path="rejection-sampling.html"><a href="rejection-sampling.html#empirical-supremum-rejection-sampling"><i class="fa fa-check"></i><b>6.3.3</b> Empirical Supremum Rejection Sampling</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="importance-sampling.html"><a href="importance-sampling.html"><i class="fa fa-check"></i><b>6.4</b> Importance Sampling</a><ul>
<li class="chapter" data-level="6.4.1" data-path="importance-sampling.html"><a href="importance-sampling.html#example-bayesian-sensitivity-analysis"><i class="fa fa-check"></i><b>6.4.1</b> Example: Bayesian Sensitivity Analysis</a></li>
<li class="chapter" data-level="6.4.2" data-path="importance-sampling.html"><a href="importance-sampling.html#example-calculating-marginal-likelihoods"><i class="fa fa-check"></i><b>6.4.2</b> Example: Calculating Marginal Likelihoods</a></li>
<li class="chapter" data-level="6.4.3" data-path="importance-sampling.html"><a href="importance-sampling.html#properties-of-the-importance-sampling-estimator"><i class="fa fa-check"></i><b>6.4.3</b> Properties of the Importance Sampling Estimator</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html"><i class="fa fa-check"></i><b>7</b> Markov Chain Monte Carlo</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Advanced Statistical Computing</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="rejection-sampling" class="section level2">
<h2><span class="header-section-number">6.3</span> Rejection Sampling</h2>
<p>What do we do if we want to generate samples of a random variable with density <span class="math inline">\(f\)</span> and there isn’t a built in function for doing this? If the random variable is of a reasonably low dimension (less than <span class="math inline">\(10\)</span>?), then <em>rejection sampling</em> is a plausible general approach.</p>
<p>The idea of rejection sampling is that although we cannot easily sample from <span class="math inline">\(f\)</span>, there exists another density <span class="math inline">\(g\)</span>, like a Normal distribution or perhaps a <span class="math inline">\(t\)</span>-distribution, from which it is easy for us to sample (because there’s a built in function or someone else wrote a nice function). Then we can sample from <span class="math inline">\(g\)</span> directly and then “reject” the samples in a strategic way to make the resulting “non-rejected” samples look like they came from <span class="math inline">\(f\)</span>. The density <span class="math inline">\(g\)</span> will be referred to as the “candidate density” and <span class="math inline">\(f\)</span> will be the “target density”.</p>
<p>In order to use the rejections sampling algorithm, we must first ensure that the support of <span class="math inline">\(f\)</span> is a subset of the support of <span class="math inline">\(g\)</span>. If <span class="math inline">\(\mathcal{X}_f\)</span> is the support of <span class="math inline">\(f\)</span> and <span class="math inline">\(\mathcal{X}_g\)</span> is the support of <span class="math inline">\(g\)</span>, then we must have <span class="math inline">\(\mathcal{X}_f\subset\mathcal{X}_g\)</span>. This makes sense: if there’s a region of the support of <span class="math inline">\(f\)</span> that <span class="math inline">\(g\)</span> can never touch, then that area will never get sampled. In addition, we must assume that <span class="math display">\[
c = \sup_{x\in\mathcal{X}_f} \frac{f(x)}{g(x)} &lt; \infty
\]</span> and that we can calculate <span class="math inline">\(c\)</span>. The easiest way to satisfy this assumption is to make sure that <span class="math inline">\(g\)</span> has heavier tails than <span class="math inline">\(f\)</span>. We cannot have that <span class="math inline">\(g\)</span> decreases at a faster rate than <span class="math inline">\(f\)</span> in the tails or else rejection sampling will not work.</p>
<div id="the-algorithm" class="section level3">
<h3><span class="header-section-number">6.3.1</span> The Algorithm</h3>
<p>The rejection sampling algorithm for drawing a sample from the target density <span class="math inline">\(f\)</span> is then</p>
<ol style="list-style-type: decimal">
<li><p>Simulate <span class="math inline">\(U\sim\text{Unif}(0, 1)\)</span>.</p></li>
<li><p>Simulate a candidate <span class="math inline">\(X\sim g\)</span> from the candidate density</p></li>
<li><p>If <span class="math display">\[
U\leq\frac{f(X)}{c\,g(X)}
\]</span> then “accept” the candidate <span class="math inline">\(X\)</span>. Otherwise, “reject” <span class="math inline">\(X\)</span> and go back to the beginning.</p></li>
</ol>
<p>The algorithm can be repeated until the desired number of samples from the target density <span class="math inline">\(f\)</span> has been accepted.</p>
<p>As a simple example, suppose we wanted to generate samples from a <span class="math inline">\(\mathcal{N}(0, 1)\)</span> density. We could use the <span class="math inline">\(t_2\)</span> distribution as our candidate density as it has heavier tails than the Normal. Plotting those two densities, along with a sample from the <span class="math inline">\(t_2\)</span> density gives us the picture below.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">2017</span><span class="op">-</span><span class="dv">12</span><span class="op">-</span><span class="dv">4</span>)
<span class="kw">curve</span>(<span class="kw">dnorm</span>(x), <span class="op">-</span><span class="dv">6</span>, <span class="dv">6</span>, <span class="dt">xlab =</span> <span class="st">&quot;x&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Density&quot;</span>, <span class="dt">n =</span> <span class="dv">200</span>)
<span class="kw">curve</span>(<span class="kw">dt</span>(x, <span class="dv">2</span>), <span class="op">-</span><span class="dv">6</span>, <span class="dv">6</span>, <span class="dt">add =</span> <span class="ot">TRUE</span>, <span class="dt">col =</span> <span class="dv">4</span>, <span class="dt">n =</span> <span class="dv">200</span>)
<span class="kw">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="kw">c</span>(<span class="st">&quot;Normal density&quot;</span>, <span class="st">&quot;t density&quot;</span>), 
       <span class="dt">col =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">4</span>), <span class="dt">bty =</span> <span class="st">&quot;n&quot;</span>, <span class="dt">lty =</span> <span class="dv">1</span>)
x &lt;-<span class="st"> </span><span class="kw">rt</span>(<span class="dv">200</span>, <span class="dv">2</span>)
<span class="kw">rug</span>(x, <span class="dt">col =</span> <span class="dv">4</span>)</code></pre></div>
<p><img src="independentMC_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>Given what we know about the standard Normal density, most of the samples should be between <span class="math inline">\(-3\)</span> and <span class="math inline">\(+3\)</span>, except perhaps in very large samples (this is a sample of size 200). From the picture, there are samples in the range of <span class="math inline">\(4\)</span>–<span class="math inline">\(6\)</span>. In order to transform the <span class="math inline">\(t_2\)</span> samples into <span class="math inline">\(\mathcal{N}(0, 1)\)</span> samples, we will need to reject many of the samples out in the tail. On the other hand, there are two <em>few</em> samples in the range of <span class="math inline">\([-2, 2]\)</span> and so we will have to disproportionaly accept samples in that range until it represents the proper <span class="math inline">\(\mathcal{N}(0, 1)\)</span> density.</p>
<p>Before we move on, it’s worth noting that the rejection sampling method requires that we can <em>evaluate</em> the target density <span class="math inline">\(f\)</span>. That is how we compute the rejection/acceptance ratio in Step 2. In most cases, this will not be a problem.</p>
</div>
<div id="properties-of-rejection-sampling" class="section level3">
<h3><span class="header-section-number">6.3.2</span> Properties of Rejection Sampling</h3>
<p>One property of the rejection sampling algorithm is that the number of draws we need to take from the candidate density <span class="math inline">\(g\)</span> before we accept a candidate is a geometric random variable with success probability <span class="math inline">\(1 / c\)</span>. We can think of the decision to accept or reject a candidate as a sequence of iid coin flips that has a specific probability of coming up “heads” (i.e. being accepted). That probability is <span class="math inline">\(1/c\)</span> and we can calculate that as follows.</p>
<span class="math display">\[\begin{eqnarray*}
\mathbb{P}(X~\text{accepted})
&amp; = &amp; 
\mathbb{P}\left(U\leq\frac{f(X)}{c\,g(X)}\right)\\
&amp; = &amp;
\int
\mathbb{P}\left(\left.U\leq\frac{f(x)}{c\,g(x)}\right| X = x\right)g(x)\,dx\\
&amp; = &amp;
\int \frac{f(x)}{c\,g(x)} g(x)\,dx\\
&amp; = &amp;
\frac{1}{c}
\end{eqnarray*}\]</span>
<p>This property of rejection sampling has implications for how we choose the candidate density <span class="math inline">\(g\)</span>. In theory, any density can be chosen as the candidate as long as its support includes the support of <span class="math inline">\(f\)</span>. However, in practice we will want to choose <span class="math inline">\(g\)</span> so that it matches <span class="math inline">\(f\)</span> as closely as possible. As a rule of thumb, candidates <span class="math inline">\(g\)</span> that match <span class="math inline">\(f\)</span> closely will have smaller values of <span class="math inline">\(c\)</span> and thus will accept candidates with higher probability. We want to avoid large values of <span class="math inline">\(c\)</span> because large values of <span class="math inline">\(c\)</span> lead to an algorithm that rejects a lot of candidates and has lower efficiency.</p>
<p>In the example above with the Normal distribution and the <span class="math inline">\(t_2\)</span> distribution, the ratio <span class="math inline">\(f(x)/g(x)\)</span> was maximized at <span class="math inline">\(x=1\)</span> (or <span class="math inline">\(x=-1\)</span>) and so the value of <span class="math inline">\(c\)</span> for that setup was 1.257, which implies an acceptance probability of about 0.8. Suppose however, that we wanted to simulate from a Uniform<span class="math inline">\((0,1)\)</span> density and we used an Exponential<span class="math inline">\((1)\)</span> as our candidate density. The plot of the two densities looks as follows.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">curve</span>(<span class="kw">dexp</span>(x, <span class="dv">1</span>), <span class="dv">0</span>, <span class="dv">1</span>, <span class="dt">col =</span> <span class="dv">4</span>, <span class="dt">ylab =</span> <span class="st">&quot;Density&quot;</span>)
<span class="kw">segments</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>)
<span class="kw">legend</span>(<span class="st">&quot;bottomleft&quot;</span>, <span class="kw">c</span>(<span class="st">&quot;f(x) Uniform&quot;</span>, <span class="st">&quot;g(x) Exponential&quot;</span>), <span class="dt">lty =</span> <span class="dv">1</span>, <span class="dt">col =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">4</span>), <span class="dt">bty =</span> <span class="st">&quot;n&quot;</span>)</code></pre></div>
<p><img src="independentMC_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Here, the ratio of <span class="math inline">\(f(x)/g(x)\)</span> is maximized at <span class="math inline">\(x=1\)</span> and so the value of <span class="math inline">\(c\)</span> is 2.718 which implies an acceptance probablity of about 0.37. While running the rejection sampling algorithm in this way to produce Uniform random variables will still work, it will be very inefficient.</p>
<p>We can now show that the distribution of the accepted values from the rejection sampling algorithm above follows the target density <span class="math inline">\(f\)</span>. We can do this by calculating the <em>distribution function</em> of the accepted values and show that this is equal to <span class="math inline">\(F(t) = \int_{-\infty}^t f(x)\, dx\)</span>.</p>
<span class="math display">\[\begin{eqnarray*}
\mathbb{P}(X\leq t\mid X~\text{accepted})
&amp; = &amp;
\frac{\mathbb{P}(X\leq t, X~\text{accepted})}{\mathbb{P}(X~\text{accepted})}\\
&amp; = &amp; 
\frac{\mathbb{P}(X\leq t, X~\text{accepted})}{1/c}\\
&amp; = &amp;
c\,
\mathbb{E}_g
\mathbb{E}\left[
\left.\mathbf{1}\{x\leq t\}\mathbf{1}\left\{U\leq\frac{f(x)}{c\,g(x)}\right\}\right| X=x
\right]\\
&amp; = &amp;
c\,
\mathbb{E}_g\left[\mathbf{1}\{X\leq t\}
\mathbb{E}\left[
\left.\mathbf{1}\left\{U\leq\frac{f(x)}{c\,g(x)}\right\}\right| X=x
\right]\right]\\
&amp; = &amp;
c\,
\mathbb{E}_g\left[
\mathbf{1}\{X\leq t\}\frac{f(X)}{c\,g(X)}
\right]\\
&amp; = &amp;
\int_{-\infty}^{\infty}
\mathbf{1}\{x\leq t\}
\frac{f(x)}{g(x)}g(x)\,dx\\
&amp; = &amp;
\int_{-\infty}^t f(x)\,dx\\
&amp; = &amp;
F(t)
\end{eqnarray*}\]</span>
<p>This shows that the distribution function of the candidate values, given that they are accepted, is equal to the distribution function corresponding to the target density.</p>
<p>A few further notes:</p>
<ol style="list-style-type: decimal">
<li><p>We only need to know <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> up to a constant of proportionality. In many applications we will not know the normalizing constant for these densities, but we do not need them. That is, if <span class="math inline">\(f(x) = k_1 f^\star(x)\)</span> and <span class="math inline">\(g(x) = k_2 g^\star(x)\)</span>, we can proceed with the algorithm using <span class="math inline">\(f^\star\)</span> and <span class="math inline">\(g^\star\)</span> even if we do not know the values of <span class="math inline">\(k_1\)</span> and <span class="math inline">\(k_2\)</span>.</p></li>
<li><p>Any number <span class="math inline">\(c^\prime \geq c\)</span> will work in the rejection sampling algorithm, but the algorithm will be less efficient.</p></li>
<li><p>Throughout the algorithm, operations can (and should!) be done on a log scale.</p></li>
<li><p>The higher the dimension of <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span>, the less efficient the rejection sampling algorithm will be.</p></li>
<li><p>Whether <span class="math inline">\(c=\infty\)</span> or not depends on the tail behavior of the the densities <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span>. If <span class="math inline">\(g(x)\downarrow 0\)</span> faster than <span class="math inline">\(f(x)\downarrow 0\)</span> as <span class="math inline">\(x\rightarrow\infty\)</span>, then <span class="math inline">\(f(x)/g(x)\uparrow\infty\)</span>.</p></li>
</ol>
</div>
<div id="empirical-supremum-rejection-sampling" class="section level3">
<h3><span class="header-section-number">6.3.3</span> Empirical Supremum Rejection Sampling</h3>
<p>What if we cannot calculate <span class="math inline">\(c = \sup_{x\in\mathcal{X}_f} \frac{f(x)}{g(x)}\)</span> or are simply too lazy to do so? Fear not, because it turns out we almost never have to do so. A slight modification of the standard rejection sampling algorithm will allow us to <em>estimate</em> <span class="math inline">\(c\)</span> while also sampling from the target density <span class="math inline">\(f\)</span>. The tradeoff (there is always a tradeoff!) is that we must make a more stringent assumption about <span class="math inline">\(c\)</span>, mainly that it is achievable. That is, there exists some value <span class="math inline">\(x_c\in\mathcal{X}_f\)</span> such that <span class="math inline">\(\frac{f(x_c)}{g(x_c)}\)</span> is <em>equal</em> to <span class="math inline">\(\sup_{x\in\mathcal{X}_f} \frac{f(x)}{g(x)}\)</span>.</p>
<p>The modified algorithm is the <em>empirical supremum rejection sampling</em> algorithm of <a href="https://academic.oup.com/biomet/article-abstract/89/4/745/242234">Caffo, Booth, and Davison</a>. The algorithm goes as follows. First we must choose some starting value of <span class="math inline">\(c\)</span>, call it <span class="math inline">\(\hat{c}\)</span>, such that <span class="math inline">\(\hat{c}&gt; 1\)</span>. Then,</p>
<ol style="list-style-type: decimal">
<li><p>Draw <span class="math inline">\(U\sim\text{Unif}(0, 1)\)</span>.</p></li>
<li><p>Draw <span class="math inline">\(X\sim g\)</span>, the candidate density.</p></li>
<li><p>Accept <span class="math inline">\(X\)</span> if <span class="math inline">\(U\leq\frac{f(X)}{\hat{c}\,g(X)}\)</span>, otherwise reject <span class="math inline">\(X\)</span>.</p></li>
<li><p>Let <span class="math inline">\(\hat{c}^\star = \max\left\{\hat{c}, \frac{f(X)}{g(X)}\right\}\)</span>.</p></li>
<li><p>Update <span class="math inline">\(\hat{c} = \hat{c}^\star\)</span>.</p></li>
<li><p>Goto Step 1.</p></li>
</ol>
<p>From the algorithm we can see that at each iteration, we get more information about the ratio <span class="math inline">\(f(X)/g(X)\)</span> and can update our estimate of <span class="math inline">\(c\)</span> accordingly.</p>
<p>One way to think of this algorithm is to conceptualize a separate sequence <span class="math inline">\(\tilde{Y}_i\)</span>, which is <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span> depending on whether <span class="math inline">\(X_i\)</span> should be rejected (<span class="math inline">\(0\)</span>) or accepted (<span class="math inline">\(1\)</span>). This sequence <span class="math inline">\(\tilde{Y}_i\)</span> is the accept/reject determination sequence. Under the standard rejection sampling algorithm, the sequence <span class="math inline">\(\tilde{Y}_i\)</span> is generated using the true value of <span class="math inline">\(c\)</span>. Under the emprical supremum rejection sampling (ESUP) scheme, we generate a slightly different sequence <span class="math inline">\(Y_i\)</span> using our continuously updated value of <span class="math inline">\(\hat{c}\)</span>.</p>
<p>If we drew values <span class="math inline">\(X_1, X_2, X_3, X_4, X_5, X_6,\dots\)</span> from the candidate density <span class="math inline">\(g\)</span>, then we could visualize the acceptance/rejection process as it might occur using the true value of <span class="math inline">\(c\)</span> and our estimate <span class="math inline">\(\hat{c}\)</span>.</p>
<div class="figure">
<img src="image/esup.png" alt="Empirical supremum rejection sampling scheme." />
<p class="caption">Empirical supremum rejection sampling scheme.</p>
</div>
<p>Following the diagram above, we can see that using the estimate <span class="math inline">\(\hat{c}\)</span>, there are two instances where we accept a value when we should have rejected it (<span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_4\)</span>). In every other instance in the sequence, the value of <span class="math inline">\(Y_i\)</span> was equal to <span class="math inline">\(\tilde{Y}_i\)</span>. The theory behind the ESUP algorithm is that eventually, the sequence <span class="math inline">\(Y_i\)</span> becomes identical to the sequence <span class="math inline">\(\tilde{Y}_i\)</span> and therefore we will accept/reject candidates in the same manner as we would have if we had used the true <span class="math inline">\(c\)</span>.</p>
<p>If <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> are discrete distributions, then the proof of the ESUP algorithm is fairly straightforward. Specifically, Caffo, Booth, and Davison showed that <span class="math inline">\(\mathbb{P}(Y_i \ne \tilde{Y}_i~\text{infinitely often}) = 0\)</span>. Recall that by assumption, there exists some <span class="math inline">\(x_c\in\mathcal{X}_f\)</span> such that <span class="math inline">\(c = \frac{f(x_c)}{g(x_c)}\)</span>. Therefore, as we independently sample candidates from <span class="math inline">\(g\)</span>, at <em>some point</em>, we will sample the value <span class="math inline">\(x_c\)</span>, in which case we will achieve the value <span class="math inline">\(c\)</span>. Once that happens, we are then using the standard rejection sampling algorithm and our estimate <span class="math inline">\(\hat{c}\)</span> never changes.</p>
<p>Let <span class="math inline">\(\gamma = \min_i\{ x_i = x_c\}\)</span>, where <span class="math inline">\(x_i\sim g\)</span>. So <span class="math inline">\(\gamma\)</span> is the first time that we see the value <span class="math inline">\(x_c\)</span> as we are sampling candidates <span class="math inline">\(x_i\)</span> from <span class="math inline">\(g\)</span>. The probability that we sample <span class="math inline">\(x_c\)</span> is <span class="math inline">\(g(x_c)\)</span> (recall that <span class="math inline">\(g\)</span> is assumed to be discrete here) and so <span class="math inline">\(\gamma\)</span> has a Geometric distribution with success probability <span class="math inline">\(g(x_c)\)</span>. Once we observe <span class="math inline">\(x_c\)</span>, the ESUP algorithm and the standard rejection sampling algorithms converge and are identical.</p>
<p>From here, we can use the coupling inequality, which tells us that <span class="math display">\[
\mathbb{P}(Y_i\ne\tilde{Y}_i)
\leq
\mathbb{P}(\gamma\geq i).
\]</span> Given that <span class="math inline">\(\gamma\sim\text{Geometric}(g(x_c))\)</span>, we know that <span class="math display">\[
\mathbb{P}(\gamma\geq i) = (1-g(x_c))^{i-1}.
\]</span> This then implies that <span class="math display">\[
\sum_{i=1}^\infty
\mathbb{P}(Y_i\ne\tilde{Y}_i) &lt; \infty
\]</span> which, by the <a href="https://en.wikipedia.org/wiki/Borel–Cantelli_lemma">Borel-Cantelli lemma</a>, implies that <span class="math inline">\(\mathbb{P}(Y_i \ne \tilde{Y}_i~\text{infinitely often}) = 0\)</span>. Therefore, eventually the sequences <span class="math inline">\(Y_i\)</span> and <span class="math inline">\(\tilde{Y}_i\)</span> must converge and at that point the ESUP algorithm will be identical to the rejection sampling algorithm.</p>
<p>In practice, we will know know exactly when the ESUP algorithm has converged to the standard rejection sampling algorithm. However, Caffo, and Davison report that the convergence is generally fast. Therefore, a reasonable approach might be to discard the first several accepted values (e.g. a “burn in”) and then use the remaining values.</p>
<p>We can see how quickly ESUP converges in a simple example where the target density is the standard Normal and the candidate density is the <span class="math inline">\(t_2\)</span> distribution. Here we simulate 1,000 draws and start with a value <span class="math inline">\(\hat{c}=1.0001\)</span>. Note that in the code below, all of the computations are done on the log scale for the sake of numerical stability.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">2017</span><span class="op">-</span><span class="dv">12</span><span class="op">-</span><span class="dv">04</span>)
N &lt;-<span class="st"> </span><span class="dv">500</span>
y_tilde &lt;-<span class="st"> </span><span class="kw">numeric</span>(N)  ## Binary accept/reject for &quot;true&quot; algorithm
y &lt;-<span class="st"> </span><span class="kw">numeric</span>(N)        ## Binary accept/reject for ESUP
log_c_true &lt;-<span class="st"> </span><span class="kw">dnorm</span>(<span class="dv">1</span>, <span class="dt">log =</span> <span class="ot">TRUE</span>) <span class="op">-</span><span class="st"> </span><span class="kw">dt</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dt">log =</span> <span class="ot">TRUE</span>)
log_chat &lt;-<span class="st"> </span><span class="kw">numeric</span>(N <span class="op">+</span><span class="st"> </span><span class="dv">1</span>)
log_chat[<span class="dv">1</span>] &lt;-<span class="st"> </span><span class="kw">log</span>(<span class="fl">1.0001</span>)  ## Starting c value
<span class="cf">for</span>(i <span class="cf">in</span> <span class="kw">seq_len</span>(N)) {
        u &lt;-<span class="st"> </span><span class="kw">runif</span>(<span class="dv">1</span>)
        x &lt;-<span class="st"> </span><span class="kw">rt</span>(<span class="dv">1</span>, <span class="dv">2</span>)
        r_true &lt;-<span class="st"> </span><span class="kw">dnorm</span>(x, <span class="dt">log =</span> <span class="ot">TRUE</span>) <span class="op">-</span><span class="st"> </span><span class="kw">dt</span>(x, <span class="dv">2</span>, <span class="dt">log =</span> <span class="ot">TRUE</span>) <span class="op">-</span><span class="st"> </span>log_c_true
        rhat &lt;-<span class="st"> </span><span class="kw">dnorm</span>(x, <span class="dt">log =</span> <span class="ot">TRUE</span>) <span class="op">-</span><span class="st"> </span><span class="kw">dt</span>(x, <span class="dv">2</span>, <span class="dt">log =</span> <span class="ot">TRUE</span>) <span class="op">-</span><span class="st"> </span>log_chat[i]
        y_tilde[i] &lt;-<span class="st"> </span><span class="kw">log</span>(u) <span class="op">&lt;=</span><span class="st"> </span>r_true
        y[i] &lt;-<span class="st"> </span><span class="kw">log</span>(u) <span class="op">&lt;=</span><span class="st"> </span>rhat
        log_chat[i<span class="op">+</span><span class="dv">1</span>] &lt;-<span class="st"> </span><span class="kw">max</span>(log_chat[i], 
                             <span class="kw">dnorm</span>(x, <span class="dt">log =</span> <span class="ot">TRUE</span>) <span class="op">-</span><span class="st"> </span><span class="kw">dt</span>(x, <span class="dv">2</span>, <span class="dt">log =</span> <span class="ot">TRUE</span>))
}</code></pre></div>
<p>Now we can plot <span class="math inline">\(\log_{10}(|\hat{c}-c|)\)</span> for each iteration to see how the magnitude of the error changes with each iteration.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">c_true &lt;-<span class="st"> </span><span class="kw">exp</span>(log_c_true)
chat &lt;-<span class="st"> </span><span class="kw">exp</span>(log_chat)
<span class="kw">plot</span>(<span class="kw">log10</span>(<span class="kw">abs</span>(chat <span class="op">-</span><span class="st"> </span>c_true)), <span class="dt">type =</span> <span class="st">&quot;l&quot;</span>,
     <span class="dt">xlab =</span> <span class="st">&quot;Iteration&quot;</span>, <span class="dt">ylab =</span> <span class="kw">expression</span>(<span class="kw">paste</span>(log[<span class="dv">10</span>], <span class="st">&quot;(Absolute Error)&quot;</span>)))</code></pre></div>
<p><img src="independentMC_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>We can see that by iteration 40 or so, <span class="math inline">\(\hat{c}\)</span> and <span class="math inline">\(c\)</span> differ only in the 5th decimal place and beyond. By the 380th iteration, they differ only beyond the 6th decimal place.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="non-uniform-random-numbers.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="importance-sampling.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
