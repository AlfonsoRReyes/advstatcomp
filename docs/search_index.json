[
["index.html", "Advanced Statistical Computing Welcome", " Advanced Statistical Computing Roger D. Peng 2017-12-12 Welcome The book covers material taught in the Johns Hopkins Biostatistics Advanced Statistical Computing course. This book is a WORK IN PROGRESS. "],
["stay-in-touch.html", "Stay in Touch!", " Stay in Touch! If you are interested in hearing more from me about things that I’m working on (books, data science courses, podcast, etc.), you can do two things: First, I encourage you to join the Johns Hopkins Data Science Lab mailing list. On this list I send out updates of my own activities as well as occasional comments on data science current events. You can also find out what my co-conspirators Jeff Leek and Brian Caffo are up to because sometimes they do really cool stuff. Second, I have a regular podcast called Not So Standard Deviations that I co-host with Dr. Hilary Parker, a Data Scientist at Stitch Fix. On this podcast, Hilary and I talk about the craft of data science and discuss common issues and problems in analyzing data. We’ll also compare how data science is approached in both academia and industry contexts and discuss the latest industry trends. You can listen to recent episodes on our SoundCloud page or you can subscribe to it in iTunes or your favorite podcasting app. For those of you who purchased a printed copy of this book, I encourage you to go to the Leanpub web site and obtain the e-book version, which is available for free. The reason is that I will occasionally update the book with new material and readers who purchase the e-book version are entitled to free updates (this is unfortunately not yet possible with printed books) and will be notified when they are released. Thanks again for purchasing this book and please do stay in touch! "],
["setup.html", "Setup", " Setup This book makes use of the following R packages, which should be installed to take full advantage of the examples. You can install all of these packages with the following code: install.packages(c(&quot;&quot;)) "],
["introduction.html", "1 Introduction", " 1 Introduction The journey from statistical model to useful output has many steps, most of which are taught in other books and courses. The purpose of this book is to focus on one particular aspect of this journey: the development and implementation of statistical algorithms. Figure 1.1: The process of statistical modeling. "],
["example-linear-models.html", "1.1 Example: Linear Models", " 1.1 Example: Linear Models Consider the simple linear model. \\[\\begin{equation} y = \\beta_0 + \\beta_1 x + \\varepsilon \\tag{1.1} \\end{equation}\\] This model has unknown parameters \\(\\beta_0\\) and \\(\\beta_1\\). Given observations \\((y_1, x_1), (y_2, x_2),\\dots, (y_n, x_n)\\), we can combine these data with the likelihood principle, which gives us a procedure for producing model parameter estimates. The likelihood can be maximized to produce maximum likelihood estimates, \\[ \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1\\bar{x} \\] and \\[ \\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n (x_i-\\bar{x})} \\] These statistics, \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\), can then be interpreted, depending on the area of application, or used for other purposes, perhaps as inputs to other procedures. In this simple example, we can see how each component of the modeling process works. Component Implementation Model Linear regression Principle/Technique Likelihood principle Algorithm Maximization Statistic \\(\\hat{\\beta}_0\\), \\(\\hat{\\beta}_1\\) In this example, the maximization of the likelihood was simple because the solution was available in closed form. However, in most other cases, there will not be a closed form solution and some specific algorithm will be needed to maximize the likelihood. Changing the implementation of a given component can lead to different outcomes further down the change and can even produce completely different outputs. Identical estimates for the parameters in this model can be produced (in this case) by replacing the likelihood principle with the principle of least squares. However, changing the principle to produce, for example, maximum a posteriori estimates would have produced different statistics at the end. "],
["principle-of-optimization-transfer.html", "1.2 Principle of Optimization Transfer", " 1.2 Principle of Optimization Transfer There is a general principle that will be repeated in this book that Kenneth Lange calls “optimization transfer”. The basic idea applies to the problem of maximizing a function \\(f\\). We want to maximize \\(f\\), but it is difficult to do so. We can compute an approximation to \\(f\\), call it \\(g\\), based on local information about \\(f\\). Instead of maximizing \\(f\\), we “transfer” the maximization problem to \\(g\\) and maximize \\(g\\) instead. We iterate Steps 2 and 3 until convergence. The difficult problem of maximizing \\(f\\) is replaced with the simpler problem of maximizing \\(g\\) coupled with iteration (otherwise known as computation). This is the optimization “transfer”. Note that all of the above applies to minimization problems, because maximizing \\(f\\) is equivalent to minimizing \\(-f\\). "],
["textbooks-vs-computers.html", "1.3 Textbooks vs. Computers", " 1.3 Textbooks vs. Computers One confusing aspect of statistical computing is that often there is a disconnect between what is printed in a statistical computing textbook and what should be implemented on the computer. In textbooks, it is usually simpler to present solutions as convenient mathematical formulas whenever possible, in order to communicate basic ideas and to provide some insight. However, directly translating these formulas into computer code is usually not advisable because there are many problematic aspects of computers that are simply not relevant when writing things down on paper. Some key issues to look for when implementing statistical or numerical solutions on the computer are Overflow - When numbers get too big, they cannot be represented on a computer and so often NAs are produced instead; Underflow - Similar to overflow, numbers can get too small for computers to represent, resulting in errors or warnings or inaccurate computation; Near linear dependence - the existence of linear dependence in matrix computations depends on the precision of a machine. Because computers are finite precision, there are commonly situations where one might think there is no linear dependence but the computer cannot tell the difference. All three of the above problems arise from the finite precision nature of all computers. One must take care to use algorithms that do calculations in the computable range and that automatically handle things like near dependence. Below, I highlight some common examples in statistics where the implementation diverges from what textbooks explain as the solution: Computing with logarithms, the least squares solution to the linear regression estimation problem, and the computation of the multivariate Normal density. Both problems, on paper, involve inverting a matrix, which is typically a warning sign in any linear algebra problem. While matrix inverses are commonly found in statistics textbooks, it’s rare in practice that you will ever want to directly compute them. This point bears repeating: If you find yourself computing the inverse of a matrix, there is usually a better way of doing whatever you are trying to do. 1.3.1 Using Logarithms Most textbooks write out functions, such as densities, in their natural form. For example, the univariate Normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\) is written \\[ f(x\\mid\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2\\sigma^2}(x-\\mu)^2} \\] and you can compute this value for any \\(x\\), \\(\\mu\\), and \\(\\sigma\\) in R with the dnorm() function. But in practice, you almost never have to compute this exact number. Usually you can get away with computing the log of this value (and with dnorm() you can set the option log = TRUE). In some situations, such as with importance sampling, you do have to compute density values on the original scale, and that can be considered a disadvantage of that technique. Computing densities with logarithms is much more numerically stable than computing densities without them. With the exponential function in the density, numbers can get very small quickly, to the point where they are too small for the machine to represent (underflow). In some situations, you may need to take the ratio of two densities and then you may end up with either underflow or overflow (if numbers get too big). Doing calculations on a log scale (and then exponentiating them later if needed) usually resolves problems of underflow or overflow. In this book (and in any other), when you see expressions like \\(f(x)/g(x)\\), you should think that this means \\(\\exp(\\log f(x) - \\log(g(x)))\\). The two are equivalent but the latter is likely more numerically stable. In fact, most of the time, you never have to re-exponentiate the values, in which case you can spend your entire time in log-land. For example, in the rejection sampling algorithm, you need to determine if \\(U\\leq\\frac{f(x)}{g(x)}\\). However, taking the log of both sides allows you to do the exact same comparison in a much more numerically stable way. 1.3.2 Linear Regression The typical linear regression model, written in matrix form, is represented as follows, \\[ y = X\\beta + \\varepsilon \\] where \\(y\\) is an \\(n\\times 1\\) observed response, \\(X\\) is the \\(n\\times p\\) predictor matrix, \\(\\beta\\) is the \\(p\\times 1\\) coefficient vector, and \\(\\varepsilon\\) is \\(n\\times 1\\) error vector. In most textbooks the solution for estimating \\(\\beta\\), whether it be via maximum likelihood or least squares, is written as \\[ \\hat{\\beta} = (X^\\prime X)^{-1}X^\\prime y. \\] And indeed, that is the solution. In R, this could be translated literally as betahat &lt;- solve(t(X) %*% X) %*% t(X) %*% y where solve() is used to invert the cross product matrix \\(X^\\prime X\\). However, one would never compute the actual value of \\(\\hat{\\beta}\\) this way on the computer. The formula presented above is only computed in textbooks. The primary reason is that computing the direct inverse of \\(X^\\prime X\\) is very expensive computationally and is a potentially unstable operation on a computer when there is high colinearity amongst the predictors. Furthermore, in computing \\(\\hat{\\beta}\\) we do not actually need the inverse of \\(X^\\prime X\\), so why compute it? A simpler approach would be to take the normal equations, \\[ X^\\prime X\\beta = X^\\prime y \\] and solve them directly. In R, we could write solve(crossprod(X), crossprod(X, y)) Rather than compute the inverse of \\(X^\\prime X\\), we directly compute \\(\\hat{\\beta}\\) via Gaussian elimination. This approach has the benefit of being more numerically stable and being much faster. set.seed(2017-07-13) X &lt;- matrix(rnorm(5000 * 100), 5000, 100) y &lt;- rnorm(5000) Here we benchmark the naive computation. library(microbenchmark) microbenchmark(solve(t(X) %*% X) %*% t(X) %*% y) Unit: milliseconds expr min lq mean median solve(t(X) %*% X) %*% t(X) %*% y 47.56642 51.34981 58.33341 54.22329 uq max neval 58.83291 101.612 100 The following timing uses the solve() function to compute \\(\\hat{\\beta}\\) via Gaussian elimination. microbenchmark(solve(t(X) %*% X) %*% t(X) %*% y, solve(crossprod(X), crossprod(X, y))) Unit: milliseconds expr min lq mean median solve(t(X) %*% X) %*% t(X) %*% y 45.96281 48.84230 53.48634 50.52306 solve(crossprod(X), crossprod(X, y)) 22.24667 22.45159 22.94948 22.70641 uq max neval cld 53.32787 148.98025 100 b 23.12598 29.17004 100 a You can see that the betweeen the two approach there is a more than 5-fold difference in computation time, with the second approach being considerably faster. However, this approach breaks down when there is any colinearity in the \\(X\\) matrix. For example, we can tack on a column to \\(X\\) that is very similar (but not identical) to the first column of \\(X\\). W &lt;- cbind(X, X[, 1] + rnorm(5000, sd = 0.0000000001)) solve(crossprod(W), crossprod(W, y)) [,1] [1,] -1.009751e+03 [2,] -8.039783e-03 [3,] 4.684944e-03 [4,] 2.700024e-02 [5,] 6.355835e-03 [6,] 8.693650e-03 [7,] 1.440320e-02 [8,] -1.433221e-02 [9,] -9.035157e-03 [10,] -2.170287e-02 [11,] -1.737509e-02 [12,] 1.681501e-02 [13,] 2.223502e-03 [14,] -1.373478e-02 [15,] 1.167108e-03 [16,] -5.869775e-03 [17,] -6.319076e-04 [18,] 2.556232e-03 [19,] -4.057100e-03 [20,] -6.623820e-04 [21,] -3.384654e-03 [22,] 1.509237e-02 [23,] 5.172755e-04 [24,] -1.656931e-02 [25,] -1.321050e-02 [26,] -6.083916e-05 [27,] 1.027881e-02 [28,] 4.948704e-03 [29,] 1.012277e-02 [30,] 4.484213e-03 [31,] -1.133879e-02 [32,] 1.723859e-03 [33,] 3.625681e-03 [34,] 7.376709e-03 [35,] 1.794856e-02 [36,] 1.547454e-02 [37,] -2.234061e-02 [38,] 1.443603e-02 [39,] -2.052446e-02 [40,] -2.553873e-03 [41,] 2.351903e-02 [42,] 2.261801e-02 [43,] 2.021437e-02 [44,] 6.262966e-03 [45,] 7.463015e-03 [46,] -1.977186e-02 [47,] -6.093681e-02 [48,] -8.966449e-03 [49,] -8.477453e-03 [50,] -1.863702e-02 [51,] -3.808376e-03 [52,] 7.120105e-03 [53,] 2.219219e-03 [54,] 4.041155e-03 [55,] 1.878475e-02 [56,] -3.440053e-02 [57,] -6.169164e-03 [58,] 1.496643e-03 [59,] -8.046797e-03 [60,] -7.373807e-03 [61,] 3.833052e-03 [62,] 2.784244e-03 [63,] 8.004059e-03 [64,] -1.676562e-04 [65,] -1.035378e-02 [66,] -3.802059e-03 [67,] 1.025505e-02 [68,] 9.116551e-03 [69,] -7.395200e-03 [70,] -1.505831e-02 [71,] 1.617219e-02 [72,] 2.888739e-02 [73,] -2.593069e-02 [74,] 2.479187e-02 [75,] -7.660286e-03 [76,] 1.043432e-02 [77,] 7.832101e-03 [78,] -1.891912e-02 [79,] 7.746156e-03 [80,] -2.321135e-03 [81,] 7.036778e-03 [82,] 1.592099e-02 [83,] -1.004987e-02 [84,] -7.116285e-03 [85,] 3.188724e-03 [86,] -1.686812e-02 [87,] -3.881375e-03 [88,] 7.061706e-03 [89,] 7.478217e-03 [90,] 9.793769e-03 [91,] -2.878374e-02 [92,] 1.151801e-02 [93,] -6.219824e-03 [94,] -2.377119e-02 [95,] 1.127488e-02 [96,] -5.908337e-03 [97,] 2.185800e-02 [98,] -1.256333e-02 [99,] 1.826857e-02 [100,] -6.703255e-03 [101,] 1.009750e+03 Now the approach doesn’t work because the cross product matrix \\(W^\\prime W\\) is singular. In practice, matrices like these can come up a lot in data analysis and it would be useful to have a way to deal with it automatically. R takes a different approach to solving for the unknown coefficients in a linear model. R uses the QR decomposition, which is not as fast, but has the added benefit of being able to automatically detect and handle colinear columns in the matrix. Here, we use the fact that \\(X\\) can be decomposed as \\(X = QR\\), where \\(Q\\) is an orthonormal matrix and \\(R\\) is an upper triangular matrix. Given that, we can write \\[ X^\\prime X\\beta = X^\\prime y \\] as \\[\\begin{eqnarray*} R^\\prime Q^\\prime QR\\beta &amp; = &amp; R^\\prime Q^\\prime y\\\\ R^\\prime R\\beta &amp; = &amp; R^\\prime Q^\\prime y\\\\ R\\beta &amp; = &amp; Q^\\prime y \\end{eqnarray*}\\] because \\(Q^\\prime Q = I\\). At this point, we can solve for \\(\\beta\\) via Gaussian elimination, which is greatly simplified because \\(R\\) is already upper triangular. The QR decomposition has the added benefit that we do not have to compute the cross product \\(X^\\prime X\\) at all, as this matrix can be numericaly unstable if it is not properly centered or scaled. We can see in R code that even with our singular matrix W above, the QR decomposition continues without error. Qw &lt;- qr(W) str(Qw) List of 4 $ qr : num [1:5000, 1:101] 70.88664 -0.01277 0.01561 0.00158 0.02451 ... $ rank : int 100 $ qraux: num [1:101] 1.01 1.03 1.01 1.02 1.02 ... $ pivot: int [1:101] 1 2 3 4 5 6 7 8 9 10 ... - attr(*, &quot;class&quot;)= chr &quot;qr&quot; Note that the output of qr() computes the rank of \\(W\\) to be 100, not 101, because of the colinear column. From there, we can get \\(\\hat{\\beta}\\) if we want using qr.coef(), betahat &lt;- qr.coef(Qw, y) We do not show it here, but the very last element of betahat is NA because a coefficient corresponding to the last column of \\(W\\) (the collinear column) could not be calculated. While the QR decomposition does handle colinearity, we do pay a price in speed. library(ggplot2) m &lt;- microbenchmark(solve(t(X) %*% X) %*% t(X) %*% y, solve(crossprod(X), crossprod(X, y)), qr.coef(qr(X), y)) autoplot(m) Compared to the approaches above, it is comparable to the naive approach but it is a much better and more stable method. In practice, we do not use functions like qr() or qr.coef() directly because higher level functions like lm() do the work for us. However, for certain narrow, highly optimized cases, it may be fruitful to turn to another matrix decomposition to compute linear regression coefficients, particularly if this must be done repeatedly in a loop. 1.3.3 Multivariate Normal Distribution Computing the multivariate normal density is a common problem in statistics, such as in fitting spatial statistical models or Gaussian process models. Because optimization procedures used to compute maximum likelihood estimates or likelihood ratios can be evaluated hundreds or thousands of times in a single run, it’s useful to have a highly efficient procedure for evaluating the multivariate Normal density. The \\(p\\)-dimensional multivariate Normal density is written as \\[ \\varphi(x\\mid\\mu,\\Sigma) = -\\frac{p}{2}\\log 2\\pi-\\frac{1}{2}\\log|\\Sigma| - \\frac{1}{2}(x-\\mu)^\\prime\\Sigma^{-1}(x-\\mu) \\] The critical, and most time-consuming, part of computing the multivariate Normal density is the quadratic form, \\[ (x-\\mu)^\\prime\\Sigma^{-1}(x-\\mu). \\] We can simplify this problem a bit by focusing on the centered version of \\(x\\) which we will refer to as \\(z=x-\\mu\\). Hence, we are trying to compute \\[ z^\\prime\\Sigma^{-1}z \\] Here, much like the linear regression example above, the key bottleneck is the inversion of the \\(p\\)-dimensional covariance matrix \\(\\Sigma\\). If we take \\(z\\) to be a \\(p\\times 1\\) column vector, then a literal translation of the mathematics into R code might look something like this, t(z) %*% solve(Sigma) %*% z But once again, we are taking on the difficult and unstable task of inverting \\(\\Sigma\\) when, at the end of the day, we do not need this inverse. Instead of taking the textbook translation approach, we can make use of the Cholesky decomposition of \\(\\Sigma\\). The Cholesky decomposition of a positive definite matrix provides \\[ \\Sigma = R^\\prime R \\] where \\(R\\) is an upper triangular matrix. \\(R\\) is sometimes referred to as the “square root” of \\(\\Sigma\\) (although it is not unique). Using the Cholesky decomposition of \\(\\Sigma\\) and the rules of matrix algebra, we can then write \\[\\begin{eqnarray*} z^\\prime\\Sigma^{-1}z &amp; = &amp; z^\\prime (R^\\prime R)^{-1}z\\\\ &amp; = &amp; z^\\prime R^{-1}R^{\\prime -1}z\\\\ &amp; = &amp; (R^{\\prime -1}z)^\\prime R^{\\prime -1}z\\\\ &amp; = &amp; v^\\prime v \\end{eqnarray*}\\] where \\(v = R^{\\prime -1} z\\) and is a \\(p\\times 1\\) vector. Furthermore, we can avoid inverting \\(R^\\prime\\) by computing \\(v\\) as the solution to the linear system \\[ R^\\prime v = z \\] Once we have computed \\(v\\), we can compute the quadratic form as \\(v^\\prime v\\), which is simply the cross product of two \\(p\\)-dimensional vectors! Another benefit of the Cholesky decomposition is that it gives us a simple way to compute the log-determinant of \\(\\Sigma\\). The log-determinant of \\(\\Sigma\\) is simply \\(2\\) times the sum of the log of the diagonal elements of \\(R\\). Here is an implementation of the naive approach to computing the quadratic form in the multivariate Normal. set.seed(2017-07-13) z &lt;- matrix(rnorm(200 * 100), 200, 100) S &lt;- cov(z) quad.naive &lt;- function(z, S) { Sinv &lt;- solve(S) rowSums((z %*% Sinv) * z) } We can first take a look at the output that this function produces. library(dplyr) quad.naive(z, S) %&gt;% summary Min. 1st Qu. Median Mean 3rd Qu. Max. 73.57 93.54 100.59 100.34 106.39 129.12 The following is a version of the quadratic form function that uses the Cholesky decomposition. quad.chol &lt;- function(z, S) { R &lt;- chol(S) v &lt;- backsolve(R, t(z), transpose = TRUE) colSums(v * v) } We can verify that this function produces the same output as the naive version. quad.chol(z, S) %&gt;% summary Min. 1st Qu. Median Mean 3rd Qu. Max. 73.57 93.54 100.59 100.34 106.39 129.12 Now, we can time both procedures to see how they perform. library(microbenchmark) microbenchmark(quad.naive(z, S), quad.chol(z, S)) Unit: microseconds expr min lq mean median uq max quad.naive(z, S) 1612.812 1736.153 2006.144 1824.102 1992.39 4435.319 quad.chol(z, S) 936.118 977.704 1148.600 1042.648 1203.15 3799.679 neval cld 100 b 100 a We can see that the version using the Cholesky decomposition takes about 60% of the time of the naive version. In a single evaluation, this may not amount to much time. However, over the course of potentially many iterations, these kinds of small savings can add up. The key lesson here is that our use of the Cholesky decomposition takes advantage of the fact that we know that the covariance matrix in a multivariate Normal is symmetric and positive definite. The naive version of the algorithm that just blindly inverts the covariance matrix is not able to take advantage of this information. "],
["solving-nonlinear-equations.html", "2 Solving Nonlinear Equations ", " 2 Solving Nonlinear Equations "],
["bisection-algorithm.html", "2.1 Bisection Algorithm", " 2.1 Bisection Algorithm The bisection algorithm is a simple method for finding the roots of one-dimensional functions. The goal is to find a root \\(x_0\\in[a, b]\\) such that \\(f(x_0)=0\\). The algorithm starts with a large interval, known to contain \\(x_0\\), and then successively reduces the size of the interval until it brackets the root. The theoretical underpinning of the algorithm is the intermediate value theorem which states that if a continuous function \\(f\\) takes values \\(f(a)\\) and \\(f(b)\\) at the end points of the interval \\([a, b]\\), then \\(f\\) must take all values between \\(f(a)\\) and \\(f(b)\\) somewhere in the interval. So if \\(f(a) &lt; \\gamma &lt; f(b)\\), then there exists a \\(c\\in[a, b]\\) such that \\(f(c)=\\gamma\\). Using this information, we can present the bisection algorithm. First we must check that \\(\\text{sign}(f(a)) \\ne \\text{sign}(f(b))\\). Otherwise, the interval does not contain the root and might need to be widened. Then we can proceed: Let \\(c = \\frac{a + b}{2}\\). If \\(f(c) = 0\\), stop and return \\(c\\). If \\(\\text{sign}(f(a))\\ne\\text{sign}(f(c))\\), then set \\(b\\leftarrow c\\). Else if \\(\\text{sign}(f(b))\\ne\\text{sign}(f(c))\\), then set \\(a\\leftarrow c\\). Goto the beginning and repeat until convergence (see below). After \\(n\\) iterations, the size of the interval bracketing the root will be \\(2^{-n}(b-a)\\). The bisection algorithm is useful, conceptually simple, and is easy to implement. In particular, you do not need any special information about the function \\(f\\) except the ability to evaluate it at various points in the interval. The downsides are that it is only useful in one dimension and its convergence is linear, which is the slowest rate of convergence for algorithms we will discuss (more on that later). The bisection algorithm can run into problems in situations where the function \\(f\\) is not well behaved. The ideal situation for the bisection algorithm looks something like this. Ideal setup for bisection algorithm. Here, \\(f(a)\\) and \\(f(b)\\) are of opposite signs and the root is clearly in between \\(a\\) and \\(b\\). In the scenario below, the algorithm will not start because \\(f(a)&gt;0\\) and \\(f(b)&gt;0\\). Derivative of \\(f\\) at the root is \\(0\\). In this next scenario, there are two roots between \\(a\\) and \\(b\\), in addition to having \\(f(a)&gt;0\\) and \\(f(b)&gt;0\\). One would need to reduce the length of the starting interval in order to find either root. Two roots within the interval. In the scenario below, the algorithm will start because \\(f(a)\\) and \\(f(b)\\) are of opposite sign, but there is no root. Interval contains an asymptote but no root. Convergence of the bisection algorithm can be determined by either having \\(|b-a|&lt;\\varepsilon\\) for some small \\(\\varepsilon\\) or having \\(|f(b)-f(a)|&lt;\\varepsilon\\). Which criterion you use will depend on the specific application and on what kinds of tolerances are required. 2.1.1 Example: Quantiles Given a cumulative distribution function \\(F(x)\\) and a number \\(p\\in (0, 1)\\), a quantile of \\(F\\) is a number \\(x\\) such that \\(F(x) = p\\). The bisection algorithm can be used to find a quantile \\(x\\) for a given \\(p\\) by defining the function \\(g(x) = F(x) - p\\) and solving for the value of \\(x\\) that achieves \\(g(x) = 0\\). Another way to put this is that we are inverting the CDF to compute \\(x = F^{-1}(p)\\). So the bisection algorithm can be used to invert functions in these situations. "],
["rates-of-convergence.html", "2.2 Rates of Convergence", " 2.2 Rates of Convergence One of the ways in which algorithms will be compared is via their rates of convergence to some limiting value. Typically, we have an interative algorithm that is trying to find the maximum/minimum of a function and we want an estimate of how long it will take to reach that optimal value. There are three rates of convergence that we will focus on here—linear, superlinear, and quadratic—which are ordered from slowest to fastest. In our context, rates of convergence are typically determined by how much information about the target function \\(f\\) we use in the updating process of the algorithm. Algorithms that use little information about \\(f\\), such as the bisection algorithm, converge slowly. Algorithms that require more information about \\(f\\), such as derivative information, typically converge more quickly. There is no free lunch! 2.2.1 Linear convergence Suppose we have a sequence \\(\\{x_n\\}\\) such that \\(x_n\\rightarrow x_\\infty\\) in \\(\\mathfrak{R}^k\\). We say the convergence is linear if there exists \\(r\\in(0, 1)\\) such that \\[ \\frac{\\|x_{n+1}-x_\\infty\\|}{\\|x_n-x_\\infty\\|}\\leq r \\] for all \\(n\\) sufficiently large. 2.2.1.1 Example The simple sequence \\(x_n = 1 + \\left(\\frac{1}{2}\\right)^n\\) converges linearly to \\(x_\\infty = 1\\) because \\[ \\frac{\\|x_{n+1}-x_\\infty\\|}{\\|x_n-x_\\infty\\|} = \\frac{\\left(\\frac{1}{2}\\right)^{n+1}}{\\left(\\frac{1}{2}\\right)^n} = \\frac{1}{2} \\] which is always in \\((0, 1)\\). 2.2.2 Superlinear Convergence We say a sequence \\(\\{x_n\\}\\) converges to \\(x_\\infty\\) superlinearly if we have \\[ \\lim_{n\\rightarrow\\infty} \\frac{\\|x_{n+1}-x_\\infty\\|}{\\|x_n-x_\\infty\\|} = 0 \\] The sequence above does not converge superlinearly because the ratio is always constant, and so never can converge to zero as \\(n\\rightarrow\\infty\\). However, the sequence \\(x_n = 1 + \\left(\\frac{1}{n}\\right)^n\\) converges superlinearly to \\(1\\). 2.2.3 Quadratic Convergence Quadratic convergence is the fastest form of convergence that we will discuss here and is generally considered desirable if possible to achieve. We say the sequence converges at a quadratic rate if there exists some constant \\(0 &lt; M &lt; \\infty\\) such that \\[ \\frac{\\|x_{n+1}-x_\\infty\\|}{\\|x_n-x_\\infty\\|^2}\\leq M \\] for all \\(n\\) sufficiently large. Extending the examples from above, the sequence \\(x_n = 1 + \\left(\\frac{1}{n}\\right)^{2^n}\\) converges quadratically to \\(1\\). With this sequence, we have \\[ \\frac{\\|x_{n+1}-x_\\infty\\|}{\\|x_n-x_\\infty\\|^2} = \\frac{\\left(\\frac{1}{n+1}\\right)^{2^{n+1}}}{\\left(\\frac{1}{n}\\right)^{(2^n)2}} = \\left(\\frac{n}{n+1}\\right)^{2^{n+1}} \\leq 1 \\] 2.2.4 Example: Bisection Algorithm For the bisection algorithm, the error that we make in estimating the root is \\(x_n = |b_n - a_n|\\), where \\(a_n\\) and \\(b_n\\) represent the end points of the bracketing interval at iteration \\(n\\). However, we know that the size of the interval in the bisection algorithm decreases by a half at each iteration. Therefore, we can write \\(x_n = 2^{-n}|b_0 - a_0|\\) and we can write the rate of convergence as \\[ \\frac{\\|x_{n+1}-x_\\infty\\|}{\\|x_n-x_\\infty\\|} = \\frac{x_{n+1}}{x_n} = \\frac{2^{-(n+1)}(b_0-a_0)}{2^{-n}(b_0-a_0)} = \\frac{1}{2} \\] Therefore, the error of the bisection algorithm converges linearly to \\(0\\). "],
["functional-iteration.html", "2.3 Functional Iteration", " 2.3 Functional Iteration We want to find a solution to the equation \\(f(x)=0\\) for \\(f: \\mathbb{R}^k\\rightarrow \\mathbb{R}\\) and \\(x\\in S\\subset \\mathbb{R}^k\\). One approach to solving this problem is to characterize solutions as fixed points of other functions. For example, if \\(f(x_0) = 0\\), then \\(x_0\\) is a fixed point of the function \\(g(x)=f(x) + x\\). Another such function might be \\(g(x) = x(f(x) + 1)\\) for \\(x\\ne 0\\). In some situations, we can construct a function \\(g\\) and a sequence \\(x_n = g(x_{n-1})\\) such that we have the sequence \\(x_n\\rightarrow x_\\infty\\) where \\(g(x_\\infty) = x_\\infty\\). In other words, the sequence of values \\(x_n\\) converges to a fixed point of \\(g\\). If this fixed point satisfies \\(f(x_\\infty) = 0\\), then we have found a solution to our original problem. The most important algorithm that we will discuss based on functional iteration is Newton’s method. The EM algorithm is also an algorithm that can be formulated as a functional iteration and we will discuss that at a later section. When can such a functional iteration procedure work? The Shrinking Lemma gives us the conditions under which this type of sequence will converge. 2.3.1 The Shrinking Lemma The Shrinking Lemma gives conditions under which a sequence derived via functional iteration will converge to a fixed point. Let \\(M\\) be a closed subset of a complete normed vector space and let \\(f: M\\rightarrow M\\) be a map. Assume that there exists a \\(K\\), \\(0&lt;K&lt;1\\), such that for all \\(x,y\\in M\\), \\[ \\|f(x)-f(y)\\|\\leq K\\|x-y\\|. \\] Then \\(f\\) has a unique fixed point, i.e. there is a unique point \\(x_0\\in M\\) such that \\(f(x_0) = x_0\\). Proof: The basic idea of the proof of this lemma is that for a give \\(x\\in M\\), we can construct a Cauchy sequence \\(\\{f^n(x)\\}\\) that converges to \\(x_0\\), where \\(f^n(x)\\) represents the \\(n\\)th functional iteration of \\(x\\), i.e. \\(f^2(x) = f(f(x))\\). Given \\(x\\in M\\), we can write \\[ \\|f^2(x)-f(x)\\| = \\|f(f(x)) - f(x)\\|\\leq K\\|f(x)-x\\|. \\] By induction, we can therefore write \\[ \\|f(^{n+1}(x)-f^{n}(x)\\|\\leq K\\|f^n(x)-f^{n-1}(x)\\|\\leq K^n\\|f(x)-x\\|. \\] It then follows that \\[\\begin{eqnarray*} \\|f^n(x)-x\\| &amp; \\leq &amp; \\|f^{n}(x)-f^{n-1}(x)\\| + \\|f^{n-1}(x)-f^{n-2}(x)\\| + \\cdots + \\|f(x)-x\\|\\\\ &amp; \\leq &amp; (K^{n-1} + K^{n-2} + \\cdots + K)\\|f(x)-x\\|\\\\ &amp; \\leq &amp; \\frac{1}{1-K}\\|f(x)-x\\|. \\end{eqnarray*}\\] Given integers \\(m\\geq 1\\) and \\(k\\geq 1\\), we can write \\[\\begin{eqnarray*} \\|f^{m+k}(x)-f^k(x)\\| &amp; \\leq &amp; K^m\\|f^k(x)-x\\|\\\\ &amp; \\leq &amp; K^m\\frac{1}{1-K}\\|f(x)-x\\| \\end{eqnarray*}\\] Therefore, there exists some \\(N\\) such that for all \\(m, n\\geq N\\) (say \\(n=m+k\\)), we have \\(\\|f^n(x)-f^m(x)\\|\\leq\\varepsilon\\), because \\(K^m\\rightarrow 0\\) as \\(m\\rightarrow\\infty\\). As a result, the sequence \\(\\{f^n(x)\\}\\) is a Cauchy sequence, so let \\(x_0\\) be its limit. Given \\(\\varepsilon&gt;0\\), let \\(N\\) be such that for all \\(n\\geq N\\), \\(\\|f^n(x)-x_0\\|\\leq\\varepsilon\\). Then we can also say that for \\(n\\geq N\\), \\[ \\|f(x_0) - f^{n+1}(x)\\|\\leq \\|x_0-f^n(x)\\|\\leq\\varepsilon \\] So what we have is \\(\\{f^n(x)\\}\\rightarrow x_0\\) and we have \\(\\{f^n(x)\\}\\rightarrow f(x_0)\\). Therefore, \\(x_0\\) is a fixed point of \\(f\\), so that \\(f(x_0)=x_0\\). To show that \\(x_0\\) is unique, suppose that \\(x_1\\) is another fixed point of \\(f\\). Then \\[ \\|x_1-x_0\\| = \\|f(x_1)-f(x_0)\\|\\leq K\\|x_1-x_0\\|. \\] Because \\(0&lt;K&lt;1\\), we must have \\(x_0=x_1\\). 2.3.2 Convergence Rates for Shrinking Maps Suppose \\(g\\) satisfies \\[ |g(x)-g(y)|\\leq K|x-y| \\] for some \\(K\\in (0,1)\\) and any \\(x, y\\in I\\), a closed interval. Therefore, \\(g\\) has a fixesd point at \\(x_\\infty\\). Assume \\(g\\) is differentiable with \\(0&lt;|g^\\prime(x_\\infty)|&lt;1\\), where \\(x_\\infty\\) is the fixed point of \\(g\\). Then \\(x_n\\rightarrow x_\\infty\\) linearly. We can show that \\[ \\frac{|x_{n+1}-x_\\infty|}{|x_n-x_\\infty|} = \\frac{|g(x_n)-g(x_\\infty)|}{|x_n-x_\\infty|} \\leq K \\frac{|x_n-x_\\infty|}{|x_n-x_\\infty|}. \\] Because \\(K\\in(0,1)\\), this shows that convergence to \\(x_\\infty\\) is linear. "],
["newtons-method.html", "2.4 Newton’s Method", " 2.4 Newton’s Method Newton’s method build a sequence of values \\(\\{x_n\\}\\) via functional iteration that converges to the root of a function \\(f\\). Let that root be called \\(x_\\infty\\) and let \\(x_n\\) be the current estimate. By the mean value theorem, we know there exists some \\(z\\) such that \\[ f(x_n) = f^\\prime(z)(x_n-x_\\infty), \\] where \\(z\\) is somewhere between \\(x_n\\) and \\(x_\\infty\\). Rearranging terms, we can write \\[ x_\\infty = x_n-\\frac{f(x_n)}{f^\\prime(z)} \\] Obviously, we do not know \\(x_\\infty\\) or \\(z\\), so we can replace them with our next iterate \\(x_{n+1}\\) and our current iterate \\(x_n\\), giving us the Newton update formula, \\[ x_{n+1} = x_n - \\frac{f(x_n)}{f^\\prime(x_n)}. \\] We will discuss Newton’s method more in the later section on general optimization, as it is a core method for minimizing functions. 2.4.1 Proof of Newton’s Method Newton’s method can be written as a functional iteration that converges to a fixed point. Let \\(f\\) be a function that is twice continuously differentiable and suppose there exists a \\(x_\\infty\\) such that \\(f(x_\\infty) = 0\\) and \\(f^\\prime(x_\\infty)\\ne 0\\). Then there exists a \\(\\delta\\) such that for any \\(x_0\\in(x_\\infty-\\delta, x_\\infty+\\delta)\\), the sequence \\[ x_n = g(x_{n-1}) = x_{n-1} - \\frac{f(x_{n-1})}{f^\\prime(x_{n-1})} \\] converges to \\(x_\\infty\\). Note that \\[\\begin{eqnarray*} g^\\prime(x) &amp; = &amp; 1-\\frac{f^\\prime(x)f^\\prime(x)-f(x)f^{\\prime\\prime}(x)}{[f^\\prime(x)]^2}\\\\ &amp; = &amp; \\frac{f(x)f^{\\prime\\prime}(x)}{[f^\\prime(x)]^2} \\end{eqnarray*}\\] Therefore, \\(g^\\prime(x_\\infty) = 0\\) because we assume \\(f(x_\\infty) = 0\\) and \\(f^\\prime(x_\\infty)\\ne 0\\). Further we know \\(g^\\prime\\) is continuous because we assumed \\(f\\) was twice continuously differentiable. Therefore, given \\(K &lt; 1\\), there exists \\(\\delta &gt; 0\\) such that for all \\(x\\in(x_\\infty-\\delta, x_\\infty+\\delta)\\), we have \\(|g^\\prime(x)|&lt; K\\). For any \\(a, b\\in(x_\\infty-\\delta, x_\\infty+\\delta)\\) we can also write \\[\\begin{eqnarray*} |g(a)-g(b)| &amp; \\leq &amp; |g^\\prime(c)||a-b|\\\\ &amp; \\leq &amp; K|a-b| \\end{eqnarray*}\\] In the interval of \\(x_\\infty\\pm\\delta\\) we have that \\(g\\) is a shrinking map. Therefore, there exists a unique fixed point \\(x_\\infty\\) such that \\(g(x_\\infty)=x_\\infty\\). This value \\(x_\\infty\\) is a root of \\(f\\). 2.4.2 Convergence Rate of Newton’s Method Although proof of Newton’s method’s convergence to a root can be done using the Shrinking Lemma, the convergence rate of Newton’s method is considerably faster than the linear rate of generic shrinking maps. This fast convergence is obtained via the additional assumptions we make about the smoothness of the function \\(f\\). Suppose again that \\(f\\) is twice continuously differentiable and that there exists \\(x_\\infty\\) such that \\(f(x_\\infty) = 0\\). Given some small \\(\\varepsilon &gt; 0\\), we can approximate \\(f\\) around \\(x_\\infty\\) with \\[\\begin{eqnarray*} f(x_\\infty+\\varepsilon) &amp; = &amp; f(x_\\infty) + \\varepsilon f^\\prime(x_\\infty)+\\frac{\\varepsilon^2}{2}f^{\\prime\\prime}(x_\\infty) + O(\\varepsilon^2)\\\\ &amp; = &amp; 0 + \\varepsilon f^\\prime(x_\\infty)+\\frac{\\varepsilon^2}{2}f^{\\prime\\prime}(x_\\infty) + O(\\varepsilon^2) \\end{eqnarray*}\\] Additionally, we can approximate \\(f^\\prime\\) with \\[ f^\\prime(x_\\infty+\\varepsilon) = f^\\prime(x_\\infty) + \\varepsilon f^{\\prime\\prime}(x_\\infty) + O(\\varepsilon) \\] Recall that Newton’s method generates the sequence \\[ x_{n+1} = x_n - \\frac{f(x_n)}{f^\\prime(x_n)}. \\] Using the time-honored method of adding and subtracting, we can write this as \\[ x_{n+1} - x_\\infty = x_n - x_\\infty - \\frac{f(x_n)}{f^\\prime(x_n)}. \\] If we let \\(\\varepsilon_{n+1}=x_{n+1} - x_\\infty\\) and \\(\\varepsilon_n=x_n - x_\\infty\\), then we can rewrite the above as \\[ \\varepsilon_{n+1} = \\varepsilon_n - \\frac{f(x_n)}{f^\\prime(x_n)} \\] Further adding and subtracting (i.e. \\(x_n = x_\\infty + \\varepsilon_n\\)) gives us \\[ \\varepsilon_{n+1} = \\varepsilon_n - \\frac{f(x_\\infty+\\varepsilon_n)}{f^\\prime(x_\\infty+\\varepsilon_n)} \\] From here, we can use the approximations written out earlier to give us \\[\\begin{eqnarray*} \\varepsilon_{n+1} &amp; \\approx &amp; \\varepsilon_n - \\frac{\\varepsilon_n f^\\prime(x_\\infty) + \\frac{\\varepsilon_n^2}{2}f^{\\prime\\prime}(x_\\infty)}{f^\\prime(x_\\infty)+\\varepsilon_n f^{\\prime\\prime}(x_\\infty)}\\\\ &amp; = &amp; \\varepsilon_n^2\\left(\\frac{\\frac{1}{2}f^{\\prime\\prime}(x_\\infty)}{f^\\prime(x_\\infty) + \\varepsilon_n f^{\\prime\\prime}(x_\\infty)}\\right) \\end{eqnarray*}\\] Dividing by \\(\\varepsilon_n^2\\) on both sides gives us \\[ \\frac{\\varepsilon_{n+1}}{\\varepsilon_n^2} \\approx \\frac{\\frac{1}{2}f^{\\prime\\prime}(x_\\infty)}{f^\\prime(x_\\infty) + \\varepsilon_n f^{\\prime\\prime}(x_\\infty)} \\] As \\(\\varepsilon_n\\downarrow 0\\), we can say that there exists some \\(M&lt;\\infty\\) such that \\[ \\frac{|\\varepsilon_{n+1}|}{|\\varepsilon_n|^2} \\leq M \\] as \\(n\\rightarrow\\infty\\), which is the definition of quadratic convergence. Of course, for this to work we need that \\(f^{\\prime\\prime}(x_\\infty)&lt;\\infty\\) and that \\(f^\\prime(x_\\infty)\\ne 0\\). In summary, Newton’s method is very fast in the neighborhood of the root and furthermore has a direct multivariate generalization (unlike the bisection method). However, the need to evaluate \\(f^\\prime\\) at each iteration requires more computation (and more assumptions about the smoothness of \\(f\\)). Additionally, Newton’s method can, in a sense, be “too fast” in that there is no guarantee that each iteration of Newton’s method is an improvement (i.e. is closer to the root). In certain cases, Newton’s method can swing wildly out of control and diverge. Newton’s method is only guaranteed to converge in the neighborhood of the root; the exact size of that neighborhood is usually not known. 2.4.3 Newton’s Method for Maximum Likelihood Estimation In many statistical modeling applications, we have a likelihood function \\(L\\) that is induced by a probability distribution that we assume generated the data. This likelihood is typically parameterized by a vector \\(\\theta\\) and maximizing \\(L(\\theta)\\) provides us with the maximum likelihood estimate (MLE), or \\(\\hat{\\theta}\\). In practice, it makes more sense to maximize the log-likehood function, or \\(\\ell(\\theta)\\), which in many common applications is equivalent to solving the score equations \\(\\ell^\\prime(\\theta) = 0\\) for \\(\\theta\\). Newton’s method can be applied to generate a sequence that converges to the MLE \\(\\hat{\\theta}\\). If we assume \\(\\theta\\) is a \\(k\\times 1\\) vector, we can iterate \\[ \\theta_{n+1} = \\theta_n - \\ell^{\\prime\\prime}(\\theta_n)^{-1}\\ell^\\prime(\\theta_n) \\] where \\(\\ell^{\\prime\\prime}\\) is the Hessian of the log-likelihood function. Note that the formula above computes an inverse of a \\(k\\times k\\) matrix, which should serve as an immediate warning sign that this is not how the algorithm should be implemented. In practice, it may make more sense to solve the system of equations \\[ [\\ell^{\\prime\\prime}(\\theta_n)]\\theta_{n+1} = [\\ell^{\\prime\\prime}(\\theta_n)]\\theta_n-\\ell^\\prime(\\theta_n). \\] rather than invert \\(\\ell^{\\prime\\prime}(\\theta_n)\\) directly at every iteration. However, it may make sense to invert \\(\\ell^{\\prime\\prime}(\\theta_n)\\) at the very end of the algorithm to obtain the observed information matrix \\(-\\ell^{\\prime\\prime}(\\hat{\\theta})\\). This observed information matrix can be used to obtain asymptotic standard errors for \\(\\hat{\\theta}\\) for making inference about \\(\\theta\\). 2.4.3.1 Example: Estimating a Poisson Mean Suppose we observe data \\(x_1, x_2, \\dots, x_n\\stackrel{iid}{\\sim}\\text{Poisson}(\\mu)\\) and we would like to estimate \\(\\mu\\) via maximum likelihood. The log-likelihood induced by the Poisson model is \\[ \\ell(\\mu) = \\sum_{i=1}^n x_i\\log\\mu - \\mu = n\\bar{x}\\log\\mu - n\\mu \\] The score function is \\[ \\ell^\\prime(\\mu) = \\frac{n\\bar{x}}{\\mu}-n \\] It is clear that setting \\(\\ell^\\prime(\\mu)\\) to zero and solving for \\(\\mu\\) gives us that \\(\\hat{\\mu}=\\bar{x}\\). However, we can visualizing \\(\\ell^\\prime(\\mu)\\) and see how the Newton iteration would work in this simple scenario. set.seed(2017-08-09) x &lt;- rpois(100, 5) xbar &lt;- mean(x) n &lt;- length(x) score &lt;- function(mu) { n * xbar / mu - n } curve(score, .3, 10, xlab = expression(mu), ylab = expression(score(mu))) abline(h = 0, lty = 3) The figure above shows that this is clearly a nice smooth function for Newton’s method to work on. Recall that for the Newton iteration, we also need the second derivative, which in this case is \\[ \\ell^{\\prime\\prime}(\\mu) = -\\frac{n\\bar{x}}{\\mu^2} \\] So the Newton iteration is then \\[\\begin{eqnarray*} \\mu_{n+1} &amp; = &amp; \\mu_n - \\left[-\\frac{n\\bar{x}}{\\mu_n^2}\\right]^{-1}\\left(\\frac{n\\bar{x}}{\\mu_n}-n \\right)\\\\ &amp; = &amp; 2\\mu_n-\\frac{\\mu_n^2}{n} \\end{eqnarray*}\\] Using the functional programming aspects of R, we can write a function that executes the functional iteration of Newton’s method for however many times we which to run the algorithm. The following Iterate() code takes a function as argument and generates an “iterator” version of it where the number of iterations is an argument. Funcall &lt;- function(f, ...) f(...) Iterate &lt;- function(f, n = 1) { function(x) { Reduce(Funcall, rep.int(list(f), n), x, right = TRUE) } } Now we can pass a single iteration of the Newton step as an argument to the Iterate() function defined above. single_iteration &lt;- function(x) { 2 * x - x^2 / xbar } g &lt;- function(x0, n) { giter &lt;- Iterate(single_iteration, n) giter(x0) } Finally, to facilitate plotting of this function, it is helpful if our iterator function is vectorized with respect to n. The Vectorize() function can help us here. g &lt;- Vectorize(g, &quot;n&quot;) Let’s use a starting point of \\(\\mu_0 = 10\\). We can plot the score function along with the values of each of the Newton iterates for 7 iterations. par(mar = c(5, 5, 4, 1)) curve(score, .35, 10, xlab = expression(mu), ylab = expression(score(mu)), cex.axis = 0.8) abline(h = 0, lty = 3) iterates &lt;- g(10, 1:7) ## Generate values for 7 functional iterations with a starting value of 10. abline(v = c(10, iterates), lty = 2) axis(3, c(10, iterates), labels = c(0, 1:7), cex = 2, cex.axis = 0.8) mtext(&quot;Iteration #&quot;, at = 2, line = 2.5) We can see that by the 7th iteration we are quite close to the root, which in this case is 5.1. Another feature to note of Newton’s algorithm here is that when the function is relatively flat, the algorithm makes large moves either to the left or right. However, when the function is relatively steep, the moves are smaller in distance. This makes sense because the size of the deviation from the current iterate depends on the inverse of \\(\\ell^{\\prime\\prime}\\) at the current iterate. When \\(\\ell^\\prime\\) is flat, \\(\\ell^{\\prime\\prime}\\) will be small and hence its inverse large. "],
["general-optimization.html", "3 General Optimization", " 3 General Optimization The general optimization problem can be stated as follows. Given a fiunction \\(f: \\mathbb{R}^k\\rightarrow\\mathbb{R}\\), we want to find \\(\\min_{x\\in S}f(x)\\), where \\(S\\subset\\mathbb{R}^k\\). The general approach to solving this problem that we will discuss is called a line search method. With line search methods, given \\(f\\) and a current estimate \\(x_n\\) of the location of the minimum, we want to Choose a direction \\(p_n\\) in \\(k\\)-dimensional space; Choose a step length in the direction \\(p_n\\), usually by solving \\(\\min_{\\alpha&gt;0} f(x_n + \\alpha p_n)\\) to get \\(\\alpha_n\\) Update our estimate with \\(x_{n+1} = x_n + \\alpha_n p_n\\). Clearly then, with line search methods, the two questions one must answer are how should we choose the direction? and how far should we step? Almost all line search approaches provide variations on the answers to those two questions. Care must be taken in addressing the problems involved with line search methods because typically one must assume that the size of the parameter space is large (i.e. \\(k\\) is large). Therefore, one of constraints for all methods is minimizing the amount of computation that needs to be done due to the large parameter space. Efficiency with respect to memory (storage of data or parameters) and computation time is key. "],
["steepest-descent.html", "3.1 Steepest Descent", " 3.1 Steepest Descent Perhaps the most obvious direction to choose when attempting to minimize a function \\(f\\) starting at \\(x_n\\) is the direction of steepest descent, or \\(-f^\\prime(x_n)\\). This is the direction that is orthogonal to the contours of \\(f\\) at the point \\(x_n\\) and hence is the direction in which \\(f\\) is changing most rapidly at \\(x_n\\). Direction of steepest descent. The updating procedure for a steepest descent algorithm, given the current estimate \\(x_n\\), is then \\[ x_{n+1} = x_n - \\alpha_n f^\\prime(x_n) \\] While it might seem logical to always go in the direction of steepest descent, it can occasionally lead to some problems. In particular, when certain parameters are highly correlated with each other, the steepest descent algorithm can require many steps to reach the minimum. The figure below illustrates a function whose contours are highly correlated and hence elliptical. Steepest descent with highly correlated parameters. Depending on the starting value, the steepest descent algorithm could take many steps to wind its way towards the minimum. 3.1.1 Example: Multivariate Normal One can use steepest descent to compute the maximum likelihood estimate of the mean in a multivariate Normal density, given a sample of data. However, when the data are highly correlated, as they are in the simulated example below, the log-likelihood surface can be come difficult to optimize. In such cases, a very narrow ridge develops in the log-likelihood that can be difficult for the steepest descent algorithm to navigate. In the example below, we actually compute the negative log-likelihood because the algorith is designed to minimize functions. set.seed(2017-08-10) mu &lt;- c(1, 2) S &lt;- rbind(c(1, .9), c(.9, 1)) x &lt;- MASS::mvrnorm(500, mu, S) nloglike &lt;- function(mu1, mu2) { dmv &lt;- mvtnorm::dmvnorm(x, c(mu1, mu2), S, log = TRUE) -sum(dmv) } nloglike &lt;- Vectorize(nloglike, c(&quot;mu1&quot;, &quot;mu2&quot;)) nx &lt;- 40 ny &lt;- 40 xg &lt;- seq(-5, 5, len = nx) yg &lt;- seq(-5, 6, len = ny) g &lt;- expand.grid(xg, yg) nLL &lt;- nloglike(g[, 1], g[, 2]) z &lt;- matrix(nLL, nx, ny) par(mar = c(4.5, 4.5, 1, 1)) contour(xg, yg, z, nlevels = 40, xlab = expression(mu[1]), ylab = expression(mu[2])) abline(h = 0, v = 0, lty = 2) Note that in the figure above the surface is highly stretched and that the minimum \\((1, 2)\\) lies in the middle of a narrow valley. For the steepest descent algorithm we will start at the point \\((-5, -2)\\) and track the path of the algorithm. library(dplyr, warn.conflicts = FALSE) norm &lt;- function(x) x / sqrt(sum(x^2)) Sinv &lt;- solve(S) ## I know I said not to do this! step1 &lt;- function(mu, alpha = 1) { D &lt;- sweep(x, 2, mu, &quot;-&quot;) score &lt;- colSums(D) %&gt;% norm mu + alpha * drop(Sinv %*% score) } steep &lt;- function(mu, n = 10, ...) { results &lt;- vector(&quot;list&quot;, length = n) for(i in seq_len(n)) { results[[i]] &lt;- step1(mu, ...) mu &lt;- results[[i]] } results } m &lt;- do.call(&quot;rbind&quot;, steep(c(-5, -2), 8)) m &lt;- rbind(c(-5, -2), m) par(mar = c(4.5, 4.5, 1, 1)) contour(xg, yg, z, nlevels = 40, xlab = expression(mu[1]), ylab = expression(mu[2])) abline(h = 0, v = 0, lty = 2) points(m, pch = 20, type = &quot;b&quot;) We can see that the path of the algorthm is rather winding as it traverses the narrow valley. Now, we have fixed the step-length in this case, which is probably not optimal. However, one can still see that the algorithm has some difficulty navigating the surface because the direction of steepest descent does not take one directly towards the minimum ever. "],
["the-newton-direction.html", "3.2 The Newton Direction", " 3.2 The Newton Direction Given a current best estimate \\(x_n\\), we can approximate \\(f\\) with a quadratic polynomial. For some small \\(p\\), \\[ f(x_n + p) \\approx f(x_n) + p^\\prime f^\\prime(x_n) + \\frac{1}{2}p^\\prime f^{\\prime\\prime}(x_n)p. \\] If we minimize the right hand side with respect to \\(p\\), we obtain \\[ p_n = f^{\\prime\\prime}(x_n)^{-1}[-f^\\prime(x_n)] \\] which we can think of as the steepest descent direction “twisted” by the inverse of the Hessian matrix \\(f^{\\prime\\prime}(x_n)^{-1}\\). Newton’s method has a “natural” step length of \\(1\\), so that the updating procedure is \\[ x_{n+1} = x_n - f^{\\prime\\prime}(x_n)^{-1}f^\\prime(x_n). \\] Newton’s method makes a quadratic approximation to the target function \\(f\\) at each step of the algorithm. This follows the “optimization transfer” principle mentioned earlier, whereby we take a complex function \\(f\\), replace it with a simpler function \\(g\\) that is easier to optimize, and then optimize the simpler function repeatedly until convergence to the solution. We can visualize how Newton’s method makes its quadratic approximation to the target function easily in one dimension. curve(-dnorm(x), -2, 3, lwd = 2, ylim = c(-0.55, .1)) xn &lt;- -1.2 abline(v = xn, lty = 2) axis(3, xn, expression(x[n])) g &lt;- function(x) { -dnorm(xn) + (x-xn) * xn * dnorm(xn) - 0.5 * (x-xn)^2 * (dnorm(xn) - xn * (xn * dnorm(xn))) } curve(g, -2, 3, add = TRUE, col = 4) op &lt;- optimize(g, c(0, 3)) abline(v = op$minimum, lty = 2) axis(3, op$minimum, expression(x[n+1])) In the above figure, the next iterate, \\(x_{n+1}\\) is actually further away from the minimum than our previous iterate \\(x_n\\). The quadratic approximation that Newton’s method makes to \\(f\\) is not guaranteed to be good at every point of the function. This shows an important “feature” of Newton’s method, which is that it is not monotone. The successive iterations that Newton’s method produces are not guaranteed to be improvements in the sense that each iterate is closer to the truth. The tradeoff here is that while Newton’s method is very fast (quadratic convergence), it can be unstable at times. Monotone algorithms (like the EM algorithm that we discuss later) that always produce improvements, are more stable, but generally converge at slower rates. In the next figure, however, we can see that the solution provided by the next approximation, \\(x_{n+2}\\), is indeed quite close to the true minimum. curve(-dnorm(x), -2, 3, lwd = 2, ylim = c(-0.55, .1)) xn &lt;- -1.2 op &lt;- optimize(g, c(0, 3)) abline(v = op$minimum, lty = 2) axis(3, op$minimum, expression(x[n+1])) xn &lt;- op$minimum curve(g, -2, 3, add = TRUE, col = 4) op &lt;- optimize(g, c(0, 3)) abline(v = op$minimum, lty = 2) axis(3, op$minimum, expression(x[n+2])) It is worth noting that in the rare event that \\(f\\) is in fact a quadratic polynomial, Newton’s method will converge in a single step because the quadratic approximation that it makes to \\(f\\) will be exact. 3.2.1 Generalized Linear Models The generalized linear model is an extension of the standard linear model to allow for non-Normal response distributions. The distributions used typically come from an exponential family whose density functions share some common characteristics. With a GLM, we typical present it as \\(y_i\\sim p(y_i\\mid\\mu_i)\\), where \\(p\\) is an exponential family distribution, \\(\\mathbb{E}[y_i]=\\mu_i\\), \\[ g(\\mu_i) = x_i^\\prime\\beta, \\] where \\(g\\) is a nonlinear link function, and \\(\\text{Var}(y_i) = V(\\mu)\\) where \\(V\\) is a known variance function. Unlike the standard linear model, the maximum likelihood estimate of the parameter vector \\(\\beta\\) cannot be obtained in closed form, so an iterative algorithm must be used to obtain the estimate. The traditional algorithm used is the Fisher scoring algorithm. This algorithm uses a linear approximation to the nonlinear link function \\(g\\), which can be written as \\[ g(y_i)\\approx g(\\mu_i) + (y_i-\\mu_i)g^\\prime(\\mu_i). \\] The typical notation of GLMs refers to \\(z_i=g(\\mu_i) + (y_i-\\mu_i)g^\\prime(\\mu_i)\\) as the working response. The Fisher scoring algorithm then works as follows. Start with \\(\\hat{\\mu}_i\\), some initial value. Compute \\(z_i = g(\\hat{\\mu}_i) + (y_i-\\hat{\\mu}_i)g^\\prime(\\hat{\\mu}_i)\\). Given the \\(n\\times 1\\) vector of working responses \\(z\\) and the \\(n\\times p\\) predictor matrix \\(X\\) we compute a weighted regression of \\(z\\) on \\(X\\) to get \\[ \\beta_n = (X^\\prime WX)^{-1}X^\\prime Wz \\] where \\(W\\) is a diagonal matrix with diagonal elements \\[ w_{ii} = \\left[g^\\prime(\\mu_i)^2V(\\mu_i)\\right]^{-1}. \\] Given \\(\\beta_n\\), we can recompute \\(\\hat{\\mu}_i=g^{-1}(x_i^\\prime\\beta_n)\\) and go to 2. Note that in Step 3 above, the weights are simply the inverses of the variance of \\(z_i\\), i.e. \\[\\begin{eqnarray*} \\text{Var}(z_i) &amp; = &amp; \\text{Var}(g(\\mu_i) + (y_i-\\mu_i)g^\\prime(\\mu_i))\\\\ &amp; = &amp; \\text{Var}((y_i-\\mu_i)g^\\prime(\\mu_i))\\\\ &amp; = &amp; V(\\mu_i)g^\\prime(\\mu_i)^2 \\end{eqnarray*}\\] Naturally, when doing a weighted regression, we would weight by the inverse of the variances. 3.2.1.1 Example: Poisson Regression For a Poisson regression, we have \\(y_i\\sim\\text{Poisson}(\\mu_i)\\) where \\(g(\\mu) = \\log\\mu_i = x_i^\\prime\\beta\\) because the log is the canonical link function for the Poisson distribution. We also have \\(g^\\prime(\\mu_i) = \\frac{1}{\\mu_i}\\) and \\(V(\\mu_i) = \\mu_i\\). Therefore, the Fisher scoring algorithm is Initialize \\(\\hat{\\mu}_i\\), perhaps using \\(y_i + 1\\) (to avoid zeros). Let \\(z_i = \\log\\hat{\\mu}_i + (y_i-\\hat{\\mu}_i)\\frac{1}{\\hat{\\mu}_i}\\) Regression \\(z\\) on \\(X\\) using the weights \\[ w_{ii} = \\left[\\frac{1}{\\hat{\\mu}_i^2}\\hat{\\mu}_i\\right]^{-1} = \\hat{\\mu}_i. \\] Using the Poisson regression example, we can draw a connection between the usual Fisher scoring algorithm for fitting GLMs and Newton’s method. Recall that if \\(\\ell(\\beta)\\) is the log-likelihood as a function of the regression paramters \\(\\beta\\), then the Newton updating scheme is \\[ \\beta_{n+1} = \\beta_n + \\ell^{\\prime\\prime}(\\beta_n)^{-1}[-\\ell^\\prime(\\beta_n)]. \\] The log-likelihoood for a Poisson regression model can be written in vector/matrix form as \\[ \\ell(\\beta) = y^\\prime X\\beta - \\exp(X\\beta)^\\prime\\mathbf{1} \\] where the exponential is taken component-wise on the vector \\(X\\beta\\). The gradient function is \\[ \\ell^\\prime(\\beta) = X^\\prime y - X^\\prime \\exp(X\\beta) = X^\\prime(y-\\mu) \\] and the Hessian is \\[ \\ell^{\\prime\\prime}(\\beta) = -X^\\prime W X \\] where \\(W\\) is a diagonal matrix with the values \\(w_{ii} = \\exp(x_i^\\prime\\beta)\\) on the diagonal. The Newton iteration is then \\[\\begin{eqnarray*} \\beta_{n+1} &amp; = &amp; \\beta_n + (-X^\\prime WX)^{-1}(-X^\\prime(y-\\mu))\\\\ &amp; = &amp; \\beta_n + (X^\\prime WX)^{-1}XW(z - X\\beta_n)\\\\ &amp; = &amp; (X^\\prime WX)^{-1}X^\\prime Wz + \\beta_n - (X^\\prime WX)^{-1}X^\\prime WX\\beta_n\\\\ &amp; = &amp; (X^\\prime WX)^{-1}X^\\prime Wz \\end{eqnarray*}\\] Therefore the iteration is exactly the same as the Fisher scoring algorithm in this case. In general, Newton’s method and Fisher scoring will coincide with any generalized linear model using an exponential family with a canonical link function. 3.2.2 Newton’s Method in R The nlm() function in R implements Newton’s method for minimizing a function given a vector of starting values. By default, one does not need to supply the gradient or Hessian functions; they will be estimated numerically by the algorithm. However, for the purposes of improving accuracy of the algorithm, both the gradient and Hessian can be supplied as attributes of the target function. As an example, we will use the nlm() function to fit a simple logistic regression model for binary data. This model specifies that \\(y_i\\sim\\text{Bernoulli}(p_i)\\) where \\[ \\log\\frac{p_i}{1-p_i} = \\beta_0 + x_i \\beta_1 \\] and the goal is to estimate \\(\\beta\\) via maximum likelihood. Given the assumed Bernoulli distribution, we can write the log-likelihood for a single observation as \\[\\begin{eqnarray*} \\log L(\\beta) &amp; = &amp; \\log\\left\\{\\prod_{i=1}^n p_i^{y_i}(1-p_i)^{1-y_i}\\right\\}\\\\ &amp; = &amp; \\sum_{i=1}^n y_i\\log p_i + (1-y_i)\\log(1-p_i)\\\\ &amp; = &amp; \\sum_{i=1}^n y_i\\log\\frac{p_i}{1-p_i}+\\log(1-p_i)\\\\ &amp; = &amp; \\sum_{i=1}^n y_i(\\beta_0 + x_i\\beta_1) + \\log\\left(\\frac{1}{1+e^{(\\beta_0 + x_i\\beta_1)}}\\right)\\\\ &amp; = &amp; \\sum_{i=1}^n y_i(\\beta_0 + x_i\\beta_1) -\\log\\left(1+e^{(\\beta_0 + x_i\\beta_1)}\\right) \\end{eqnarray*}\\] If we take the very last line of the above derivation and take a single element inside the sum, we have \\[ \\ell_i(\\beta) = y_i(\\beta_0 + x_i\\beta_1) -\\log\\left(1+e^{(\\beta_0 + x_i\\beta_1)}\\right) \\] We will need the gradient and Hessian of this with respect to \\(\\beta\\). Because the sum and the derivative are exchangeable, we can then sum each of the individual gradients and Hessians to get the full gradient and Hessian for the entire sample, so that \\[ \\ell^\\prime(\\beta) = \\sum_{i=1}^n\\ell_i^\\prime(\\beta) \\] and \\[ \\ell^{\\prime\\prime}(\\beta) = \\sum_{i=1}^n \\ell_i^{\\prime\\prime}(\\beta). \\] Now, taking the gradient and Hessian of the above expression may be mildly inconvenient, but it is far from impossible. Nevertheless, R provides an automated way to do symbolic differentiation so that manual work can be avoided. The deriv() function computes the gradient and Hessian of an expression symbolically so that it can be used in minimization routines. It cannot compute gradients of arbitrary expressions, but it it does support a wide range of common statistical functions. 3.2.2.1 Example: Trends in p-values Over Time The tidypvals package written by Jeff Leek contains datasets taken from the literature collecting p-values associated with various publications along with some information about those publications (i.e. journal, year, DOI). One question that comes up is whether there has been any trend over time in the claimed statistical significance of publications, where “statistical significance” is defined as having a p-value less than \\(0.05\\). The tidypvals package is available from GitHub and can be installed using the install_github() function in the remotes package. remotes::install_github(&quot;jtleek/tidypvals&quot;) Once installed, we will make use of the jager2014 dataset. In particular, we are interseted in creating an indicator of whether a p-value is less than \\(0.05\\) and regressing it on the year variable. library(tidypvals) library(dplyr) jager &lt;- mutate(tidypvals::jager2014, pvalue = as.numeric(as.character(pvalue)), y = ifelse(pvalue &lt; 0.05 | (pvalue == 0.05 &amp; operator == &quot;lessthan&quot;), 1, 0), x = year - 2000) %&gt;% tbl_df Note here that we have subtracted the year 2000 off of the year variable so that \\(x=0\\) corresponds to year == 2000. Next we compute the gradient and Hessian of the negative log-likelihood with respect to \\(\\beta_0\\) and \\(\\beta_1\\) using the deriv() function. Below, we specify function.arg = TRUE in the call to deriv() because we want deriv() to return a function whose arguments are b0 and b1. nll_one &lt;- deriv(~ -(y * (b0 + x * b1) - log(1 + exp(b0 + b1 * x))), c(&quot;b0&quot;, &quot;b1&quot;), function.arg = TRUE, hessian = TRUE) Here’s what that function looks like. nll_one function (b0, b1) { .expr6 &lt;- exp(b0 + b1 * x) .expr7 &lt;- 1 + .expr6 .expr11 &lt;- .expr6/.expr7 .expr15 &lt;- .expr7^2 .expr18 &lt;- .expr6 * x .expr19 &lt;- .expr18/.expr7 .value &lt;- -(y * (b0 + x * b1) - log(.expr7)) .grad &lt;- array(0, c(length(.value), 2L), list(NULL, c(&quot;b0&quot;, &quot;b1&quot;))) .hessian &lt;- array(0, c(length(.value), 2L, 2L), list(NULL, c(&quot;b0&quot;, &quot;b1&quot;), c(&quot;b0&quot;, &quot;b1&quot;))) .grad[, &quot;b0&quot;] &lt;- -(y - .expr11) .hessian[, &quot;b0&quot;, &quot;b0&quot;] &lt;- .expr11 - .expr6 * .expr6/.expr15 .hessian[, &quot;b0&quot;, &quot;b1&quot;] &lt;- .hessian[, &quot;b1&quot;, &quot;b0&quot;] &lt;- .expr19 - .expr6 * .expr18/.expr15 .grad[, &quot;b1&quot;] &lt;- -(y * x - .expr19) .hessian[, &quot;b1&quot;, &quot;b1&quot;] &lt;- .expr18 * x/.expr7 - .expr18 * .expr18/.expr15 attr(.value, &quot;gradient&quot;) &lt;- .grad attr(.value, &quot;hessian&quot;) &lt;- .hessian .value } The function nll_one() produced by deriv() evaluates the negative log-likelihood for each data point. The output from nll_one() will have attributes &quot;gradient&quot; and &quot;hessian&quot; which represent the gradient and Hessian, respectively. For example, using the data from the jager dataset, we can evaluate the negative log-likelihood at \\(\\beta_0=0, \\beta_1=0\\). x &lt;- jager$x y &lt;- jager$y str(nll_one(0, 0)) atomic [1:15653] 0.693 0.693 0.693 0.693 0.693 ... - attr(*, &quot;gradient&quot;)= num [1:15653, 1:2] -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 ... ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ : NULL .. ..$ : chr [1:2] &quot;b0&quot; &quot;b1&quot; - attr(*, &quot;hessian&quot;)= num [1:15653, 1:2, 1:2] 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 ... ..- attr(*, &quot;dimnames&quot;)=List of 3 .. ..$ : NULL .. ..$ : chr [1:2] &quot;b0&quot; &quot;b1&quot; .. ..$ : chr [1:2] &quot;b0&quot; &quot;b1&quot; The nll_one() function evaluates the negative log-likelihood at each data point, but does not sum the points up as would be required to evaluate the full negative log-likelihood. Therefore, we will write a separate function that does that for the negative log-likelihood, gradient, and Hessian. nll &lt;- function(b) { v &lt;- nll_one(b[1], b[2]) f &lt;- sum(v) ## log-likelihood gr &lt;- colSums(attr(v, &quot;gradient&quot;)) ## gradient vector hess &lt;- apply(attr(v, &quot;hessian&quot;), c(2, 3), sum) ## Hessian matrix attributes(f) &lt;- list(gradient = gr, hessian = hess) f } Now, we can evaluate the full negative log-likelihood with the nll() function. Note that nll() takes a single numeric vector as input as this is what the nlm() function is expecting. nll(c(0, 0)) [1] 10849.83 attr(,&quot;gradient&quot;) b0 b1 -4586.5 -21854.5 attr(,&quot;hessian&quot;) b0 b1 b0 3913.25 19618.25 b1 19618.25 137733.75 Using \\(\\beta_0=0,\\beta_1=0\\) as the initial value, we can call nlm() to minimize the negative log-likelihood. res &lt;- nlm(nll, c(0, 0)) res $minimum [1] 7957.248 $estimate [1] 1.54208200 -0.04029523 $gradient [1] -20.247726 -7.013226 $code [1] 4 $iterations [1] 100 Note first in the output that there is a code with the value 4 and that the number of iterations is 100. Whenever the number of iterations in an optimization algorithm is a nice round number, the chances are good that it it some preset iteration limit. This in turn usually means the algorithm didn’t converge. In the help for nlm() we also learn that the code value of 4 means “iteration limit exceeded”, which is generally not good. Luckily, the solution is simple: we can increase the iteration limit and let the algorithm run longer. res &lt;- nlm(nll, c(0, 0), iterlim = 1000) res $minimum [1] 7956.976 $estimate [1] 1.57028963 -0.04415989 $gradient [1] -0.027396191 -0.009546944 $code [1] 2 $iterations [1] 260 Here we see that the number of iterations used was 260, which is well below the iteration limit. Now we get code equal to 2 which means that “successive iterates within tolerance, current iterate is probably solution”. Sounds like good news! Lastly, most optimization algorithms have an option to scale your parameter values so that they roughly vary on the same scale. If your target function has paramters that vary on wildly different scales, this can cause a practical problem for the computer (it’s not a problem for the theory). The way to deal with this in nlm() is to use the typsize arguemnt, which is a vector equal in length to the parameter vector which provides the relative sizes of the parameters. Here, I give typsize = c(1, 0.1), which indicates to nlm() that the first paramter, \\(\\beta_0\\), should be roughly \\(10\\) times larger than the second parameter, \\(\\beta_1\\) when the target function is at its minimum. res &lt;- nlm(nll, c(0, 0), iterlim = 1000, typsize = c(1, 0.1)) res $minimum [1] 7956.976 $estimate [1] 1.57030986 -0.04416181 $gradient [1] -0.001604698 0.077053181 $code [1] 1 $iterations [1] 48 Running this call to nlm() you’ll notice that the solution is the same but the number of iterations is actually much less than before (48 iterations) which means the algorithm ran faster. Generally speaking, scaling the parameter vector appropriately (if possible) improves the performance of all optimization algorithms and in my experience is almost always a good idea. The specific values given to the typsize argument are not important; rather their relationships to each other (i.e. orders of magnitude) are what matter. "],
["quasi-newton.html", "3.3 Quasi-Newton", " 3.3 Quasi-Newton Quasi-Newton methods arise from the desire to use something like Newton’s method for its speed but without having to compute the Hessian matrix each time. The idea is that if the Newton iteration is \\[ \\theta_{n+1} = \\theta_n-f^{\\prime\\prime}(\\theta_n)^{-1}f^\\prime(\\theta_n) \\] is there some other matrix that we can use to replace either \\(f^{\\prime\\prime}(\\theta_n)\\) or \\(f^{\\prime\\prime}(\\theta_n)^{-1}\\)? That is can we use a revised iteration, \\[ \\theta_{n+1} = \\theta_n-B_n^{-1}f^\\prime(\\theta_n) \\] where \\(B_n\\) is simpler to compute but still allows the algorithm to converge quickly? This is a challenging problem because \\(f^{\\prime\\prime}(\\theta_n)\\) gives us a lot of information about the surface of \\(f\\) at \\(\\theta_n\\) and throwing out this information results in, well, a severe loss of information. The idea with Quasi-Newton is to find a solution \\(B_n\\) to the problem \\[ f^\\prime(\\theta_n)-f^\\prime(\\theta_{n-1}) = B_n (\\theta_n-\\theta_{n-1}). \\] The equation above is sometimes referred to as the secant equation. Note first that this requires us to store two values, \\(\\theta_n\\) and \\(\\theta_{n-1}\\). Also, in one dimension, the solution is trivial: we can simply divide the left-hand-side by \\(\\theta_n-\\theta_{n-1}\\). However, in more than one dimension, there exists an infinite number of solutions and we need some way to constrain the problem to arrive at a sensible answer. The key to Quasi-Newton approaches in general is that while we initially may not have much information about \\(f\\), with each iteration we obtain just a little bit more. Specifically, we learn more about the Hessian matrix through successive differences in \\(f^\\prime\\). Therefore, with each iteration we can incorporate this newly obtained information into our estimate of the Hessian matrix. The constraints placed on the matrix \\(B_n\\) is that it be symmetric and that it be close to \\(B_{n-1}\\). These constraints can be satisfied by updating \\(B_n\\) via the addition of rank one matrices. If we let \\(y_n = f^\\prime(\\theta_n)-f^\\prime(\\theta_{n-1})\\) and \\(s_n = \\theta_n-\\theta_{n-1}\\), then the secant equation is \\(y_n = B_ns_n\\). One updating procedures for \\(B_n\\) \\[ B_{n} = B_{n-1} + \\frac{y_ny_n^\\prime}{y_n^\\prime s_n} - \\frac{B_{n-1}s_ns_n^\\prime B_{n-1}^\\prime}{s_n^\\prime B_{n-1}s_n} \\] The above updating procedure was developed by Broyden, Fletcher, Goldfarb, and Shanno (BFGS). An analogous approach, which solves the following secant equation, \\(H_n y_n = s_n\\) was proposed by Davidon, Fletcher, and Powell (DFP). Note that in the case of the BFGS method, we actually use \\(B_n^{-1}\\) in the Newton update. However, it is not necessary to solve for \\(B_{n}\\) and then invert it directly. We can directly update \\(B_{n-1}^{-1}\\) to produce \\(B_{n}^{-1}\\) via the Sherman-Morrison update formula. This formula allows us to generate the new inverse matrix by using the previous inverse and some matrix multiplication. 3.3.1 Quasi-Newton Methods in R Quasi-Newton methods in R can be accessed through the optim() function, which is a general purpose optimization function. The optim() function implements a variety of methods but in this section we will focus on the &quot;BFGS&quot; and &quot;L-BFGS-B&quot;methods. 3.3.1.1 Example: Truncated Normal and Mixture of Normal Distributions The data were obtained from the U.S. Environmental Protection Agency’s Air Quality System web page. For this example we will be using the daily average concentrations of nitrogen dioxide (NO2) for 2016 found in this file. In particular, we will focus on the data for monitors located in Washington State. library(readr) library(tidyr) dat0 &lt;- read_csv(&quot;daily_42602_2016.csv.bz2&quot;) names(dat0) &lt;- make.names(names(dat0)) dat &lt;- filter(dat0, State.Name == &quot;Washington&quot;) %&gt;% unite(site, State.Code, County.Code, Site.Num) %&gt;% rename(no2 = Arithmetic.Mean, date = Date.Local) %&gt;% select(site, date, no2) A kernel density estimate of the NO2 data shows the following distribution. library(ggplot2) ggplot(dat, aes(x = no2)) + geom_density() As an initial stab at characterizing the distribution of the NO2 values (and to demonstrate the use of optim() for fitting models), we will try to fit a truncated Normal model to the data. The truncated Normal can make sense for these kinds of data because they are strictly positive, making a standard Normal distribution inappropriate. For the truncated normal, truncated from below at \\(0\\), the density of the data is \\[ f(x) = \\frac{\\frac{1}{\\sigma}\\varphi\\left(\\frac{x-\\mu}{\\sigma}\\right)}{\\int_0^\\infty \\frac{1}{\\sigma}\\varphi\\left(\\frac{x-\\mu}{\\sigma}\\right)\\,dx}. \\] The unknown parameters are \\(\\mu\\) and \\(\\sigma\\). Given the density, we can attempt to estimate \\(\\mu\\) and \\(\\sigma\\) by maximum likelihood. In this case, we will minimize the negative log-likelihood of the data. We can use the deriv() function to compute the negative log-likelihood and its gradient automatically. Because we are using quasi-Newton methods here we do not need the Hessian matrix. nll_one &lt;- deriv(~ -log(dnorm((x - mu)/s) / s) + log(0.5), c(&quot;mu&quot;, &quot;s&quot;), function.arg = TRUE) The optim() function works a bit differently from nlm() in that instead of having the gradient as an attribute of the negative log-likelhood, the gradient needs to be a separate function. First the negative log-likelihood. nll &lt;- function(p) { v &lt;- nll_one(p[1], p[2]) sum(v) } Then the gradient function. nll_grad &lt;- function(p) { v &lt;- nll_one(p[1], p[2]) colSums(attr(v, &quot;gradient&quot;)) } Now we can pass the nll() and nll_grad() functions to optim() to obtain estimates of \\(\\mu\\) and \\(\\sigma\\). We will use starting values of \\(\\mu=1\\) and \\(\\sigma=5\\). To use the &quot;BFGS&quot; quasi-Newton method you need to specify it in the method argument. The default method for optim() is the Nelder-Mead simplex method. We also specify hessian = TRUE to tell optim() to numerically calculate the Hessian matrix at the optimum point. x &lt;- dat$no2 res &lt;- optim(c(1, 5), nll, gr = nll_grad, method = &quot;BFGS&quot;, hessian = TRUE) Warning in log(.expr4): NaNs produced Warning in log(.expr4): NaNs produced Warning in log(.expr4): NaNs produced res $par [1] 13.23731 8.26315 $value [1] 4043.641 $counts function gradient 35 19 $convergence [1] 0 $message NULL $hessian [,1] [,2] [1,] 2.087005e+01 5.659674e-04 [2,] 5.659674e-04 4.174582e+01 The optim() function returns a list with 5 elements (plus a Hessian matrix if hessian = TRUE is set). The first element that you should check is the onvergence code. If convergece is 0, that is good. Anything other than 0 could indicate a problem, the nature of which depends on the algorithm you are using (see the help page for optim() for more details). This time we also had optim() compute the Hessian (numerically) at the optimal point so that we could derive asymptotic standard errors if we wanted. First note that there were a few messages printed to the console while the algorithm was running indicating that NaNs were produced by the target function. This is likely because the function was attempting to take the log of negative numbers. Because we used the &quot;BFGS&quot; algorithm, we were conducting an unconstrained optimization. Therefore, it’s possible that the algorithm’s search produced negative values for \\(\\sigma\\), which don’t make sense in this context. In order to constrain the search, we can use the &quot;L-BFGS-B&quot; methods which is a “limited memory” BFGS algorithm with “box constraints”. This allows you to put a lower and upper bound on each parameter in the model. Note that optim() allows your target function to produce NA or NaN values, and indeed from the output it seems that the algorithm eventually converged on the answer anyway. But since we know that the parameters in this model are constrained, we can go ahead and use the alternate approach. Here we set the lower bound for all parameters to be 0 but allow the upper bound to be infinity (Inf), which is the default. res &lt;- optim(c(1, 5), nll, gr = nll_grad, method = &quot;L-BFGS-B&quot;, hessian = TRUE, lower = 0) res $par [1] 13.237470 8.263546 $value [1] 4043.641 $counts function gradient 14 14 $convergence [1] 0 $message [1] &quot;CONVERGENCE: REL_REDUCTION_OF_F &lt;= FACTR*EPSMCH&quot; $hessian [,1] [,2] [1,] 20.868057205 -0.000250771 [2,] -0.000250771 41.735838073 We can see now that the warning messages are gone, but the solution is identical to that produced by the original &quot;BFGS&quot; method. The maximum likelihood estimate of \\(\\mu\\) is 13.24 and the estimate of \\(\\sigma\\) is 8.26. If we wanted to obtain asymptotic standard errors for these parameters, we could look at the Hessian matrix. solve(res$hessian) %&gt;% diag %&gt;% sqrt [1] 0.2189067 0.1547909 In this case though, we don’t care much for the standard errors so we will move on. We can plot the original density smooth of the data versus the fitted truncated Normal model to see how well we charaterize the distribution. First we will evaluate the fitted model at 100 points between 0 and 50. xpts &lt;- seq(0, 50, len = 100) dens &lt;- data.frame(xpts = xpts, ypts = dnorm(xpts, res$par[1], res$par[2])) Then we can overlay the fitted model on top of the density using geom_line(). ggplot(dat, aes(x = no2)) + geom_density() + geom_line(aes(x = xpts, y = ypts), data = dens, col = &quot;steelblue&quot;, lty = 2) It’s not a great fit. Looking at the density smooth of the data, it’s clear that there are two modes to the data, suggesting that a truncated Normal might not be sufficient to characterize the data. One alternative in this case would be a mixture of two Normals, which might capture the two modes. For a two-component mixture, the density for the data would be \\[ f(x) = \\lambda\\frac{1}{\\sigma}\\varphi\\left(\\frac{x-\\mu_1}{\\sigma_1}\\right) + (1-\\lambda)\\frac{1}{\\sigma}\\varphi\\left(\\frac{x-\\mu_2}{\\sigma_2}\\right). \\] Commonly, we see that this model is fit using more complex algorithms like the EM algorithm or Markov chain Monte Carlo methods. While those methods do provide greater stability in the estimation process (as we will see later), we can in fact use Newton-type methods to maximize the likelihood directly with a little care. First we can write out the negative log-likelihood symbolically and allow R’s deriv() function to compute the gradient function. nll_one &lt;- deriv(~ -log(lambda * dnorm((x-mu1)/s1)/s1 + (1-lambda)*dnorm((x-mu2)/s2)/s2), c(&quot;mu1&quot;, &quot;mu2&quot;, &quot;s1&quot;, &quot;s2&quot;, &quot;lambda&quot;), function.arg = TRUE) Then, as before, we can specify separate negative log-likelihood (nll) and gradient R functions (nll_grad). nll &lt;- function(p) { p &lt;- as.list(p) v &lt;- do.call(&quot;nll_one&quot;, p) sum(v) } nll_grad &lt;- function(p) { v &lt;- do.call(&quot;nll_one&quot;, as.list(p)) colSums(attr(v, &quot;gradient&quot;)) } Finally, we can pass those functions into optim() with an initial vector of parameters. Here, we are careful to specify We are using the &quot;L-BFGS-B&quot; method so that we specify a lower bound of 0 for all parameters and an upper bound of 1 for the \\(\\lambda\\) parameter We set the parscale option in the list of control parameters, which is similar to the typsize argument to nlm(). The goal here is to give optim() a scaling for each parameter around the optimal point. x &lt;- dat$no2 pstart &lt;- c(5, 10, 2, 3, 0.5) res &lt;- optim(pstart, nll, gr = nll_grad, method = &quot;L-BFGS-B&quot;, control = list(parscale = c(2, 2, 1, 1, 0.1)), lower = 0, upper = c(Inf, Inf, Inf, Inf, 1)) The algorithm appears to run without any warnings or messages. We can take a look at the output. res $par [1] 3.7606598 16.1469811 1.6419640 7.2378153 0.2348927 $value [1] 4879.924 $counts function gradient 17 17 $convergence [1] 0 $message [1] &quot;CONVERGENCE: REL_REDUCTION_OF_F &lt;= FACTR*EPSMCH&quot; The convergence code of 0 is a good sign and the parameter estimates in the par vector all seem reasonable. We can overlay the fitted model on to the density smooth to see how the model does. xpts &lt;- seq(0, 50, len = 100) dens &lt;- with(res, { data.frame(xpts = xpts, ypts = par[5]*dnorm(xpts, par[1], par[3]) + (1-par[5])*dnorm(xpts, par[2], par[4])) }) ggplot(dat, aes(x = no2)) + geom_density() + geom_line(aes(x = xpts, y = ypts), data = dens, col = &quot;steelblue&quot;, lty = 2) The fit is still not wonderful, but at least this model captures roughly the locations of the two modes in the density. Also, it would seem that the model captures the tail of the density reasonably well, although this would need to be checked more carefully by looking at the quantiles. Finally, as with most models and optimization schemes, it’s usually a good idea to vary the starting points to see if our current estimate is a local mode. pstart &lt;- c(1, 20, 5, 2, 0.1) res &lt;- optim(pstart, nll, gr = nll_grad, method = &quot;L-BFGS-B&quot;, control = list(parscale = c(2, 2, 1, 1, 0.1)), lower = 0, upper = c(Inf, Inf, Inf, Inf, 1)) res $par [1] 3.760571 16.146834 1.641961 7.237776 0.234892 $value [1] 4879.924 $counts function gradient 22 22 $convergence [1] 0 $message [1] &quot;CONVERGENCE: REL_REDUCTION_OF_F &lt;= FACTR*EPSMCH&quot; Here we see that with a slightly different starting point we get the same values and same minimum negative log-likelihood. "],
["conjugate-gradient.html", "3.4 Conjugate Gradient", " 3.4 Conjugate Gradient Conjugate gradient methods represent a kind of steepest descent approach “with a twist”. With steepest descent, we begin our minimization of a function \\(f\\) starting at \\(x_0\\) by traveling in the direction of the negative gradient \\(-f^\\prime(x_0)\\). In subsequent steps, we continue to travel in the direction of the negative gradient evaluated at each successive point until convergence. The conjugate gradient approach begins in the same manner, but diverges from steepest descent after the first step. In subsequent steps, the direction of travel must be conjugate to the direction most recently traveled. Two vectors \\(u\\) and \\(v\\) are conjugate with respect to the matrix \\(A\\) if \\(u^\\prime Av = 0\\). Before we go further, let’s take a concrete example. Suppose we want to minimize the quadratic function \\[ f(x) = \\frac{1}{2} x^\\prime Ax - x^\\prime b \\] where \\(x\\) is a \\(p\\)-dimensional vector and \\(A\\) is a \\(p\\times p\\) symmetric matrix. Starting at a point \\(x_0\\), both steepest descent and conjugate gradient would take us in the direction of \\(p_0 = -f^\\prime(x_0) = b-Ax_0\\), which is the negative gradient. Once we have moved in that direction to the point \\(x_1\\), the next direction, \\(p_1\\) must satisfy \\(p_0^\\prime A p_1 = 0\\). So we can begin with the steepest descent direction \\(-f^\\prime(x_1)\\) but then we must modify it to make it conjugate to \\(p_0\\). The constraint \\(p_0^\\prime Ap_1=0\\) allows us to back calculate the next direction, starting with \\(-f^\\prime(x_1)\\), because we have \\[ p_0^\\prime A\\left(-f^\\prime - \\frac{p_0^\\prime A(-f^\\prime)}{p_0^\\prime Ap_0}p_0\\right)=0. \\] Without the presence of the matrix \\(A\\), this is process is simply Gram-Schmidt orthogonalization. We can continue with this process, each time taking the steepest descent direction and modifying it to make it conjugate with the previous direction. The conjugate gradient process is (somewhat) interesting here because for minimizing a \\(p\\)-dimensional quadratic function it will converge within \\(p\\) steps. In reality, we do not deal with exactly quadratic functions and so the above-described algorithm is not feasible. However, the nonlinear conjugate gradient method draws from these ideas and develops a reasonable algorithm for finding the minimum of an arbitrary smooth function. It has the feature that it only requires storage of two gradient vectors, which for large problems with many parameters, is a significant savings in storage versus Newton-type algorithms which require storage of a gradient vector and a \\(p\\times p\\) Hessian matrix. The Fletcher-Reeves nonlinear conjugate gradient algorithm works as follows. Starting with \\(x_0\\), Let \\(p_0 = -f^\\prime(x_0)\\). Solve \\[ \\min_{\\alpha&gt;0} f(x_0 + \\alpha p_0) \\] for \\(\\alpha\\) and set \\(x_1=x_0+\\alpha p_0\\). For \\(n = 1, 2, \\dots\\) let \\(r_n = -f^\\prime(x_n)\\) and \\(r_{n-1}=-f^\\prime(x_{n-1})\\). Then set \\[ \\beta_n = \\frac{r_n^\\prime r_n}{r_{n-1}^\\prime r_{n-1}}. \\] Finally, update \\(p_{n} = r_n + \\beta_n * p_{n-1}\\), and let \\[ x_{n+1} = x_n + \\alpha^\\star p_{n}, \\] where \\(\\alpha^\\star\\) is the solution to the problem \\(\\min_{\\alpha &gt; 0}f(x_n + \\alpha p_n)\\). Check convergence and if not converged, repeat. It is perhaps simpler to describe this method with an illustration. Here, we show the contours of a 2-dimensional function. f &lt;- deriv(~ x^2 + y^2 + a * x * y, c(&quot;x&quot;, &quot;y&quot;), function.arg = TRUE) a &lt;- 1 n &lt;- 40 xpts &lt;- seq(-3, 2, len = n) ypts &lt;- seq(-2, 3, len = n) gr &lt;- expand.grid(x = xpts, y = ypts) feval &lt;- with(gr, f(x, y)) z &lt;- matrix(feval, nrow = n, ncol = n) par(mar = c(5, 4, 1, 1)) contour(xpts, ypts, z, nlevels = 20) x0 &lt;- -2.5 ## Initial point y0 &lt;- 1.2 points(x0, y0, pch = 19, cex = 2) We will use as a starting point the point \\((-2.5, 1.2)\\), as indicated in the figure above. From the figure, it is clear that ideally we would be able to travel in the direction that would take us directly to the minimum of the function, shown here. par(mar = c(5, 4, 1, 1)) contour(xpts, ypts, z, nlevels = 20) points(x0, y0, pch = 19, cex = 2) arrows(x0, y0, 0, 0, lwd = 3, col = &quot;grey&quot;) If only life were so easy? The idea behind conjugate gradient is the construct that direction using a series of conjugate directions. First we start with the steepest descent direction. Here, we extract the gradient and find the optimal \\(\\alpha\\) value (i.e. the step size). f0 &lt;- f(x0, y0) p0 &lt;- drop(-attr(f0, &quot;gradient&quot;)) ## Get the gradient function f.sub &lt;- function(alpha) { ff &lt;- f(x0 + alpha * p0[1], y0 + alpha * p0[2]) as.numeric(ff) } op &lt;- optimize(f.sub, c(0, 4)) ## Compute the optimal alpha alpha &lt;- op$minimum Now that we’ve computed the gradient and the optimal \\(\\alpha\\), we can take a step in the steepest descent direction x1 &lt;- x0 + alpha * p0[1] y1 &lt;- y0 + alpha * p0[2] and plot our progress to date. par(mar = c(5, 4, 1, 1)) contour(xpts, ypts, z, nlevels = 20) points(x0, y0, pch = 19, cex = 2) arrows(x0, y0, 0, 0, lwd = 3, col = &quot;grey&quot;) arrows(x0, y0, x1, y1, lwd = 2) Now we need to compute the next direction in which to travel. We begin with the steepest descent direction again. The figure below shows what direction that would be (without any modifications for conjugacy). f1 &lt;- f(x1, y1) f1g &lt;- drop(attr(f1, &quot;gradient&quot;)) ## Get the gradient function p1 &lt;- -f1g ## Steepest descent direction op &lt;- optimize(f.sub, c(0, 4)) ## Compute the optimal alpha alpha &lt;- op$minimum x2 &lt;- x1 + alpha * p1[1] ## Find the next point y2 &lt;- y1 + alpha * p1[2] Now we can plot the next direction that is chosen by the usual steepest descent approach. par(mar = c(5, 4, 1, 1)) contour(xpts, ypts, z, nlevels = 20) points(x0, y0, pch = 19, cex = 2) arrows(x0, y0, 0, 0, lwd = 3, col = &quot;grey&quot;) arrows(x0, y0, x1, y1, lwd = 2) arrows(x1, y1, x2, y2, col = &quot;red&quot;, lwd = 2, lty = 2) However, the conjugate gradient approach computes a slightly different direction in which to travel. f1 &lt;- f(x1, y1) f1g &lt;- drop(attr(f1, &quot;gradient&quot;)) beta &lt;- drop(crossprod(f1g) / crossprod(p0)) ## Fletcher-Reeves p1 &lt;- -f1g + beta * p0 ## Conjugate gradient direction f.sub &lt;- function(alpha) { ff &lt;- f(x1 + alpha * p1[1], y1 + alpha * p1[2]) as.numeric(ff) } op &lt;- optimize(f.sub, c(0, 4)) ## Compute the optimal alpha alpha &lt;- op$minimum x2c &lt;- x1 + alpha * p1[1] ## Find the next point y2c &lt;- y1 + alpha * p1[2] Finally, we can plot the direction in which the conjugate gradient method takes. par(mar = c(5, 4, 1, 1)) contour(xpts, ypts, z, nlevels = 20) points(x0, y0, pch = 19, cex = 2) arrows(x0, y0, 0, 0, lwd = 3, col = &quot;grey&quot;) arrows(x0, y0, x1, y1, lwd = 2) arrows(x1, y1, x2, y2, col = &quot;red&quot;, lwd = 2, lty = 2) arrows(x1, y1, x2c, y2c, lwd = 2) In this case, because the target function was exactly quadratic, the process converged on the minimum in exactly 2 steps. We can see that the steepest descent algorithm would have taken many more steps to wind its way towards the minimum. "],
["coordinate-descent.html", "3.5 Coordinate Descent", " 3.5 Coordinate Descent The idea behind coordinate descent methods is simple. If \\(f\\) is a \\(k\\)-dimensional function, we can minimize \\(f\\) by successively minimizing each of the individual dimensions of \\(f\\) in a cyclic fashion, while holding the values of \\(f\\) in the other dimensions fixed. This approach is sometimes referred to as cyclic coordinate descent. The primary advantage of this approach is that it takes an arbitrarily complex \\(k\\)-dimensional problem and reduces it to a collection of \\(k\\) one-dimensional problems. The disadvantage is that convergence can often be painfully slow, particularly in problems where \\(f\\) is not well-behaved. In statistics, a popular version of this algorithm is known as backfitting and is used to fit generalized additive models. If we take a simple quadratic function we can take a detailed look at how coordinate descent works. Let’s use the function \\[ f(x, y) = x^2 + y^2 + xy. \\] We can make a contour plot of this function near the minimum. f &lt;- function(x, y) { x^2 + y^2 + x * y } n &lt;- 30 xpts &lt;- seq(-1.5, 1.5, len = n) ypts &lt;- seq(-1.5, 1.5, len = n) gr &lt;- expand.grid(x = xpts, y = ypts) feval &lt;- with(gr, matrix(f(x, y), nrow = n, ncol = n)) par(mar = c(5, 4, 1, 1)) contour(xpts, ypts, feval, nlevels = 20, xlab = &quot;x&quot;, ylab = &quot;y&quot;) points(-1, -1, pch = 19, cex = 2) abline(h = -1) Let’s take as our initial point \\((-1, -1)\\) and begin our minimization along the \\(x\\) dimension. We can draw a transect at the \\(y = -1\\) level (thus holding \\(y\\) constant) and attempt to find the minimum along that transect. Because \\(f\\) is a quadratic function, the one-dimensional function induced by holding \\(y = -1\\) is also a quadratic. feval &lt;- f(xpts, y = -1) plot(xpts, feval, type = &quot;l&quot;, xlab = &quot;x&quot;, ylab = &quot;f(x | y = -1)&quot;) We can minimize this one-dimensional function with the optimize() function (or we could do it by hand if we’re not lazy). fx &lt;- function(x) { f(x, y = -1) } op &lt;- optimize(fx, c(-1.5, 1.5)) op $minimum [1] 0.5 $objective [1] 0.75 Granted, we could have done this analytically because we are looking at a simple quadratic function. But in general, you will need a one-dimensional optimizer (like the optimize() function in R) to complete each of the coordinate descent iterations. This completes one iteration of the coordinate descent algorithm and our new starting point is \\((0.5, -1)\\). Let’s store this new \\(x\\) value and move on to the next iteration, which will minimize along the \\(y\\) direction. x1 &lt;- op$minimum feval &lt;- with(gr, matrix(f(x, y), nrow = n, ncol = n)) par(mar = c(5, 4, 1, 1)) contour(xpts, ypts, feval, nlevels = 20, xlab = &quot;x&quot;, ylab = &quot;y&quot;) points(-1, -1, pch = 1, cex = 2) ## Initial point abline(h = -1, lty = 2) points(x1, -1, pch = 19, cex = 2) ## After one step abline(v = x1) The transect drawn by holding \\(x = 0.5\\) is shown in the Figure above. The one-dimensional function corresponding to that transect is shown below (again, a one-dimensional quadratic function). feval &lt;- f(x = x1, ypts) plot(xpts, feval, type = &quot;l&quot;, xlab = &quot;x&quot;, ylab = sprintf(&quot;f(x = %.1f | y)&quot;, x1)) Minimizing this one-dimensional function, we get the following. fy &lt;- function(y) { f(x = x1, y) } op &lt;- optimize(fy, c(-1.5, 1.5)) op $minimum [1] -0.25 $objective [1] 0.1875 This completes another iteration of the coordinate descent algorithm and we can plot our progress below. y1 &lt;- op$minimum feval &lt;- with(gr, matrix(f(x, y), nrow = n, ncol = n)) par(mar = c(5, 4, 1, 1)) contour(xpts, ypts, feval, nlevels = 20, xlab = &quot;x&quot;, ylab = &quot;y&quot;) points(-1, -1, pch = 1, cex = 2) ## Initial point abline(h = -1, lty = 2) points(x1, -1, pch = 1, cex = 2) ## After one step abline(v = x1, lty = 2) points(x1, y1, pch = 19, cex = 2) ## After two steps abline(h = y1) ## New transect We can see that after two iterations we are quite a bit closer to the minimum. But we still have a ways to go, given that we can only move along the coordinate axis directions. For a truly quadratic function, this is not an efficient way to find the minimum, particularly when Newton’s method will find the minimum in a single step! Of course, Newton’s method can achieve that kind of performance because it uses two derivatives worth of information. The coordinate descent approach uses no derivative information. There’s no free lunch! In the above example, the coordinates \\(x\\) and \\(y\\) were moderately correlated but not dramatically so. In general, coordinate descent algorithms show very poor performance when the coordinates are strongly correlated. The specifics of the coordinate descent algorithm will vary greatly depending on the general function being minimized, but the essential algorithm is as follows. Given a function \\(f:\\mathbb{R}^p\\rightarrow\\mathbb{R}\\), For \\(j = 1,\\dots, p\\), minimize \\(f_j(x) = f(\\dots,x_{j-1},x,x_{j+1},\\dots)\\) wbere \\(x_1,\\dots,x_{j-1},x_{j+1},\\dots,x_p\\) are all held fixed at their current values. For this use any simple one-dimensional optimizer. Check for convergence. If not converged, go back to 1. 3.5.1 Convergence Rates To take a look at the convergence rate for coordinate descent, we will use as an example, a slightly more general version of the quadratic function above, \\[ f(x, y) = x^2 + y^2 + axy, \\] where here, \\(a\\), represents the amount of correlation or coupling between the \\(x\\) and \\(y\\) coordinates. If \\(a=0\\) there is no coupling and \\(x\\) and \\(y\\) vary independently. At each iteration of the coordinate descent algorithm, we minimize a one-dimensional version of this function. If we fix \\(y = c\\), then we want to minimize \\[ f_{y=c}(x) = x^2 + c^2 + acx. \\] Taking the derivative of this with respect to \\(x\\) and setting it equal to zero gives us the minimum at \\[ x_{min} = \\frac{-ac}{2} \\] Similarly, if we fix \\(x = c\\), then we can minimize an analogous function \\(f_{x=c}(y)\\) to get a minimum point of \\(y_{min} = \\frac{-ac}{2}\\). Looking at the coordinate descent algorithm, we can develop the recurrence relationship \\[ \\left( \\begin{array}{c} x_{n+1}\\\\ y_{n+1} \\end{array} \\right) = \\left( \\begin{array}{c} -\\frac{a}{2}y_n\\\\ -\\frac{a}{2}x_{n+1} \\end{array} \\right) \\] Rewinding this back to the inital point, we can then write that \\[ |x_{n} - x_0| = |x_n-0| = \\left(\\frac{a}{2}\\right)^{2n-1} y_0. \\] where \\(x_0\\) is the minimum point along the \\(x\\) direction. We can similarly say that \\[ |y_n-y_0| = \\left(\\frac{a}{2}\\right)^{2n} x_0. \\] Looking at the rates of convergence separately for each dimension, we can then show that in the \\(x\\) direction, \\[ \\frac{|x_{n+1}-x_0|}{|x_n-x_0|} = \\frac{ \\left(\\frac{a}{2}\\right)^{2(n+1)-1}y_0 }{ \\left(\\frac{a}{2}\\right)^{2n-1}y_0 } = \\left(\\frac{a}{2}\\right)^2. \\] In order to achieve linear convergence for this algorithm, we must have \\(\\left(\\frac{a}{2}\\right)^2\\in (0, 1)\\), which can be true for some values of \\(a\\). But for values of \\(a\\geq 2\\) we would not even be able to obtain linear convergence. In summary, coordinate descent algorithms are conceptually (and computationally) easy to implement but depending on the nature of the target function, convergence may not be possible, even under seemingly reasonable scenarios like the simple one above. Given that we typically do not have very good information about the nature of the target function, particularly in high-dimensional problems, coordinate descent algorithms should be used with care. 3.5.2 Generalized Additive Models Before we begin this section, I want to point out that Brian Caffo has a nice video introduction to generalized additive models on his YouTube channel. Generalized additive models represent an interesting class of models that provide nonparametric flexibility to estimate a high-dimensional function without succumbing to the notorious “curse of dimensionality”. In the traditional linear model, we model the outcome \\(y\\) as \\[ y = \\alpha + \\beta_1x_1 + \\beta_2x_2 \\cdots + \\beta_px_p + \\varepsilon. \\] Generalized additive models replace this formulation with a slightly more general one, \\[ y = \\alpha + s_1(x_1\\mid \\lambda_1) + s_2(x_2\\mid \\lambda_2) + \\cdots + s_p(x_p\\mid \\lambda_p) + \\varepsilon \\] where \\(s_1,\\dots,s_p\\) are smooth functions whose smoothness is controled by the parameters \\(\\lambda_1,\\dots,\\lambda_p\\). The key compromise of this model is that rather than estimate an arbitrary smooth \\(p\\)-dimensional function, we estimate a series of \\(p\\) one-dimensional functions. This is a much simpler problem but still allows us to capture a variety of nonlinear relationships. The question now is how do we estimate these smooth functions? Hastie and Tibshirani proposed a backfitting algorithm whereby each \\(s_j()\\) would be estimated one-at-a-time while holding all of the other functions constant. This is essentially a coordinate descent algorithm where the coordinates are one-dimensional functions in a function space. The \\(s_j()\\) functions can be estimated using any kind of smoother. Hastie and Tibshirani used running median smoothers for robustness in many of their examples, but one could use splines, kernel smoothers, or many others. The backfitting algorithm for additive models works as follows. Given a model of the form \\[ y_i = \\alpha + \\sum_{j=1}^p s_j(x_{ij}\\mid \\lambda_j) + \\varepsilon_i \\] where \\(i=1,\\dots,n\\), 1. Initialize \\(\\alpha = \\frac{1}{n}\\sum_{i=1}^n y_i\\), \\(s_1 = s_2 = \\cdots = s_p = 0\\). Given current values \\(s_1^{(n)},\\dots,s_p^{(n)}\\), for \\(j = 1,\\dots,p\\), Let \\[ r_{ij} = y_i - \\alpha - \\sum_{\\ell\\ne j}s_\\ell(x_{i\\ell}\\mid \\lambda_\\ell) \\] so that \\(r_{ij}\\) is the partial residual for predictor \\(j\\) and observation \\(i\\). Given this set of partial residuals \\(r_{1j},\\dots,r_{nj}\\), we can estimate \\(s_j\\) by smoothing the relationship between \\(r_{ij}\\) and \\(x_{ij}\\) using any smoother we choose. Essentially, we need to solve the mini-problem \\[ r_{ij} = s_j(x_{ij}\\mid\\lambda_j) + \\varepsilon_i \\] using standard nonparametric smoothers. As part of this process, we may need to estimate \\(\\lambda_j\\) using a procedure like generalized cross-validation or something similar. At the end of this step we have \\(s_1^{(n+1)},\\dots,s_p^{(n+1)}\\) We can evaluate \\[ \\Delta = \\sum_{j=1}^p\\left\\|s_j^{(n+1)} - s_j^{(n)}\\right\\| \\] or \\[ \\Delta = \\frac{\\sum_{j=1}^p\\left\\|s_j^{(n+1)} - s_j^{(n)}\\right\\|}{\\sum_{j=1}^p \\left\\|s_j^{(n)}\\right\\|}. \\] where \\(\\|\\cdot\\|\\) is some reasonable metric. If \\(\\Delta\\) is less than some pre-specificed tolerance, we can stop the algorithm. Otherwise, we can go back to Step 2 and do another round of backfitting. "],
["the-em-algorithm.html", "4 The EM Algorithm", " 4 The EM Algorithm The EM algorithm is one of the most popular algorithms in all of statistics. A quick look at Google Scholar shows that the paper by Art Dempster, Nan Laird, and Don Rubin has been cited more than 50,000 times. The EM stands for “Expectation-Maximization”, which indicates the two-step nature of the algorithm. At a high level, there are two steps: The “E-Step” and the “M-step” (duh!). The EM algorithm is not so much an algorithm as a methodology for creating a family of algorithms. We will get into how exactly it works a bit later, but suffice it to say that when someone says “We used the EM algorithm,” that probably isn’t enough information to understand exactly what they did. The devil is in the details and most problems will need a bit of hand crafting. That said, there are a number of canonical problems now where an EM-type algorithm is the standard approach. The basic idea underlying the EM algorithm is as follows. We observe some data that we represent with \\(Y\\). However, there are some missing data, that we represent with \\(Z\\), that make life difficult for us. Together, the observed data \\(Y\\) and the missing data \\(Z\\) make up the complete data \\(X = (Y, Z)\\). We imagine the complete data have a density \\(g(y, z\\mid\\theta)\\) that is parametrized by the vector of parameters \\(\\theta\\). Because of the missing data, we cannot evaluate \\(g\\). The observed data have the density \\[ f(y\\mid\\theta) = \\int g(y, z\\mid\\theta)\\,dz \\] and the observed data log-likelihood is \\(\\ell(\\theta\\mid y) = \\log f(y\\mid\\theta)\\). The problem now is that \\(\\ell(\\theta\\mid y)\\) is difficult to evaluate or maximize because of the integral (for discrete problems this will be a sum). However, in order to estimate \\(\\theta\\) via maximum likelihood using only the observed data, we need to be able to maximize \\(\\ell(\\theta\\mid y)\\). The complete data density usually has some nice form (like being an exponential family member) so that if we had the missing data \\(Z\\), we could easily evaluate \\(g(y,z\\mid\\theta)\\). Given this setup, the basic outline of the EM algorithm works as follows: E-step: Let \\(\\theta_0\\) be the current estimate of \\(\\theta\\). Define \\[ Q(\\theta\\mid\\theta_0) = \\mathbb{E}\\left[\\log g(y,z\\mid\\theta)\\mid y, \\theta_0\\right] \\] M-step: Maximize \\(Q(\\theta\\mid\\theta_0)\\) with respect to \\(\\theta\\) to get the next value of \\(\\theta\\). Goto 1 unless converged. In the E-step, the expectation is taken with respect to the missing data density, which is \\[ h(z\\mid y,\\theta) = \\frac{g(y,z\\mid\\theta)}{f(y\\mid\\theta)}. \\] Because we do not know \\(\\theta\\), we can plug in \\(\\theta_0\\) to evaluate the missing data density. In particular, one can see that it’s helpful if the \\(\\log g(y, z \\mid\\theta)\\) is linear in the missing data so that taking the expectation is a simple operation. "],
["em-algorithm-for-exponential-families.html", "4.1 EM Algorithm for Exponential Families", " 4.1 EM Algorithm for Exponential Families Data that are generated from a regular exponential family distribution have a density that takes the form \\[ g(x\\mid\\theta) = h(x) \\exp(\\theta^\\prime t(x))/a(\\theta). \\] where \\(\\theta\\) is the canonical parameter and \\(t(x)\\) is the vector of sufficient statistics. When thinking about the EM algorithm, the idea scenario is that the complete data density can be written as an exponential family. In that case, for the E-step, if \\(y\\) represents the observed component of the complete data, we can write \\[\\begin{eqnarray*} Q(\\theta\\mid\\theta_0) &amp; = &amp; \\mathbb{E}[\\log g(x\\mid\\theta)\\mid y, \\theta_0]\\\\ &amp; = &amp; \\log h(x)-\\theta^\\prime \\mathbb{E}[t(x)\\mid y, \\theta_0] - \\log a(\\theta) \\end{eqnarray*}\\] (Note: We can ignore the \\(h(x)\\) term because it does not involve the \\(\\theta\\) parameter.) In order to maximize this function with respect to \\(\\theta\\), we can take the derivative and set it equal to zero, \\[ Q^\\prime(\\theta\\mid\\theta_0) = \\mathbb{E}[t(x)\\mid y,\\theta_0] - \\mathbb{E}_\\theta[t(x)] = 0. \\] Hence, for exponential family distributions, executing the M-step is equivalent to setting \\[ \\mathbb{E}[t(x)\\mid y,\\theta_0] = \\mathbb{E}_\\theta[t(x)] \\] where \\(\\mathbb{E}_\\theta[t(x)]\\) is the unconditional expectation of the complete data and \\(\\mathbb{E}[t(x)\\mid y,\\theta_0]\\) is the conditional expectation of the missing data, given the observed data. "],
["canonical-examples.html", "4.2 Canonical Examples", " 4.2 Canonical Examples In this section, we give some canonical examples of how the EM algorithm can be used to estimate model parameters. These examples are simple enough that they can be solved using more direct methods, but they are nevertheless useful for demonstrating how to set up the two-step EM algorithm in various scenarios. 4.2.1 Two-Part Normal Mixture Model Suppose we have data \\(y_1,\\dots,y_n\\) that are sampled independently from a two-part mixture of Normals model with density \\[ f(y\\mid\\theta) = \\lambda\\varphi(y\\mid\\mu_1,\\sigma_1^2) + (1-\\lambda)\\varphi(y\\mid\\mu_2,\\sigma_2^2). \\] where \\(\\varphi(y\\mid\\mu,\\sigma^2)\\) is the Normal density with mean \\(\\mu\\) and variance \\(\\sigma^2\\). The unknown parameter vector is \\(\\theta = (\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2, \\lambda)\\) and the log-likelihood is \\[ \\log f(y_1,\\dots,y_n\\mid\\theta) = \\log \\sum_{i=1}^n \\lambda\\varphi(y_i\\mid\\mu_1,\\sigma_1) + (1-\\lambda)\\varphi(y_i\\mid\\mu_2,\\sigma_2). \\] This problem is reasonably simple enough that it could be solved using a direct optimization method like Newton’s method, but the EM algorithm provides a nice stable approach to finding the optimum. The art of applying the EM algorithm is coming up with a useful complete data model. In this example, the approach is to hypothesize that each observation comes from one of two populations parameterized by \\((\\mu_1, \\sigma_1^2)\\) and \\((\\mu_2,\\sigma^2_2)\\), respectively. The “missing data” in this case are the labels identifying which observation came from which population. Therefore, we assert that there are missing data \\(z_1,\\dots,z_n\\) such that \\[ z_i\\sim\\text{Bernoulli}(\\lambda). \\] When \\(z_i=1\\), \\(y_i\\) comes from population 1 and when \\(z_i=0\\), \\(y_i\\) comes from population 2. The idea is then that the data are sampled in two stages. First we sample \\(z_i\\) to see which population the data come from and then given \\(z_i\\), we can sample \\(y_i\\) from the appropriate Normal distribution. The joint density of the observed and missing data, i.e. the complete data density, is then \\[ g(y,z\\mid\\theta) = \\varphi(y\\mid\\mu_1,\\sigma_1^2)^{z}\\varphi(y\\mid\\mu_2,\\sigma^2_2)^{1-z}\\lambda^z(1-\\lambda)^{1-z}. \\] It’s easy to show that \\[ \\sum_{z=0}^1 g(y, z\\mid\\theta) = f(y\\mid\\theta) \\] so that when we “integrate” out the missing data, we get the observed data density. The complete data log-likelihood is then \\[ \\log g(y, z\\mid\\theta) = \\sum_{i=1}^n z_i\\log\\varphi(y_i\\mid\\mu_1,\\sigma^2_1) + (1-z_i)\\log\\varphi(y_i\\mid\\mu_2,\\sigma^2_2) + z_i\\log\\lambda + (1-z_i)\\log(1-\\lambda). \\] Note that this function is nice and linear in the missing data \\(z_i\\). To evaluate the \\(Q(\\theta\\mid\\theta_0)\\) function we need to take the expectation of the above expression with respect to the missing data density \\(h(z\\mid y, \\theta)\\). But what is that? The missing data density will be proportional to the complete data density, so that \\[\\begin{eqnarray*} h(z\\mid y,\\theta) &amp; \\propto &amp; \\varphi(y\\mid\\mu_1,\\sigma_1^2)^z\\varphi(y\\mid\\mu_2,\\sigma_2^2)^{1-z}\\lambda^z(1-\\lambda)^{1-z}\\\\ &amp; = &amp; (\\lambda \\varphi(y\\mid\\mu_1,\\sigma_1^2))^z((1-\\lambda)\\varphi(y\\mid\\mu_2,\\sigma_2^2))^{1-z}\\\\ &amp; = &amp; \\text{Bernoulli}\\left( \\frac{\\lambda \\varphi(y\\mid\\mu_1,\\sigma_1^2)}{\\lambda \\varphi(y\\mid\\mu_1,\\sigma_1^2) + (1-\\lambda)\\varphi(y\\mid\\mu_2,\\sigma_2^2)} \\right) \\end{eqnarray*}\\] From this, what we need to compute the \\(Q()\\) function is \\(\\pi_i = \\mathbb{E}[z_i\\mid y_i, \\theta_0]\\). Given that, wen then compute the \\(Q()\\) function in the E-step. \\[\\begin{eqnarray*} Q(\\theta\\mid\\theta_0) &amp; = &amp; \\mathbb{E}\\left[ \\sum_{i=1}^n z_i\\log\\varphi(y\\mid\\mu_1,\\sigma_1^2) + (1-z_i)\\log\\varphi(y\\mid\\mu_2,\\sigma_2^2) + z_i\\log\\lambda + (1-z_i)\\log(1-\\lambda) \\right]\\\\ &amp; = &amp; \\sum_{i=1}^n \\pi_i\\log\\varphi(y\\mid\\mu_1,\\sigma_1^2) + (1-\\pi_i)\\varphi(y\\mid\\mu_2,\\sigma_2^2) + \\pi_i\\log\\lambda + (1-\\pi_i)\\log(1-\\lambda)\\\\ &amp; = &amp; \\sum_{i=1}^n \\pi_i\\left[ -\\frac{1}{2}\\log 2\\pi\\sigma_1^2-\\frac{1}{2\\sigma_1^2}(y_i-\\mu_1)^2 \\right] + (1-\\pi_i)\\left[ -\\frac{1}{2}\\log 2\\pi\\sigma_2^2-\\frac{1}{2\\sigma_2^2}(y_i-\\mu_2)^2 \\right]\\\\ &amp; &amp; + \\pi_i\\log\\lambda + (1-\\pi_i)\\log(1-\\lambda) \\end{eqnarray*}\\] In order to compute \\(\\pi_i\\), we will need to use the current estimates of \\(\\mu_1, \\sigma_1^2, \\mu_2\\), and \\(\\sigma_2^2\\) (in addition to the data \\(y_1,\\dots, y_n\\)). We can then compute the gradient of \\(Q\\) in order maximize it for the current iteration. After doing that we get the next values, which are \\[\\begin{eqnarray*} \\hat{\\mu}_1 &amp; = &amp; \\frac{\\sum \\pi_i y_i}{\\sum \\pi_i}\\\\ \\hat{\\mu}_2 &amp; = &amp; \\frac{\\sum (1-\\pi_i) y_i}{\\sum 1-\\pi_i}\\\\ \\hat{\\sigma}_1^2 &amp; = &amp; \\frac{\\sum\\pi_i(y_i-\\mu_1)^2}{\\sum\\pi_i}\\\\ \\hat{\\sigma}_2^2 &amp; = &amp; \\frac{\\sum(1-\\pi_i)(y_i-\\mu_2)^2}{\\sum(1-\\pi_i)}\\\\ \\hat{\\lambda} &amp; = &amp; \\frac{1}{n}\\sum\\pi_i \\end{eqnarray*}\\] Once we have these updated estimates, we can go back to the E-step and recompute our \\(Q\\) function. 4.2.2 Censored Exponential Data Suppose we have survival times \\(x_1,\\dots,x_n\\sim\\text{Exponential}(\\lambda)\\). However, we do not observe these survival times because some of them are censored at times \\(c_1,\\dots,c_n\\). Because the censoring times are known, what we actually observe are the data \\((\\min(y_1, c_1), \\delta_1),\\dots,(\\min(y_n,c_n),\\delta_n)\\), where \\(\\delta=1\\) if \\(y_i\\leq c_i\\) and \\(\\delta=0\\) if \\(y_i\\) is censored at time \\(c_i\\). The complete data density is simply the exponential distribution with rate parameter \\(\\lambda\\), \\[ g(x_1,\\dots,x_n\\mid\\lambda) = \\prod_{i=1}^n\\frac{1}{\\lambda}\\exp(-x_i/\\lambda). \\] To do the E-step, we need to compute \\[ Q(\\lambda\\mid\\lambda_0) = \\mathbb{E}[\\log g(x_1,\\dots,x_n\\mid\\lambda)\\mid \\mathbf{y}, \\lambda_0]\\\\ \\] We can divide the data into the observations that we fully observe (\\(\\delta_i=1\\)) and those that are censored (\\(\\delta_i=0\\)). For the censored data, their complete survival time is “missing”, so can denote the complete survival time as \\(z_i\\). Given that, the \\(Q(\\lambda\\mid\\lambda_0)\\) function is \\[ Q(\\lambda\\mid\\lambda_0) = \\mathbb{E}\\left\\{\\left.-n\\log\\lambda-\\frac{1}{\\lambda}\\left[ \\sum_{i=1}^n \\delta_i y_i + (1-\\delta_i) z_i. \\right] \\right|\\mathbf{y},\\lambda_0 \\right\\} \\] But what is \\(\\mathbb{E}[z_i\\mid y_i,\\lambda_0]\\)? Because we assume the underlying data are exponentially distributed, we can use the “memoryless” property of the exponential distribution. That is, given that we have survived until the censoring time \\(c_i\\), our expected survival time beyond that is simply \\(\\lambda\\). Because we don’t know \\(\\lambda\\) yet we can plug in our current best estimate. Now, for the E-step we have \\[ Q(\\lambda\\mid\\lambda_0) = -n\\log\\lambda-\\frac{1}{\\lambda} \\left[ \\sum_{i=1}^n\\delta_i y_i+(1-\\delta_i)(c_i + \\lambda_0) \\right] \\] With the \\(Q\\) function removed of missing data, we can execute the M-step and maximize the above function to get \\[ \\hat{\\lambda} = \\frac{1}{n}\\left[ \\sum_{i=1}^n\\delta_iy_i +(1-\\delta_i)(c_i+\\lambda_0) \\right] \\] We can then update \\(\\lambda_0=\\hat{\\lambda}\\) and go back and repeat the E-step. "],
["a-minorizing-function.html", "4.3 A Minorizing Function", " 4.3 A Minorizing Function One of the positive qualities of the EM algorithm is that it is very stable. Unlike Newton’s algorithm, where each iteration may or may not be closer to the optimal value, each iteratation of the EM algorithm is designed to increase the observed log-likelihood. This is the ascent property of the EM algorithm, which we will show later. This stability, though, comes at a price—the EM algorithm’s convergence rate is linear (while Newton’s algorithm is quadratic). This can make running the EM algorithm painful at times, particularly when one has to compute standard errors via a resampling approach like the bootstrap. The EM algorithm is a minorization approach. Instead of directly maximizing the log-likelihood, which is difficult to evaluate, the algorithm constructs a minorizing function and optimizes that function instead. What is a minorizing function? Following Chapter 7 of Jan de Leeuw’s Block Relaxation Algorithms in Statistics a function \\(g\\) minorizes \\(f\\) over \\(\\mathcal{X}\\) at \\(y\\) if \\(g(x) \\leq f(x)\\) for all \\(x\\in\\mathcal{X}\\) \\(g(y) = f(y)\\) In the description of the EM algorithm above, \\(Q(\\theta\\mid\\theta_0)\\) is the minorizing function. The benefits of this approach are The \\(Q(\\theta\\mid\\theta_0)\\) is a much nicer function that is easy to optimize Because the \\(Q(\\theta\\mid\\theta_0)\\) minorizes \\(\\ell(\\theta\\mid y)\\), maximizing it is guaranteed to increase (or at least not decrease) \\(\\ell(\\theta\\mid y)\\). This is because if \\(\\theta_n\\) is our current estimate of \\(\\theta\\) and \\(Q(\\theta\\mid\\theta_n)\\) minorizes \\(\\ell(\\theta\\mid y)\\) at \\(\\theta_n\\), then we have \\[ \\ell(\\theta_{n+1}\\mid y) \\geq Q(\\theta_{n+1}\\mid\\theta_n) \\geq Q(\\theta_n\\mid\\theta_n) = \\ell(\\theta_n\\mid y). \\] Let’s take a look at how this minorization process works. We can begin with the observe log-likelihood \\[ \\log f(y\\mid\\theta) = \\log\\int g(y,z\\mid\\theta)\\,dz. \\] Using the time-honored strategy of adding and subtracting, we can show that if \\(\\theta_0\\) is our current estimate of \\(\\theta\\), \\[\\begin{eqnarray*} \\log f(y\\mid\\theta)-\\log f(y\\mid\\theta_0) &amp; = &amp; \\log\\int g(y,z\\mid\\theta)\\,dz - \\log\\int g(y,z\\mid\\theta_0)\\,dz\\\\ &amp; = &amp; \\log\\frac{\\int g(y,z\\mid\\theta)\\,dz}{\\int g(y,z\\mid\\theta_0)\\,dz}\\\\ &amp; = &amp; \\log\\frac{\\int g(y,z\\mid\\theta_0)\\frac{g(y,z\\mid\\theta)}{g(y,z\\mid\\theta_0)}\\,dz}{\\int g(y,z\\mid\\theta_0)\\,dz} \\end{eqnarray*}\\] Now, because we have defined \\[ h(z\\mid y,\\theta) = \\frac{g(y,z\\mid\\theta)}{f(y\\mid\\theta)} = \\frac{g(y,z\\mid\\theta)}{\\int g(y,z\\mid\\theta)\\,dz} \\] we can write \\[\\begin{eqnarray*} \\log f(y\\mid\\theta)-\\log f(y\\mid\\theta_0) &amp; = &amp; \\log\\int h(z\\mid y, \\theta_0)\\frac{g(y,z\\mid\\theta)}{g(y,z\\mid\\theta_0)}\\,dz\\\\ &amp; = &amp; \\log \\mathbb{E}\\left[\\left.\\frac{g(y,z\\mid\\theta)}{g(y,z\\mid\\theta_0)}\\right| y, \\theta_0\\right] \\end{eqnarray*}\\] Because the \\(\\log\\) function is concave, Jensen’s inequality tells us that \\[ \\log \\mathbb{E}\\left[\\left.\\frac{g(y,z\\mid\\theta)}{g(y,z\\mid\\theta_0)}\\right| y, \\theta_0\\right] \\geq \\mathbb{E}\\left[\\log\\left.\\frac{g(y,z\\mid\\theta)}{g(y,z\\mid\\theta_0)}\\right| y, \\theta_0\\right]. \\] Taking this, we can then write \\[ \\log f(y\\mid\\theta)-\\log f(y\\mid\\theta_0) \\geq \\mathbb{E}\\left[\\log\\left.\\frac{g(y,z\\mid\\theta)}{g(y,z\\mid\\theta_0)}\\right| y, \\theta_0\\right], \\] which then gives us \\[\\begin{eqnarray*} \\log f(y\\mid\\theta) &amp; \\geq &amp; \\log f(y\\mid\\theta_0) + \\mathbb{E}[\\log g(y,z\\mid\\theta)\\mid y, \\theta_0] - \\mathbb{E}[\\log g(y,z\\mid\\theta_0)\\mid y, \\theta_0]\\\\ &amp; = &amp; \\log f(y\\mid\\theta_0) + Q(\\theta\\mid\\theta_0) - Q(\\theta_0\\mid\\theta_0) \\end{eqnarray*}\\] The right-hand side of the above equation, the middle part of which is a function of \\(\\theta\\), is our minorizing function. We can see that for \\(\\theta=\\theta_0\\) we have that the minorizing function is equal to \\(\\log f(y\\mid\\theta_0)\\). 4.3.1 Example: Minorization in a Two-Part Mixture Model We will revisit the two-part Normal mixture model from before. Suppose we have data \\(y_1,\\dots,y_n\\) that are sampled independently from a two-part mixture of Normals model with density \\[ f(y\\mid\\lambda) = \\lambda\\varphi(y\\mid\\mu_1,\\sigma_1^2) + (1-\\lambda)\\varphi(y\\mid\\mu_2,\\sigma_2^2). \\] We can simulate some data from this model. mu1 &lt;- 1 s1 &lt;- 2 mu2 &lt;- 4 s2 &lt;- 1 lambda0 &lt;- 0.4 n &lt;- 100 set.seed(2017-09-12) z &lt;- rbinom(n, 1, lambda0) ## &quot;Missing&quot; data x &lt;- rnorm(n, mu1 * z + mu2 * (1-z), s1 * z + (1-z) * s2) hist(x) rug(x) For the purposes of this example, let’s assume that \\(\\mu_1,\\mu_2,\\sigma_1^2\\), and \\(\\sigma_2^2\\) are known. The only unknown parameter is \\(\\lambda\\), the mixing proportion. The observed data log-likelihood is \\[ \\log f(y_1,\\dots,y_n\\mid\\lambda) = \\log \\sum_{i=1}^n \\lambda\\varphi(y_i\\mid\\mu_1,\\sigma^2_1) + (1-\\lambda)\\varphi(y_i\\mid\\mu_2,\\sigma^2_2). \\] We can plot the observed data log-likelihood in this case with the simulated data above. First, we can write a function encoding the mixture density as a function of the data and \\(\\lambda\\). f &lt;- function(x, lambda) { lambda * dnorm(x, mu1, s1) + (1-lambda) * dnorm(x, mu2, s2) } Then we can write the log-likelihood as a function of \\(\\lambda\\) and plot it. loglike &lt;- function(lambda) { sum(log(f(x, lambda))) } loglike &lt;- Vectorize(loglike, &quot;lambda&quot;) ## Vectorize for plotting par(mar = c(5,4, 1, 1)) curve(loglike, 0.01, 0.95, n = 200, ylab = &quot;Log-likelihood&quot;, xlab = expression(lambda)) Note that the true value is \\(\\lambda = 0.4\\). We can compute the maximum likelihood estimate in this simple case with op &lt;- optimize(loglike, c(0.1, 0.9), maximum = TRUE) op$maximum [1] 0.3097435 In this case it would appear that the maximum likelihood estimate exhibits some bias, but we won’t worry about that right now. We can illustrate how the minorizing function works by starting with an initial value of \\(\\lambda_0 = 0.8\\). lam0 &lt;- 0.8 minor &lt;- function(lambda) { p1 &lt;- sum(log(f(x, lam0))) pi &lt;- lam0 * dnorm(x, mu1, s1) / (lam0 * dnorm(x, mu1, s1) + (1 - lam0) * dnorm(x, mu2, s2)) p2 &lt;- sum(pi * dnorm(x, mu1, s1, log = TRUE) + (1-pi) * dnorm(x, mu2, s2, log = TRUE) + pi * log(lambda) + (1-pi) * log(1-lambda)) p3 &lt;- sum(pi * dnorm(x, mu1, s1, log = TRUE) + (1-pi) * dnorm(x, mu2, s2, log = TRUE) + pi * log(lam0) + (1-pi) * log(1-lam0)) p1 + p2 - p3 } minor &lt;- Vectorize(minor, &quot;lambda&quot;) Now we can plot the minorizing function along with the observed log-likelihood. par(mar = c(5,4, 1, 1)) curve(loglike, 0.01, 0.95, ylab = &quot;Log-likelihood&quot;, xlab = expression(lambda)) curve(minor, 0.01, 0.95, add = TRUE, col = &quot;red&quot;) legend(&quot;topright&quot;, c(&quot;obs. log-likelihood&quot;, &quot;minorizing function&quot;), col = 1:2, lty = 1, bty = &quot;n&quot;) Maximizing the minorizing function gives us the next estimate of \\(\\lambda\\) in the EM algorithm. It’s clear from the picture that maximizing the minorizing function will increase the observed log-likelihood. par(mar = c(5,4, 2, 1)) curve(loglike, 0.01, 0.95, ylab = &quot;Log-likelihood&quot;, xlab = expression(lambda), xlim = c(-0.5, 1), ylim = c()) abline(v = lam0, lty = 2) mtext(expression(lambda[0]), at = lam0, side = 3) curve(minor, 0.01, 0.95, add = TRUE, col = &quot;red&quot;, lwd = 2) op &lt;- optimize(minor, c(0.1, 0.9), maximum = TRUE) abline(v = op$maximum, lty = 2) lam0 &lt;- op$maximum curve(minor, 0.01, 0.95, add = TRUE, col = &quot;blue&quot;, lwd = 2) abline(v = lam0, lty = 2) mtext(expression(lambda[1]), at = lam0, side = 3) op &lt;- optimize(minor, c(0.1, 0.9), maximum = TRUE) abline(v = op$maximum, lty = 2) mtext(expression(lambda[2]), at = op$maximum, side = 3) legend(&quot;topleft&quot;, c(&quot;obs. log-likelihood&quot;, &quot;1st minorizing function&quot;, &quot;2nd minorizing function&quot;), col = c(1, 2, 4), lty = 1, bty = &quot;n&quot;) In the figure above, the second minorizing function is constructed using \\(\\lambda_1\\) and maximized to get \\(\\lambda_2\\). This process of constructing the minorizing function and maximizing can be repeated until convergence. This is the EM algorithm at work! 4.3.2 Constrained Minimization With and Adaptive Barrier The flip side of minorization is majorization, which is used in minimization problems. We can implement a constrained minimization procedure by creating a surrogate function that majorizes the target function and satisfies the constraints. Specifically, the goal is to minimize a funtion \\(f(\\theta)\\) subject to a set of constraints of the form \\(g_i(\\theta) \\geq 0\\) where \\[ g_i(\\theta) = u_i^\\prime \\theta - c_i \\] and where \\(u_i\\) is a vector of the same length as \\(\\theta\\), \\(c_i\\) is a constant, and \\(i=1,\\dots,\\ell\\). These constraints are linear constraints on the parameters. Given the constraints and \\(\\theta_n\\), the estimate of \\(\\theta\\) at iteration \\(n\\), we can construct the surrogate function, \\[ R(\\theta\\mid\\theta_n) = f(\\theta) - \\lambda \\sum_{i=1}^\\ell g_i(\\theta_n)\\log g_i(\\theta)-u_i^\\prime\\theta \\] with \\(\\lambda &gt; 0\\). "],
["missing-information-principle.html", "4.4 Missing Information Principle", " 4.4 Missing Information Principle So far, we have described the EM algorithm for computing maximum likelihood estimates in some missing data problems. But the original presentation of the EM algorithm did not discuss how to obtain any measures of uncertainty, such as standard errors. One obvious candidate would be the observed information matrix. However, much like with the observed log-likelihood, the observed information matrix is difficult to compute because of the missing data. Recalling the notation from the previous section, let \\(f(y\\mid\\theta)\\) be the observed data density, \\(g(y,z\\mid\\theta)\\) the complete data density, and \\(h(z\\mid y,\\theta) := g(y,z\\mid\\theta)/f(y\\mid\\theta)\\) the missing data density. From this we can write the following series of identities: \\[\\begin{eqnarray*} f(y\\mid\\theta) &amp; = &amp; \\frac{g(y,z\\mid\\theta)}{h(z\\mid y,\\theta)}\\\\ -\\log f(y\\mid\\theta) &amp; = &amp; -\\log g(y,z\\mid\\theta) - [-\\log h(z\\mid y,\\theta)]\\\\ \\mathbb{E}\\left[-\\frac{\\partial}{\\partial\\theta\\partial\\theta^\\prime} \\log f(y\\mid\\theta) \\right] &amp; = &amp; \\mathbb{E}\\left[-\\frac{\\partial}{\\partial\\theta\\partial\\theta^\\prime} \\log g(y,z\\mid\\theta)\\right] - \\mathbb{E}\\left[ -\\frac{\\partial}{\\partial\\theta\\partial\\theta^\\prime} \\log h(z\\mid y,\\theta) \\right]\\\\ I_Y(\\theta) &amp; = &amp; I_{Y,Z}(\\theta) - I_{Z\\mid Y}(\\theta) \\end{eqnarray*}\\] Here, we refer to \\(I_Y(\\theta)\\) as the observed data information matrix, \\(I_{Y,Z}(\\theta)\\) as the complete data information matrix, and \\(I_{Z\\mid Y}(\\theta)\\) as the missing information matrix. This identity allows for the the nice interpretation as the “observed information” equals the “complete information” minus the “missing information”. If we could easily evaluate the \\(I_Y(\\theta)\\), we could simply plug in the maximum likelihood estimate \\(\\hat{\\theta}\\) and obtain standard errors from \\(I_Y(\\hat{\\theta})\\). However, beause of the missing data, \\(I_Y(\\theta)\\) is difficult ot evaluate. Presumably, \\(I_{Y,Z}(\\theta)\\) is reasonable to compute because it is based on the complete data. What then is \\(I_{Z\\mid Y}(\\theta)\\), the missing information matrix? Let \\(S(y\\mid\\theta) = \\frac{\\partial}{\\partial\\theta}\\log f(y\\mid\\theta)\\) be the observed score function and let \\(S(y,z\\mid\\theta) = \\frac{\\partial}{\\partial\\theta}\\log g(y,z\\mid\\theta)\\) be the complete data score function. In a critically important paper, Tom Louis showed that \\[ I_{Z\\mid Y}(\\theta) = \\mathbb{E}\\left[ S(y, z\\mid\\theta)S(y, z\\mid\\theta)^\\prime \\right] - S(y\\mid\\theta)S(y\\mid\\theta)^\\prime. \\] with the expectation taken with respect to the missing data density \\(h(z\\mid y,\\theta)\\). The first part of the right hand side involves computations on the complete data, which is fine. Unfortunately, the second part involves the observed score function, which is presumably difficult to evaluate. However, by definition, \\(S(y\\mid\\hat{\\theta}) = 0\\) at the maximum likelihood estimate \\(\\hat{\\theta}\\). Therefore, we can write the observed information matrix at the MLE as \\[ I_Y(\\hat{\\theta}) = I_{Y,Z}(\\hat{\\theta}) - \\mathbb{E}\\left[ S(y, z\\mid\\theta)S(y, z\\mid\\theta)^\\prime \\right] \\] so that all computations are done on the complete data. Note also that \\[\\begin{eqnarray*} I_{Y,Z}(\\hat{\\theta}) &amp; = &amp; -\\mathbb{E}\\left[\\left. \\frac{\\partial}{\\partial\\theta\\partial\\theta^\\prime} \\log g(y,z\\mid\\theta)\\right|\\hat{\\theta}, y \\right]\\\\ &amp; = &amp; -Q^{\\prime\\prime}(\\hat{\\theta}\\mid\\hat{\\theta}) \\end{eqnarray*}\\] Meilijson showed that when the observed data \\(\\mathbf{y}=y_1,\\dots,y_n\\) are iid, then \\[ S(\\mathbf{y}\\mid\\theta)=\\sum_{i=1}^n S(y_i\\mid\\theta) \\] and hence \\[\\begin{eqnarray*} I_Y(\\theta) &amp; = &amp; \\text{Var}(S(\\mathbf{y}\\mid\\theta))\\\\ &amp; = &amp; \\frac{1}{n}\\sum_{i=1}^n S(y_i\\mid\\theta)S(y_i\\mid\\theta)^\\prime - \\frac{1}{n^2}S(\\mathbf{y}\\mid\\theta)S(\\mathbf{y}\\mid\\theta)^\\prime. \\end{eqnarray*}\\] Again, because \\(S(\\mathbf{y}\\mid\\hat{\\theta})=0\\) at the MLE, we can ignore the second part of the expression if we are interested in obtaining the observed information at the location of the MLE. As for the first part of the expression, Louis also showed that \\[ S(y_i\\mid\\theta) = \\mathbb{E}[S(y_i, z_i\\mid\\theta)\\mid y_i, \\theta_0]. \\] where the expectation is once again taken with respect to the missing data density. Therefore, we can transfer computations on the observed score function to computations on the complete score function. "],
["acceleration-methods.html", "4.5 Acceleration Methods", " 4.5 Acceleration Methods Dempster et al. showed that the convergence rate for the EM algorithm is linear, which can be painfully slow for some problems. Therefore, a cottage industry has developed around the notion of speeding up the convergence of the algorithm. Two approaches that we describe here are one proposed by Tom Louis based on the Aitken acceleration technique and the SQUAREM approach of Varadhan and Roland. 4.5.1 Louis’s Acceleration If we let \\(M(\\theta)\\) be the map representing a single iteration of the EM algorithm, so that \\(\\theta_{n+1} = M(\\theta_n)\\). Then under standard regularity conditions, we can approximate \\(M\\) near the optimum value \\(\\theta^\\star\\) with \\[ \\theta_{n+1} = M(\\theta_{n}) \\approx \\theta_n + J(\\theta_{n-1})(\\theta_n - \\theta_{n-1}) \\] where Dempster et al. 1977 showed that \\(J\\) is \\[ J(\\theta) = I_{Z|Y}(\\theta)I_{Z,Y}(\\theta)^{-1}, \\] which can be interpreted as characterizing the proportion of missing data. (Dempster et al. also showed that the rate of convergence of the EM algorithm is determined by the modulus of the largest eigenvalue of \\(J(\\theta^\\star)\\).) Furthermore, for large \\(j\\) and \\(n\\), we have \\[ \\theta_{n + j + 1} - \\theta_{n+j} \\approx J^{(n)}(\\theta^\\star)(\\theta_{j+1}-\\theta_{j}) \\] where \\(\\theta^\\star\\) is the MLE, and \\(J^{(n)}(\\theta^\\star)\\) is \\(J\\) multiplied by itself \\(n\\) times. Then if \\(\\theta^\\star\\) is the limit of the sequence \\(\\{\\theta_n\\}\\), we can write (trivially) for any \\(j\\) \\[ \\theta^\\star = \\theta_j + \\sum_{k=1}^\\infty (\\theta_{k + j} - \\theta_{k+j-1}) \\] We can then approximate this with \\[\\begin{eqnarray*} \\theta^\\star &amp; \\approx &amp; \\theta_j + \\left( \\sum_{k = 0}^\\infty J^{(k)}(\\theta^\\star) \\right) (\\theta_{j+1}-\\theta_j)\\\\ &amp; = &amp; \\theta_j + (I-J(\\theta^\\star))^{-1} (\\theta_{j+1}-\\theta_j) \\end{eqnarray*}\\] The last equivalence is possible because the eigenvalues of \\(J\\) are all less than one in absolute value. Given this relation, the acceleration method proposed by Louis works as follows. Given \\(\\theta_n\\), the current estimate of \\(\\theta\\), Compute \\(\\theta_{n+1}\\) using the standard EM algorithm Compute \\((I-\\hat{J})^{-1} = I_{Y,Z}(\\theta_n)I_Y(\\theta_n)^{-1}\\) Let \\(\\theta^\\star = \\theta_n+(I-\\hat{J})^{-1}(\\theta_{n+1}-\\theta_n)\\). Set \\(\\theta_{n+1} = \\theta^\\star\\). The cost of using Louis’s technique is minimal if the dimension of \\(\\theta\\) is small. Ultimately, it comes down to the cost of inverting \\(I_Y(\\theta_n)\\) relative to running a single iteration of the EM algorithm. Further, it’s worth emphasizing that the convergence of the approach is only guaranteed for values of \\(\\theta\\) in a neighborhood of the optimum \\(\\theta^star\\), but the size and nature of that neighborhood is typically unknown in applications. Looking at the algorithm described above, we can gather some basic heuristics of how it works. When the information in the observed data is high relative to the complete data, then the value of \\((I-\\hat{J})^{-1}\\) will be close to \\(1\\) and the sequence of iterates generated by the algorithm will be very similar to the usual EM sequence. However, if the proportion of missing data is high, then \\((I-\\hat{J})^{-1}\\) will be much greater than \\(1\\) and the modifications that the algorithm makes to the usual EM sequence will be large. 4.5.1.1 Example: Normal Mixture Model Recall that the data are generated as follows. mu1 &lt;- 1 s1 &lt;- 2 mu2 &lt;- 4 s2 &lt;- 1 lambda0 &lt;- 0.4 n &lt;- 100 set.seed(2017-09-12) z &lt;- rbinom(n, 1, lambda0) y &lt;- rnorm(n, mu1 * z + mu2 * (1-z), s1 * z + (1-z) * s2) hist(y) rug(y) If we assume \\(\\mu_1\\), \\(\\mu_2\\), \\(\\sigma_1\\) and \\(\\sigma_2\\) are known, then we can visualize the observed data log-likelihood as a function of \\(\\lambda\\). f &lt;- function(y, lambda) { lambda * dnorm(y, mu1, s1) + (1-lambda) * dnorm(y, mu2, s2) } loglike &lt;- Vectorize( function(lambda) { sum(log(f(y, lambda))) } ) curve(loglike, 0.01, 0.95, n = 200, xlab = expression(lambda)) Because the observed log-likelihood is relatively simple in this case, we can maximize it directly and obtain the true maximum likelihood estimate. op &lt;- optimize(loglike, c(0.01, 0.95), maximum = TRUE, tol = 1e-8) op$maximum [1] 0.3097386 We can encode the usual EM iteration as follows. The M function represents a single iteration of the EM algorithm as a function of the current value of \\(\\lambda\\). make_pi &lt;- function(lambda, y, mu1, mu2, s1, s2) { lambda * dnorm(y, mu1, s1) / (lambda * dnorm(y, mu1, s1) + (1 - lambda) * (dnorm(y, mu2, s2))) } M &lt;- function(lambda0) { pi.est &lt;- make_pi(lambda0, y, mu1, mu2, s1, s2) mean(pi.est) } We can also encode the accelerated version here with the function Mstar. The functions Iy and Iyz encode the observed and complete data information matrices. Iy &lt;- local({ d &lt;- deriv3(~ log(lambda * dnorm(y, mu1, s1) + (1-lambda) * dnorm(y, mu2, s2)), &quot;lambda&quot;, function.arg = TRUE) function(lambda) { H &lt;- attr(d(lambda), &quot;hessian&quot;) sum(H) } }) Iyz &lt;- local({ d &lt;- deriv3(~ pihat * log(lambda) + (1-pihat) * log(1-lambda), &quot;lambda&quot;, function.arg = TRUE) function(lambda) { H &lt;- attr(d(lambda), &quot;hessian&quot;) sum(H) } }) Mstar &lt;- function(lambda0) { lambda1 &lt;- M(lambda0) pihat &lt;- make_pi(lambda0, y, mu1, mu2, s1, s2) lambda0 + (Iyz(lambda0) / Iy(lambda0)) * (lambda1 - lambda0) } Taking a starting value of \\(\\lambda = 0.1\\), we can see the speed at which the original EM algorithm and the accelerated versions converge toward the MLE. lambda0 &lt;- 0.1 lambda0star &lt;- 0.1 iter &lt;- 6 EM &lt;- numeric(iter) Accel &lt;- numeric(iter) for(i in 1:iter) { pihat &lt;- make_pi(lambda0, y, mu1, mu2, s1, s2) lambda1 &lt;- M(lambda0) lambda1star &lt;- Mstar(lambda0star) EM[i] &lt;- lambda1 Accel[i] &lt;- lambda1star lambda0 &lt;- lambda1 lambda0star &lt;- lambda1star } results &lt;- data.frame(EM = EM, Accel = Accel, errorEM = abs(EM - op$maximum), errorAccel = abs(Accel - op$maximum)) After six iterations, we have the following. format(results, scientific = FALSE) EM Accel errorEM errorAccel 1 0.2354541 0.2703539 0.0742845130 0.039384732683244 2 0.2850198 0.3075326 0.0247188026 0.002206035238572 3 0.3014516 0.3097049 0.0082869721 0.000033703087859 4 0.3069478 0.3097384 0.0027907907 0.000000173380975 5 0.3087971 0.3097386 0.0009414640 0.000000006066448 6 0.3094208 0.3097386 0.0003177924 0.000000005785778 One can see from the errorAccel column that the accelerated method’s error decreases much faster than the standard method’s error (in the errorEM column). The accelerated method appears to be close to the MLE by the third iteration whereas the standard EM algorithm hasn’t quite gotten there by six iterations. 4.5.2 SQUAREM Let \\(M(\\theta)\\) be the map representing a single iteration of the EM algorithm so that \\(\\theta_{n+1} = M(\\theta_n)\\). Given the current value \\(\\theta_0\\), Let \\(\\theta_1 = M(\\theta_0)\\) Let \\(\\theta_2 = M(\\theta_1)\\) Compute the difference \\(r = \\theta_1-\\theta_0\\) Let \\(v = (\\theta_2-\\theta_1) - r\\) Compute the step length \\(\\alpha\\) Modify \\(\\alpha\\) if necessary Let \\(\\theta^\\prime = \\theta_0 - 2\\alpha r + \\alpha^2 v\\) Let \\(\\theta_1 = M(\\theta^\\prime)\\) Compare \\(\\theta_1\\) with \\(\\theta_0\\) and check for convergence. If we have not yet converged, let \\(\\theta_0 = \\theta_1\\) and go back to Step 1. "],
["integration.html", "5 Integration", " 5 Integration In statistical applications we often need to compute quantities of the form \\[ \\mathbb{E}_f g(X) = \\int g(x) f(x)\\,dx \\] where \\(X\\) is a random variable drawn from a distribution with probability mass function \\(f\\). Another quantity that we often need to compute is the normalizing constant for a probability density function. If \\(X\\) has a density that is proportional to \\(p(x\\mid\\theta)\\) then its normalizing constant is \\(\\int p(x\\mid\\theta)\\,dx\\). In both problems—computing the expectation and computing the normalizing constant—an integral must be evaluated. Approaches to solving the integration problem roughly fall into two categories. The first categories involves identifying a sequence of estimates to the integral that eventually converge to the true value as some index (usually involving time or resources) goes to infinity. Adaptive quadrature, independent Monte Carlo and Markov chain Monte Carlo techniques all fall into this category. Given enough time and resources, these techniques should converge to the true value of the integral. The second category of techniques involves identifying a class of alternative functions that are easier to work with, finding the member of that class that best matches the true function, and then working with the alternate function instead to compute the integral. Laplace approximation, variational inference, and approximate Bayes computation (ABC) fall into this category of approaches. For a given dataset, these approaches will not provide the true integral value regardless of time and resources, but as the sample size increases, the approximations will get better. "],
["laplace-approximation.html", "5.1 Laplace Approximation", " 5.1 Laplace Approximation The first technique that we will discuss is Laplace approximation. This technique can be used for reasonably well behaved functions that have most of their mass concentrated in a small area of their domain. Technically, it works for functions that are in the class of \\(\\mathfrak{L}^2\\), meaning that \\[ \\int g(x)^2\\,dx &lt; \\infty \\] Such a function generally has very rapidly decreasing tails so that in the far reaches of the domain we would not expect to see large spikes. Imagine a function that looks as follows We can see that this function has most of its mass concentrated around the point \\(x_0\\) and that we could probably approximate the area under the function with something like a step function. The benefit of using something like a step function is that the area under a step function is trivial to compute. If we could find a principled and automatic way to find that approximating step function, and it were easier than just directly computing the integral in the first place, then we could have an alternative to computing the integral. In other words, we could perhaps say that \\[ \\int g(x)\\,dx \\approx g(x_0)\\varepsilon \\] for some small value of \\(\\varepsilon\\). In reality, we actually have some more sophisticated functions that we can use besides step functions, and that’s how the Laplace approximation works. The general idea is to take a well-behaved uni-modal function and approximate it with a Normal density function, which is a very well-understood quantity. Suppose we have a function \\(g(x)\\in\\mathfrak{L}^2\\) which achieves its maximum at \\(x_0\\). We want to compute \\[ \\int_a^b g(x)\\, dx. \\] Let \\(h(x) = \\log g(x)\\) so that we have \\[ \\int_a^b g(x)\\,dx = \\int_a^b \\exp(h(x))\\,dx \\] From here we can take a Taylor series approximation of \\(h(x)\\) around the point \\(x_0\\) to give us \\[ \\int_a^b \\exp(h(x))\\,dx \\approx \\int_a^b\\exp\\left(h(x_0) + h^\\prime(x_0)(x-x_0) + \\frac{1}{2}h^{\\prime\\prime}(x_0)(x-x_0)^2\\right)\\,dx \\] Because we assumed \\(h(x)\\) achieves its maximum at \\(x_0\\), we know \\(h^\\prime(x_0) = 0\\). Therefore, we can simplify the above expression to be \\[ = \\int_a^b\\exp\\left(h(x_0) + \\frac{1}{2}h^{\\prime\\prime}(x_0)(x-x_0)^2\\right)\\,dx \\] Given that \\(h(x_0)\\) is a constant that doesn’t depend on \\(x\\), we can pull it outside the integral. In addition, we can rearrange some of the terms to give us \\[ = \\exp(h(x_0)) \\int_a^b \\exp\\left(-\\frac{1}{2}\\frac{(x-x_0)^2}{-h^{\\prime\\prime}(x_0)^{-1}}\\right)\\,dx \\] Now that looks more like it, right? Inside the integral we have a quantity that is proportional to a Normal density with mean \\(x_0\\) and variance \\(-h^{\\prime\\prime}(x_0)^{-1}\\). At this point we are just one call to the pnorm() function away from approximating our integral. All we need is to compute our normalizing constants. If we let \\(\\Phi(x\\mid \\mu, \\sigma^2)\\) be the cumulative distribution function for the Normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\) (and \\(\\varphi\\) is its density function), then we can write the above expression as \\[\\begin{eqnarray*} &amp; = &amp; \\exp(h(x_0)) \\sqrt{\\frac{2\\pi}{-h^{\\prime\\prime}(x_0)}} \\int_a^b \\varphi(x\\mid x_0,-h^{\\prime\\prime}(x_0)^{-1})\\,dx\\\\ &amp; = &amp; \\exp(h(x_0)) \\sqrt{\\frac{2\\pi}{-h^{\\prime\\prime}(x_0)}} \\left[ \\Phi\\left(b\\mid x_0,-h^{\\prime\\prime}(x_0)^{-1}\\right) - \\Phi\\left(a\\mid x_0,-h^{\\prime\\prime}(x_0)^{-1}\\right) \\right] \\end{eqnarray*}\\] Recall that \\(\\exp(h(x_0)) = g(x_0)\\). If \\(b=\\infty\\) and \\(a = -\\infty\\), as is commonly the case, then the term in the square brackets is equal to \\(1\\), making the Laplace approximation equal to the value of the function \\(g(x)\\) at its mode multiplied by a constant that depends on the curvature of the function \\(h\\). One final note about the Laplace approximation is that it replaces the problem of integrating a function with the problem of maximizing it. In order to compute the Laplace approximation, we have to compute the location of the mode, which is an optimization problem. Often, this problem is faster to solve using well-understood function optimizers than integrating the same function would be. 5.1.1 Computing the Posterior Mean In Bayesian computations we often want to compute the posterior mean of a parameter given the observed data. If \\(y\\) represents data we observe and \\(y\\) comes from the distribution \\(f(y\\mid\\theta)\\) with parameter \\(\\theta\\) and \\(\\theta\\) has a prior distribution \\(\\pi(\\theta)\\), then we usually want to compute the posterior distribution \\(p(\\theta\\mid y)\\) and its mean, \\[ \\mathbb{E}_p[\\theta] = \\int \\theta\\, p(\\theta\\mid y)\\,d\\theta. \\] We can then write \\[\\begin{eqnarray*} \\int \\theta\\,p(\\theta\\mid y)\\,dx &amp; = &amp; \\frac{ \\int\\theta\\,f(y\\mid\\theta)\\pi(\\theta)\\,d\\theta }{ \\int f(y\\mid\\theta)\\pi(\\theta)\\,d\\theta }\\\\ &amp; = &amp; \\frac{ \\int\\theta\\,\\exp(\\log f(y\\mid\\theta)\\pi(\\theta))\\,d\\theta }{ \\int\\exp(\\log f(y\\mid\\theta)\\pi(\\theta)\\,d\\theta) } \\end{eqnarray*}\\] Here, we’ve used the age old trick of exponentiating and log-ging. If we let \\(h(\\theta) = \\log f(y\\mid\\theta)\\pi(\\theta)\\), then we can use the same Laplace approximation procedure described in the previous section. However, in order to do that we must know where \\(h(\\theta)\\) achieves its maximum. Because \\(h(\\theta)\\) is simply a monotonic transformation of a function proportional to the posterior density, we know that \\(h(\\theta)\\) achieves its maximum at the posterior mode. Let \\(\\hat{\\theta}\\) be the posterior mode of \\(p(\\theta\\mid y)\\). Then we have \\[\\begin{eqnarray*} \\int \\theta\\,p(\\theta\\mid y)\\,dx &amp; \\approx &amp; \\frac{ \\int\\theta \\exp\\left( h(\\hat{\\theta}) + \\frac{1}{2}h^{\\prime\\prime}(\\hat{\\theta})(\\theta-\\hat{\\theta})^2 \\right)\\,d\\theta }{ \\int \\exp\\left( h(\\hat{\\theta}) + \\frac{1}{2}h^{\\prime\\prime}(\\hat{\\theta})(\\theta-\\hat{\\theta})^2 \\right)\\,d\\theta }\\\\ &amp; = &amp; \\frac{ \\int\\theta \\exp\\left( \\frac{1}{2}h^{\\prime\\prime}(\\hat{\\theta})(\\theta-\\hat{\\theta})^2 \\right)\\,d\\theta }{ \\int \\exp\\left( \\frac{1}{2}h^{\\prime\\prime}(\\hat{\\theta})(\\theta-\\hat{\\theta})^2 \\right)\\,d\\theta }\\\\ &amp; = &amp; \\frac{ \\int \\theta \\sqrt{\\frac{2\\pi}{-h^{\\prime\\prime}(\\hat{\\theta})}} \\varphi\\left(\\theta\\mid\\hat{\\theta},-h^{\\prime\\prime}(\\hat{\\theta})^{-1}\\right) \\,d\\theta }{ \\int \\sqrt{\\frac{2\\pi}{-h^{\\prime\\prime}(\\hat{\\theta})}} \\varphi\\left(\\theta\\mid\\hat{\\theta},-h^{\\prime\\prime}(\\hat{\\theta})^{-1}\\right) \\,d\\theta }\\\\ &amp; = &amp; \\hat{\\theta} \\end{eqnarray*}\\] Hence, the Laplace approximation to the posterior mean is equal to the posterior mode. This approximation is likely to work well when the posterior is unimodal and relatively symmetric around the model. Furthermore, the more concentrated the posterior is around \\(\\hat{\\theta}\\), the better. 5.1.1.1 Example: Poisson Data with a Gamma Prior In this simple example, we will use data drawn from a Poisson distribution with a mean that has a Gamma prior distribution. The model is therefore \\[\\begin{eqnarray*} Y \\mid \\mu &amp; \\sim &amp; \\text{Poisson}(\\mu)\\\\ \\mu &amp; \\sim &amp; \\text{Gamma}(a, b) \\end{eqnarray*}\\] where the Gamma density is \\[ f(\\mu) = \\frac{1}{b^{a}\\Gamma(a)}\\mu^{a - 1}e^{-\\mu/b} \\] In this case, given an observation \\(y\\), the posterior distribution is simply a Gamma distribution with shape parameter \\(y + a\\) and scale parameter \\((1 + 1/b)\\). Suppose we observe \\(y = 2\\). We can draw the posterior distribution and prior distribution as follows. make_post &lt;- function(y, shape, scale) { function(x) { dgamma(x, shape = y + shape, scale = 1 / (1 + 1 / scale)) } } set.seed(2017-11-29) y &lt;- 2 prior.shape &lt;- 3 prior.scale &lt;- 3 p &lt;- make_post(y, prior.shape, prior.scale) curve(p, 0, 12, n = 1000, lwd = 3, xlab = expression(mu), ylab = expression(paste(&quot;p(&quot;, mu, &quot; | y)&quot;))) curve(dgamma(x, shape = prior.shape, scale = prior.scale), add = TRUE, lty = 2) legend(&quot;topright&quot;, legend = c(&quot;Posterior&quot;, &quot;Prior&quot;), lty = c(1, 2), lwd = c(3, 1), bty = &quot;n&quot;) Because this is a Gamma distribution, we can also compute the posterior mode in closed form. pmode &lt;- (y + prior.shape - 1) * (1 / (1 + 1 / prior.scale)) pmode [1] 3 We can also compute the mean. pmean &lt;- (y + prior.shape) * (1 / (1 + 1 / prior.scale)) pmean [1] 3.75 From the skewness in the figure above, it’s clear that the mean and the mode should not match. We can now see what the Laplace approximation to the posterior looks like in this case. First, we can compute the gradient and Hessian of the Gamma density. a &lt;- prior.shape b &lt;- prior.scale fhat &lt;- deriv3(~ mu^(y + a - 1) * exp(-mu * (1 + 1/b)) / ((1/(1+1/b))^(y+a) * gamma(y + a)), &quot;mu&quot;, function.arg = TRUE) Then we can compute the quadratic approximation to the density via the lapprox() function below. post.shape &lt;- y + prior.shape - 1 post.scale &lt;- 1 / (length(y) + 1 / prior.scale) lapprox &lt;- Vectorize(function(mu, mu0 = pmode) { deriv &lt;- fhat(mu0) grad &lt;- attr(deriv, &quot;gradient&quot;) hess &lt;- drop(attr(deriv, &quot;hessian&quot;)) f &lt;- function(x) dgamma(x, shape = post.shape, scale = post.scale) hpp &lt;- (hess * f(mu0) - grad^2) / f(mu0)^2 exp(log(f(mu0)) + 0.5 * hpp * (mu - mu0)^2) }, &quot;mu&quot;) Plotting the true posterior and the Laplace approximation gives us the following. curve(p, 0, 12, n = 1000, lwd = 3, xlab = expression(mu), ylab = expression(paste(&quot;p(&quot;, mu, &quot; | y)&quot;))) curve(dgamma(x, shape = prior.shape, scale = prior.scale), add = TRUE, lty = 2) legend(&quot;topright&quot;, legend = c(&quot;Posterior Density&quot;, &quot;Prior Density&quot;, &quot;Laplace Approx&quot;), lty = c(1, 2, 1), lwd = c(3, 1, 1), col = c(1, 1, 2), bty = &quot;n&quot;) curve(lapprox, 0.001, 12, n = 1000, add = TRUE, col = 2, lwd = 2) The solid red curve is the Laplace approximation and we can see that in the neighborhood of the mode, the approximation is reasonable. However, as we move farther away from the mode, the tail of the Gamma is heavier on the right. Of course, this Laplace approximation is done with only a single observation. One would expect the approximation to improve as the sample size increases. In this case, with respect to the posterior mode as an approximation to the posterior mean, we can see that the difference between the two is simply \\[ \\hat{\\theta}_{\\text{mean}} - \\hat{\\theta}_{\\text{mode}} = \\frac{1}{n + 1/b} \\] which clearly goes to zero as \\(n\\rightarrow\\infty\\). "],
["variational-inference.html", "5.2 Variational Inference", " 5.2 Variational Inference I should probably put something here about variational inference. "],
["independent-monte-carlo.html", "6 Independent Monte Carlo", " 6 Independent Monte Carlo Suppose we want to compute for some function \\(h: \\mathbb{R}^k\\rightarrow\\mathbb{R}\\), \\[ \\mathbb{E}_f[h(X)] = \\int h(x)f(x)\\,dx. \\] If we could simulate \\(x_1,\\dots,x_n\\stackrel{\\text{i.i.d.}}{\\sim} f\\), then by the law of large numbers, we would have \\[ \\frac{1}{n}\\sum_{i=1}^n h(x_i) \\longrightarrow \\mathbb{E}_f[h(X)]. \\] Furthermore, we have that \\[ \\text{Var}\\left(\\frac{1}{n}\\sum_{i=1}^n h(x_i)\\right) = \\frac{1}{n^2}\\sum_{i=1}^n \\text{Var}(h(X_i)) = \\frac{1}{n}\\text{Var}(h(X_1)), \\] which, we will note for now, does not depend on the dimension of the random variable \\(X_1\\). This approach to computing the expectation above is known as Monte Carlo integration which takes advantage of the law of large numbers saying that averages of numbers converge to their expectation. Monte Carlo integration can be quite useful, but it takes the problem of computing the integral directly (or via approximation) and replaces it with a problem of drawing samples from an arbitrary density function \\(f\\). Before we go further, we will take a brief diversion into random number generation. "],
["random-number-generation.html", "6.1 Random Number Generation", " 6.1 Random Number Generation In order to use the simulation-based techniques described in this book, we will need to be able to generate sequences of random numbers. Most of the time in the software we are using, there is a function that will do this for us. For example, in R, if we want to generate uniformly distributed random numbers over a fixed interval, we can use the runif() function. Nevertheless, there are two issues that are worth considering here: It is useful to know a little about what is going on under the hood of these random number generators. How exactly is the sequence of numbers created? Built-in functions in R are only useful for well-known or well-characterized distributions. However, wil many simulation-based techniques, we will want to generate random numbers from distributions that we have likely never seen before and for which there will not be any built-in function. 6.1.1 Pseudo-random Numbers The truth is that R, along with most other analytics packages, does not generate genuine random numbers. R generates pseudo-random numbers that appear to be random but are actually generated in a deterministic way. This approach sounds worse, but it’s actually better for two reasons. First, generating genuine random numbers can be slow and often will depend on some outside source of entropy/randomness. Second, genuine random numbers are not reproducible, so if you wanted to re-create some results based on simulations, you would not ever be able to do so. Pseudo-random number generators (PRNGs have a long history that we will not cover here. One useful thing to know is that this is tricky area and it is not simple to wander in and start developing your own PRNGs. It is useful to know how the systems work, but after that it’s best to leave the specifics to the experts. The most commonly used class of PRNGs in scientific applications is the linear congruential generator. The basic idea behind an LCG is that we have a starting seed, and then from there we generate pseudo-random numbers via a recurrence relation. Most LCGs have the form \\[ X_{n+1} = (aX_n + c)~\\text{mod}~m \\] where \\(a\\) is called the multiplier, \\(c\\) is the increment, and \\(m\\) is the modulus. For \\(n=0\\), the value \\(X_0\\) is the seed. Modular arithmetic is needed in order to prevent the sequence from going off to infinity. For most generators, the values \\(X_0,X_1,\\dots\\) are integers. However, we could for example generate Uniform(0, 1) variates by taking \\(U_n = X_n / m\\). Given the recurrence relation above and the modular arithmetic, the maximum number of distinct values that can be generated by an LCG is \\(m\\), so we would need \\(m\\) to be very large. The hope is that if the PRNG is well-designed, the sequence should hit every number from \\(0\\) to \\(m-1\\) before repeating. If a number is repeated before all numbers are seen, then the generator has a period in it that is shorter than the maximal period that is possible. Setting the values of \\(a\\), \\(c\\), and \\(m\\) is a tricky business and can require some experimentation. In summary, don’t do this at home. As an (historical) example, the random number generator proposed by the book Numerical Recipes specified that \\(a = 1664525\\), \\(c = 1013904223\\), and \\(m = 2^{32}\\). Perhaps the biggest problem with using the historical LCGs for generating random numbers is that their periods are too short, even if they manage to hit the maximal period. Given the scale of simulations being conducted today, even a period of \\(2^{32}\\) would likely be too short to appear sufficiently random. Most analytical software systems have since moved on to other more sophisticated generators. For example, the default in R is the Mersenne-Twister, which has a long period of \\(2^{19937} − 1\\). Further notes: The randomness of a pseudo-random sequence can be checked via statistical tests of uniformity, such as the Kolmogorov-Smirnoff test, Chi-square test, or the Marsaglia “die hard” tests. Many PRNGs generate sequences that look random in one dimension but do not look random when embedded into higher dimensions. It is possible for PRNGs to generate numbers that lie on a higher-dimensional hyperplane that still look random in one dimension. "],
["non-uniform-random-numbers.html", "6.2 Non-Uniform Random Numbers", " 6.2 Non-Uniform Random Numbers Uniform random numbers are useful, but usually we want to generate random numbers from some non-uniform distribution. There are a few ways to do this depending on the distribution. 6.2.1 Inverse CDF Transformation The most generic method (but not necessarily the simplest) uses the inverse of the cumulative distribution function of the distribution. Suppose we wnat to draw samples from a distribution with density \\(f\\) and cumulative distribution function \\(F(x) = \\int_{-\\infty}^x f(t)\\,dt\\). Then we can do the following: Draw \\(U\\sim \\text{Unif}(0, 1)\\) using any suitable PRNG. Let \\(X = F^{-1}(U)\\). Then \\(X\\) is distributed according to \\(f\\). Of course this method requires the inversion of the CDF, which is usually not possible. However, it works well for the Exponential(\\(\\lambda\\)) distribution. Here, we have that \\[ f(x) = \\frac{1}{\\lambda}e^{-x/lambda} \\] and \\[ F(x) = 1-e^{-x/\\lambda}. \\] Therefore, the inverse of the CDF is \\[ F^{-1}(u) = -\\lambda\\log(1-u). \\] First we can draw our uniform random variables. set.seed(2017-12-4) u &lt;- runif(100) hist(u) rug(u) Then we can apply the inverse CDF. lambda &lt;- 2 ## Exponential with mean 2 x &lt;- -lambda * log(1 - u) hist(x) rug(x) The problem with this method is that inverting the CDF is usually a difficult process and so other methods will be needed to generate other random variables. 6.2.2 Other Transformations The inverse of the CDF is not the only function that we can use to transform uniform random variables into random variables with other distributions. Here are some common transformations. To generate Normal random variables, we can Generate \\(U_1, U_2\\sim\\text{Unif}(0, 1)\\) using a standard PRNG. Let \\[\\begin{eqnarray*} Z_1 &amp; = &amp; \\sqrt{-2\\log U_1}\\cos(2\\pi\\, U_2)\\\\ Z_2 &amp; = &amp; \\sqrt{-2\\log U_1}\\sin(2\\pi\\, U_2) \\end{eqnarray*}\\] Then \\(Z_1\\) and \\(Z_2\\) are distributed independent \\(N(0, 1)\\). What about multivariate Normal random variables with arbitrary covariance structure? This can be done by applying an affine transformation to independent Normals. If we want to generate \\(X\\sim\\mathcal{N}(\\mu, \\Sigma)\\), we can Generate \\(Z\\sim\\mathcal{N}(0, I)\\) where \\(I\\) is the identity matrix; Let \\(\\Sigma = LL^\\prime\\) be the Cholesky decomposition of \\(\\Sigma\\). Let \\(X = \\mu + Lz\\). Then \\(X\\sim\\mathcal{N}(\\mu, \\Sigma)\\). In reality, you will not need to apply any of the transformations described above because almost any worthwhile analytical software system will have these generators built in, if not carved in stone. However, once in a while, it’s still nice to know how things work. "],
["rejection-sampling.html", "6.3 Rejection Sampling", " 6.3 Rejection Sampling What do we do if we want to generate samples of a random variable with density \\(f\\) and there isn’t a built in function for doing this? If the random variable is of a reasonably low dimension (less than \\(10\\)?), then rejection sampling is a plausible general approach. The idea of rejection sampling is that although we cannot easily sample from \\(f\\), there exists another density \\(g\\), like a Normal distribution or perhaps a \\(t\\)-distribution, from which it is easy for us to sample (because there’s a built in function or someone else wrote a nice function). Then we can sample from \\(g\\) directly and then “reject” the samples in a strategic way to make the resulting “non-rejected” samples look like they came from \\(f\\). The density \\(g\\) will be referred to as the “candidate density” and \\(f\\) will be the “target density”. In order to use the rejections sampling algorithm, we must first ensure that the support of \\(f\\) is a subset of the support of \\(g\\). If \\(\\mathcal{X}_f\\) is the support of \\(f\\) and \\(\\mathcal{X}_g\\) is the support of \\(g\\), then we must have \\(\\mathcal{X}_f\\subset\\mathcal{X}_g\\). This makes sense: if there’s a region of the support of \\(f\\) that \\(g\\) can never touch, then that area will never get sampled. In addition, we must assume that \\[ c = \\sup_{x\\in\\mathcal{X}_f} \\frac{f(x)}{g(x)} &lt; \\infty \\] and that we can calculate \\(c\\). The easiest way to satisfy this assumption is to make sure that \\(g\\) has heavier tails than \\(f\\). We cannot have that \\(g\\) decreases at a faster rate than \\(f\\) in the tails or else rejection sampling will not work. 6.3.1 The Algorithm The rejection sampling algorithm for drawing a sample from the target density \\(f\\) is then Simulate \\(U\\sim\\text{Unif}(0, 1)\\). Simulate a candidate \\(X\\sim g\\) from the candidate density If \\[ U\\leq\\frac{f(X)}{c\\,g(X)} \\] then “accept” the candidate \\(X\\). Otherwise, “reject” \\(X\\) and go back to the beginning. The algorithm can be repeated until the desired number of samples from the target density \\(f\\) has been accepted. As a simple example, suppose we wanted to generate samples from a \\(\\mathcal{N}(0, 1)\\) density. We could use the \\(t_2\\) distribution as our candidate density as it has heavier tails than the Normal. Plotting those two densities, along with a sample from the \\(t_2\\) density gives us the picture below. set.seed(2017-12-4) curve(dnorm(x), -6, 6, xlab = &quot;x&quot;, ylab = &quot;Density&quot;, n = 200) curve(dt(x, 2), -6, 6, add = TRUE, col = 4, n = 200) legend(&quot;topright&quot;, c(&quot;Normal density&quot;, &quot;t density&quot;), col = c(1, 4), bty = &quot;n&quot;, lty = 1) x &lt;- rt(200, 2) rug(x, col = 4) Given what we know about the standard Normal density, most of the samples should be between \\(-3\\) and \\(+3\\), except perhaps in very large samples (this is a sample of size 200). From the picture, there are samples in the range of \\(4\\)–\\(6\\). In order to transform the \\(t_2\\) samples into \\(\\mathcal{N}(0, 1)\\) samples, we will need to reject many of the samples out in the tail. On the other hand, there are two few samples in the range of \\([-2, 2]\\) and so we will have to disproportionaly accept samples in that range until it represents the proper \\(\\mathcal{N}(0, 1)\\) density. Before we move on, it’s worth noting that the rejection sampling method requires that we can evaluate the target density \\(f\\). That is how we compute the rejection/acceptance ratio in Step 2. In most cases, this will not be a problem. 6.3.2 Properties of Rejection Sampling One property of the rejection sampling algorithm is that the number of draws we need to take from the candidate density \\(g\\) before we accept a candidate is a geometric random variable with success probability \\(1 / c\\). We can think of the decision to accept or reject a candidate as a sequence of iid coin flips that has a specific probability of coming up “heads” (i.e. being accepted). That probability is \\(1/c\\) and we can calculate that as follows. \\[\\begin{eqnarray*} \\mathbb{P}(X~\\text{accepted}) &amp; = &amp; \\mathbb{P}\\left(U\\leq\\frac{f(X)}{c\\,g(X)}\\right)\\\\ &amp; = &amp; \\int \\mathbb{P}\\left(\\left.U\\leq\\frac{f(x)}{c\\,g(x)}\\right| X = x\\right)g(x)\\,dx\\\\ &amp; = &amp; \\int \\frac{f(x)}{c\\,g(x)} g(x)\\,dx\\\\ &amp; = &amp; \\frac{1}{c} \\end{eqnarray*}\\] This property of rejection sampling has implications for how we choose the candidate density \\(g\\). In theory, any density can be chosen as the candidate as long as its support includes the support of \\(f\\). However, in practice we will want to choose \\(g\\) so that it matches \\(f\\) as closely as possible. As a rule of thumb, candidates \\(g\\) that match \\(f\\) closely will have smaller values of \\(c\\) and thus will accept candidates with higher probability. We want to avoid large values of \\(c\\) because large values of \\(c\\) lead to an algorithm that rejects a lot of candidates and has lower efficiency. In the example above with the Normal distribution and the \\(t_2\\) distribution, the ratio \\(f(x)/g(x)\\) was maximized at \\(x=1\\) (or \\(x=-1\\)) and so the value of \\(c\\) for that setup was 1.257, which implies an acceptance probability of about 0.8. Suppose however, that we wanted to simulate from a Uniform\\((0,1)\\) density and we used an Exponential\\((1)\\) as our candidate density. The plot of the two densities looks as follows. curve(dexp(x, 1), 0, 1, col = 4, ylab = &quot;Density&quot;) segments(0, 1, 1, 1) legend(&quot;bottomleft&quot;, c(&quot;f(x) Uniform&quot;, &quot;g(x) Exponential&quot;), lty = 1, col = c(1, 4), bty = &quot;n&quot;) Here, the ratio of \\(f(x)/g(x)\\) is maximized at \\(x=1\\) and so the value of \\(c\\) is 2.718 which implies an acceptance probablity of about 0.37. While running the rejection sampling algorithm in this way to produce Uniform random variables will still work, it will be very inefficient. We can now show that the distribution of the accepted values from the rejection sampling algorithm above follows the target density \\(f\\). We can do this by calculating the distribution function of the accepted values and show that this is equal to \\(F(t) = \\int_{-\\infty}^t f(x)\\, dx\\). \\[\\begin{eqnarray*} \\mathbb{P}(X\\leq t\\mid X~\\text{accepted}) &amp; = &amp; \\frac{\\mathbb{P}(X\\leq t, X~\\text{accepted})}{\\mathbb{P}(X~\\text{accepted})}\\\\ &amp; = &amp; \\frac{\\mathbb{P}(X\\leq t, X~\\text{accepted})}{1/c}\\\\ &amp; = &amp; c\\, \\mathbb{E}_g \\mathbb{E}\\left[ \\left.\\mathbf{1}\\{x\\leq t\\}\\mathbf{1}\\left\\{U\\leq\\frac{f(x)}{c\\,g(x)}\\right\\}\\right| X=x \\right]\\\\ &amp; = &amp; c\\, \\mathbb{E}_g\\left[\\mathbf{1}\\{X\\leq t\\} \\mathbb{E}\\left[ \\left.\\mathbf{1}\\left\\{U\\leq\\frac{f(x)}{c\\,g(x)}\\right\\}\\right| X=x \\right]\\right]\\\\ &amp; = &amp; c\\, \\mathbb{E}_g\\left[ \\mathbf{1}\\{X\\leq t\\}\\frac{f(X)}{c\\,g(X)} \\right]\\\\ &amp; = &amp; \\int_{-\\infty}^{\\infty} \\mathbf{1}\\{x\\leq t\\} \\frac{f(x)}{g(x)}g(x)\\,dx\\\\ &amp; = &amp; \\int_{-\\infty}^t f(x)\\,dx\\\\ &amp; = &amp; F(t) \\end{eqnarray*}\\] This shows that the distribution function of the candidate values, given that they are accepted, is equal to the distribution function corresponding to the target density. A few further notes: We only need to know \\(f\\) and \\(g\\) up to a constant of proportionality. In many applications we will not know the normalizing constant for these densities, but we do not need them. That is, if \\(f(x) = k_1 f^\\star(x)\\) and \\(g(x) = k_2 g^\\star(x)\\), we can proceed with the algorithm using \\(f^\\star\\) and \\(g^\\star\\) even if we do not know the values of \\(k_1\\) and \\(k_2\\). Any number \\(c^\\prime \\geq c\\) will work in the rejection sampling algorithm, but the algorithm will be less efficient. Throughout the algorithm, operations can (and should!) be done on a log scale. The higher the dimension of \\(f\\) and \\(g\\), the less efficient the rejection sampling algorithm will be. Whether \\(c=\\infty\\) or not depends on the tail behavior of the the densities \\(f\\) and \\(g\\). If \\(g(x)\\downarrow 0\\) faster than \\(f(x)\\downarrow 0\\) as \\(x\\rightarrow\\infty\\), then \\(f(x)/g(x)\\uparrow\\infty\\). 6.3.3 Empirical Supremum Rejection Sampling What if we cannot calculate \\(c = \\sup_{x\\in\\mathcal{X}_f} \\frac{f(x)}{g(x)}\\) or are simply too lazy to do so? Fear not, because it turns out we almost never have to do so. A slight modification of the standard rejection sampling algorithm will allow us to estimate \\(c\\) while also sampling from the target density \\(f\\). The tradeoff (there is always a tradeoff!) is that we must make a more stringent assumption about \\(c\\), mainly that it is achievable. That is, there exists some value \\(x_c\\in\\mathcal{X}_f\\) such that \\(\\frac{f(x_c)}{g(x_c)}\\) is equal to \\(\\sup_{x\\in\\mathcal{X}_f} \\frac{f(x)}{g(x)}\\). The modified algorithm is the empirical supremum rejection sampling algorithm of Caffo, Booth, and Davison. The algorithm goes as follows. First we must choose some starting value of \\(c\\), call it \\(\\hat{c}\\), such that \\(\\hat{c}&gt; 1\\). Then, Draw \\(U\\sim\\text{Unif}(0, 1)\\). Draw \\(X\\sim g\\), the candidate density. Accept \\(X\\) if \\(U\\leq\\frac{f(X)}{\\hat{c}\\,g(X)}\\), otherwise reject \\(X\\). Let \\(\\hat{c}^\\star = \\max\\left\\{\\hat{c}, \\frac{f(X)}{g(X)}\\right\\}\\). Update \\(\\hat{c} = \\hat{c}^\\star\\). Goto Step 1. From the algorithm we can see that at each iteration, we get more information about the ratio \\(f(X)/g(X)\\) and can update our estimate of \\(c\\) accordingly. One way to think of this algorithm is to conceptualize a separate sequence \\(\\tilde{Y}_i\\), which is \\(0\\) or \\(1\\) depending on whether \\(X_i\\) should be rejected (\\(0\\)) or accepted (\\(1\\)). This sequence \\(\\tilde{Y}_i\\) is the accept/reject determination sequence. Under the standard rejection sampling algorithm, the sequence \\(\\tilde{Y}_i\\) is generated using the true value of \\(c\\). Under the emprical supremum rejection sampling (ESUP) scheme, we generate a slightly different sequence \\(Y_i\\) using our continuously updated value of \\(\\hat{c}\\). If we drew values \\(X_1, X_2, X_3, X_4, X_5, X_6,\\dots\\) from the candidate density \\(g\\), then we could visualize the acceptance/rejection process as it might occur using the true value of \\(c\\) and our estimate \\(\\hat{c}\\). Empirical supremum rejection sampling scheme. Following the diagram above, we can see that using the estimate \\(\\hat{c}\\), there are two instances where we accept a value when we should have rejected it (\\(X_1\\) and \\(X_4\\)). In every other instance in the sequence, the value of \\(Y_i\\) was equal to \\(\\tilde{Y}_i\\). The theory behind the ESUP algorithm is that eventually, the sequence \\(Y_i\\) becomes identical to the sequence \\(\\tilde{Y}_i\\) and therefore we will accept/reject candidates in the same manner as we would have if we had used the true \\(c\\). If \\(f\\) and \\(g\\) are discrete distributions, then the proof of the ESUP algorithm is fairly straightforward. Specifically, Caffo, Booth, and Davison showed that \\(\\mathbb{P}(Y_i \\ne \\tilde{Y}_i~\\text{infinitely often}) = 0\\). Recall that by assumption, there exists some \\(x_c\\in\\mathcal{X}_f\\) such that \\(c = \\frac{f(x_c)}{g(x_c)}\\). Therefore, as we independently sample candidates from \\(g\\), at some point, we will sample the value \\(x_c\\), in which case we will achieve the value \\(c\\). Once that happens, we are then using the standard rejection sampling algorithm and our estimate \\(\\hat{c}\\) never changes. Let \\(\\gamma = \\min_i\\{ x_i = x_c\\}\\), where \\(x_i\\sim g\\). So \\(\\gamma\\) is the first time that we see the value \\(x_c\\) as we are sampling candidates \\(x_i\\) from \\(g\\). The probability that we sample \\(x_c\\) is \\(g(x_c)\\) (recall that \\(g\\) is assumed to be discrete here) and so \\(\\gamma\\) has a Geometric distribution with success probability \\(g(x_c)\\). Once we observe \\(x_c\\), the ESUP algorithm and the standard rejection sampling algorithms converge and are identical. From here, we can use the coupling inequality, which tells us that \\[ \\mathbb{P}(Y_i\\ne\\tilde{Y}_i) \\leq \\mathbb{P}(\\gamma\\geq i). \\] Given that \\(\\gamma\\sim\\text{Geometric}(g(x_c))\\), we know that \\[ \\mathbb{P}(\\gamma\\geq i) = (1-g(x_c))^{i-1}. \\] This then implies that \\[ \\sum_{i=1}^\\infty \\mathbb{P}(Y_i\\ne\\tilde{Y}_i) &lt; \\infty \\] which, by the Borel-Cantelli lemma, implies that \\(\\mathbb{P}(Y_i \\ne \\tilde{Y}_i~\\text{infinitely often}) = 0\\). Therefore, eventually the sequences \\(Y_i\\) and \\(\\tilde{Y}_i\\) must converge and at that point the ESUP algorithm will be identical to the rejection sampling algorithm. In practice, we will know know exactly when the ESUP algorithm has converged to the standard rejection sampling algorithm. However, Caffo, and Davison report that the convergence is generally fast. Therefore, a reasonable approach might be to discard the first several accepted values (e.g. a “burn in”) and then use the remaining values. We can see how quickly ESUP converges in a simple example where the target density is the standard Normal and the candidate density is the \\(t_2\\) distribution. Here we simulate 1,000 draws and start with a value \\(\\hat{c}=1.0001\\). Note that in the code below, all of the computations are done on the log scale for the sake of numerical stability. set.seed(2017-12-04) N &lt;- 500 y_tilde &lt;- numeric(N) ## Binary accept/reject for &quot;true&quot; algorithm y &lt;- numeric(N) ## Binary accept/reject for ESUP log_c_true &lt;- dnorm(1, log = TRUE) - dt(1, 2, log = TRUE) log_chat &lt;- numeric(N + 1) log_chat[1] &lt;- log(1.0001) ## Starting c value for(i in seq_len(N)) { u &lt;- runif(1) x &lt;- rt(1, 2) r_true &lt;- dnorm(x, log = TRUE) - dt(x, 2, log = TRUE) - log_c_true rhat &lt;- dnorm(x, log = TRUE) - dt(x, 2, log = TRUE) - log_chat[i] y_tilde[i] &lt;- log(u) &lt;= r_true y[i] &lt;- log(u) &lt;= rhat log_chat[i+1] &lt;- max(log_chat[i], dnorm(x, log = TRUE) - dt(x, 2, log = TRUE)) } Now we can plot \\(\\log_{10}(|\\hat{c}-c|)\\) for each iteration to see how the magnitude of the error changes with each iteration. c_true &lt;- exp(log_c_true) chat &lt;- exp(log_chat) plot(log10(abs(chat - c_true)), type = &quot;l&quot;, xlab = &quot;Iteration&quot;, ylab = expression(paste(log[10], &quot;(Absolute Error)&quot;))) We can see that by iteration 40 or so, \\(\\hat{c}\\) and \\(c\\) differ only in the 5th decimal place and beyond. By the 380th iteration, they differ only beyond the 6th decimal place. "],
["importance-sampling.html", "6.4 Importance Sampling", " 6.4 Importance Sampling With rejection sampling, we ultimately obtain a sample from the target density \\(f\\). With that sample, we can create any number of summaries, statistics, or visualizations. However, what if we are interested in the more narrow problem of computing a mean, such as \\(\\mathbb{E}_f[h(X)]\\) for some function \\(h:\\mathbb{R}^k\\rightarrow\\mathbb{R}\\)? Clearly, this is a problem that can be solved with rejection sampling: First obtain a sample \\(x_1,\\dots,x_n\\sim f\\) and then compute \\[ \\hat{\\mu}_n = \\frac{1}{n}\\sum_{i=1}^n h(x_i). \\] with the obtained sample. As \\(n\\rightarrow\\infty\\) we know by the Law of Large Numbers that \\(\\hat{\\mu}_n\\rightarrow\\mathbb{E}_f[h(X)]\\). Further, the Central Limit Theorem gives us \\(\\sqrt{n}(\\hat{\\mu}_n-\\mathbb{E}_f[h(X)])\\longrightarrow\\mathcal{N}(0,\\sigma^2)\\). So far so good. However, with rejection sampling, in order to obtain a sample of size \\(n\\), we must generate, on average, \\(c\\times n\\) candidates from \\(g\\), the candidate density, and then reject about \\((c-1)\\times n\\) of them. If \\(c\\approx 1\\) then this will not be too inefficient. But in general, if \\(c\\) is much larger than \\(1\\) then we will be generating a lot of candidates from \\(g\\) and ultimately throwing most of them away. It’s worth noting that in most cases, the candidates generated from \\(g\\) fall within the domain of \\(f\\), so that they are in fact values that could plausibly come from \\(f\\). They are simply over- or under-represented in the frequency with which they appear. For example, if \\(g\\) has heavier tails than \\(f\\), then there will be too many extreme values generated from \\(g\\). Rejection sampling simply thins out those extreme values to obtain the right proportion. But what if we could take those rejected values and, instead of discarding them, simply downweight or upweight them in a specific way? Note that we can rewrite the target estimation as follows, \\[ \\mathbb{E}_f[h(X)] = \\mathbb{E}_g\\left[\\frac{f(X)}{g(X)}h(X)\\right]. \\] Hence, if \\(x_1,\\dots,x_n\\sim g\\), drawn from the candidate density, we can say \\[ \\tilde{\\mu}_n = \\frac{1}{n}\\sum_{i=1}^n\\frac{f(x_i)}{g(x_i)}h(x_i) = \\frac{1}{n}\\sum_{i=1}^n w_i h(x_i) \\approx \\mathbb{E}_f[h(X)] \\] In the equation above, the values \\(w_i=f(x_i)/g(x_i)\\) are referred to as the importance weights because they take each of the candidates \\(x_i\\) generated from \\(g\\) and reweight them when taking the average. Note that if \\(f = g\\), so that we are simply sampling from the target density, then this estimator is just the sample mean of the \\(h(x_i)\\)s. The estimator \\(\\tilde{\\mu}_n\\) is known as the importance sampling estimator. When comparing rejection sampling with importance sampling, we can see that Rejection sampling samples directly from \\(f\\) and then uses the samples to compute a simple mean Importance sampling samples from \\(g\\) and then reweights those samples by \\(f(x)/g(x)\\) For estimating expectations, one might reasonably believe that the importance sampling approach is more efficient than the rejection sampling approach because it does not discard any data. In fact, we can see this by writing the rejection sampling estimator of the expectation in a different way. Let \\(c=\\sup_{x\\in\\mathcal{X}_f}f(x)/g(x)\\). Given a sample \\(x_1,\\dots,x_n\\sim g\\) and \\(u_1,\\dots,u_n\\sim\\text{Unif}(0,1)\\), then \\[ \\hat{\\mu}_n = \\frac{ \\sum_i\\mathbf{1}\\left\\{u_i\\leq\\frac{f(x_i)}{c\\,g(x_i)}\\right\\}h(x_i) }{ \\sum_i\\mathbf{1}\\left\\{u_i\\leq\\frac{f(x_i)}{c\\,g(x_i)}\\right\\} } \\] What importance sampling does, effectively, is replace the indicator functions in the above expression with their expectation. So instead of having a hard threshold, where observation \\(x_i\\) is either included (accepted) or not (rejected), importance sampling smooths out the acceptance/rejection process so that every observation plays some role. If we take the expectation of the indicator functions above, we get (note that the \\(c\\)s cancel) \\[ \\tilde{\\mu}_n = \\frac{ \\sum_i \\frac{f(x_i)}{g(x_i)}h(x_i) }{ \\sum_i \\frac{f(x_i)}{g(x_i)} } = \\frac{ \\frac{1}{n}\\sum_i \\frac{f(x_i)}{g(x_i)}h(x_i) }{ \\frac{1}{n}\\sum_i \\frac{f(x_i)}{g(x_i)} } \\] which is roughly equivalent to the importance sampling estimate if we take into account that \\[ \\frac{1}{n}\\sum_{i=1}^n\\frac{f(x_i)}{g(x_i)}\\approx 1 \\] because \\[ \\mathbb{E}_g\\left[\\frac{f(X)}{g(X)}\\right] = \\int \\frac{f(x)}{g(x)}g(x)\\,dx = 1 \\] The point of all this is to show that the importance sampling estimator of the mean can be seen as a “smoothed out” version of the rejection sampling estimator. The advantage of the importance sampling estimator is that it does not discard any data and thus is more efficient. Note that we do not need to know the normalizing constants for the target density or the candidate density. If \\(f^\\star\\) and \\(g^\\star\\) are the unnormalized target and candidate densities, respectively, then we can use the modified importance sampling estimator, \\[ \\mu^\\star_n = \\frac{ \\sum_i\\frac{f^\\star(x_i)}{g^\\star(x_i)}h(x_i) }{ \\sum_i\\frac{f^\\star(x_i)}{g^\\star(x_i)} }. \\] We can then use Slutsky’s Theorem to say that \\(\\mu^\\star_n\\rightarrow\\mathbb{E}_f[h(X)]\\). 6.4.1 Example: Bayesian Sensitivity Analysis An interesting application of importance sampling is the examination of the sensitivity of posterior inferences with respect to prior specification. Suppose we observe data \\(y\\) with density \\(f(y\\mid\\theta)\\) and we specify a prior for \\(\\theta\\) as \\(\\pi(\\theta\\mid\\psi_0)\\), where \\(\\psi_0\\) is a hyperparameter. The posterior for \\(\\theta\\) is thus \\[ p(\\theta\\mid y, \\psi_0) \\propto f(y\\mid\\theta)\\pi(\\theta\\mid\\psi_0) \\] and we would like to compute the posterior mean of \\(\\theta\\). If we can draw \\(\\theta_1,\\dots,\\theta_n\\), a sample of size \\(n\\) from \\(p(\\theta\\mid y,\\psi_0)\\), then we can estimate the posterior mean with \\(\\frac{1}{n}\\sum_i\\theta_i\\). However, this posterior mean is estimated using a specific hyperparameter \\(\\psi_0\\). What if we would like to see what the posterior mean would be for a different value of \\(\\psi\\)? Do we need to draw a new sample of size \\(n\\)? Thankfully, the answer is no. We can simply take our existing sample \\(\\theta_1,\\dots,\\theta_n\\) and reweight it to get our new posterior mean under a different value of \\(\\psi\\). Given a sample \\(\\theta_1,\\dots,\\theta_n\\) drawn from \\(p(\\theta\\mid y,\\psi_0)\\), we would like to know \\(\\mathbb{E}[\\theta\\mid y, \\psi]\\) for some \\(\\psi\\ne\\psi_0\\). The idea is to treat our original \\(p(\\theta\\mid y,\\psi_0)\\) as a “candidate density” from which we have already drawn a large sample \\(\\theta_1,\\dots,\\theta_n\\). Then we want know the posterior mean of \\(\\theta\\) under a “target density” \\(p(\\theta\\mid y,\\psi)\\). We can then write our importance sampling estimator as \\[\\begin{eqnarray*} \\frac{ \\sum_i\\theta_i\\frac{p(\\theta_i\\mid y, \\psi)}{p(\\theta_i\\mid y,\\psi_0)} }{ \\sum_i\\frac{p(\\theta_i\\mid y, \\psi)}{p(\\theta_i\\mid y,\\psi_0)} } &amp; = &amp; \\frac{ \\sum_i\\theta_i\\frac{f(y\\mid\\theta_i)\\pi(\\theta_i\\mid\\psi)}{f(y\\mid\\theta_i)\\pi(\\theta_i\\mid\\psi_0)} }{ \\sum_i\\frac{f(y\\mid\\theta_i)\\pi(\\theta_i\\mid\\psi)}{f(y\\mid\\theta_i)\\pi(\\theta_i\\mid\\psi_0)} }\\\\ &amp; = &amp; \\frac{ \\sum_i\\theta_i\\frac{\\pi(\\theta_i\\mid\\psi)}{\\pi(\\theta_i\\mid\\psi_0)} }{ \\sum_i\\frac{\\pi(\\theta_i\\mid\\psi)}{\\pi(\\theta_i\\mid\\psi_0)} }\\\\ &amp; \\approx &amp; \\mathbb{E}[\\theta\\mid y,\\psi] \\end{eqnarray*}\\] In this case, the importance sampling weights are simply the ratio of the prior under \\(\\psi\\) to the prior under \\(\\psi_0\\). 6.4.2 Example: Calculating Marginal Likelihoods 6.4.3 Properties of the Importance Sampling Estimator So far we’ve talked about how to estimate an expectation with respect to an arbitrary target density \\(f\\) using importance sampling. However, we haven’t discussed yet what is the variance of that estimator. An analysis of the variance of the importance sampling estimator is assisted by the Delta method and by viewing the importance sampling estimator as a ratio estimator. Recall that the Delta method states that if \\(Y_n\\) is a \\(k\\)-dimensional random variable with mean \\(\\mu\\), \\(g:\\mathbb{R}^k\\rightarrow\\mathbb{R}\\) and is differentiable, and further we have \\[ \\sqrt{n}(Y_n-\\mu)\\stackrel{D}{\\longrightarrow}\\mathcal{N}(0,\\Sigma) \\] as \\(n\\rightarrow\\infty\\), then \\[ \\sqrt{n}(g(Y_n)-g(\\mu)) \\stackrel{D}{\\longrightarrow} \\mathcal{N}(0, g^\\prime(\\mu)^\\prime\\Sigma g^\\prime(\\mu)) \\] as \\(n\\rightarrow\\infty\\). For the importance sampling estimator, we have \\(f\\) is the target density, \\(g\\) is the candidate density, and \\(x_1,\\dots,x_n\\) are samples from \\(g\\). The estimator of \\(\\mathbb{E}_f[h(X)]\\) is written as \\[ \\frac{\\frac{1}{n}\\sum_i h(x_i) w(x_i)}{\\frac{1}{n}\\sum_i w(x_i)} \\] where \\[ w(x_i) = \\frac{f(x_i)}{g(x_i)} \\] are the importance sampling weights. If we let \\(g((a, b)) = a/b\\), then \\(g^\\prime((a,b)) = (1/b, -a/b^2)\\). If we define the vector \\(Y_n = \\left(\\frac{1}{n}\\sum h(x_i) w_i,\\,\\frac{1}{n}\\sum w_i\\right)\\) then the importance sampling estimator is simply \\(g(Y_n)\\). Furthremore, we have \\[ \\mathbb{E}_g[Y_n] = \\mathbb{E}_g\\left[\\left(\\frac{1}{n}\\sum h(x_i) w(x_i),\\,\\frac{1}{n}\\sum w(x_i)\\right)\\right] = (\\mathbb{E}_f[h(X)], 1) = \\mu \\] and \\[ \\Sigma = n\\,\\text{Var}(Y_n) = \\left( \\begin{array}{cc} \\text{Var}(h(X)w(X)) &amp; \\text{Cov}(h(X)w(X), w(X))\\\\ \\text{Cov}(h(X)w(X), w(X)) &amp; \\text{Var}(w(X)) \\end{array} \\right) \\] Note that the above quantity can be estimated consistently using the sample versions of each quantity in the matrix. Therefore, the variance of the importance sampling estimator of \\(\\mathbb{E}_f[h(X)]\\) is \\(g^\\prime(Y_n)^\\prime\\Sigma g^\\prime(Y_n)\\) which we can expand to \\[ n\\left( \\frac{\\sum h(x_i)w(x_i)}{\\sum w(x_i)} \\right)^2 \\left( \\frac{\\sum h(x_i)^2w(x_i)^2}{\\left(\\sum h(x_i)w(x_i)\\right)^2} - 2\\frac{\\sum h(x_i)w(x_i)^2}{\\left(\\sum h(x_i)w(x_i)\\right)\\left(\\sum w(x_i)\\right)} + \\frac{\\sum w(x_i)^2}{\\left(\\sum w(x_i)\\right)^2} \\right) \\] Given this, for the importance sampling estimator, we need the following to be true, \\[ \\mathbb{E}_g\\left[h(X)^2w(X)^2\\right] = \\mathbb{E}_g\\left[h(X)\\frac{f(X)}{g(X)}\\right] &lt; \\infty, \\] \\[ \\mathbb{E}_g[w(X)^2] = \\mathbb{E}_g\\left[\\left(\\frac{f(X)}{g(X)}\\right)^2\\right] &lt; \\infty, \\] and \\[ \\mathbb{E}_g\\left[h(X)w(X)^2\\right] = \\mathbb{E}_g\\left[h(X)\\left(\\frac{f(X)}{g(X)}\\right)^2\\right] &lt; \\infty. \\] All of the above conditions are true if the conditions for rejection sampling are satisfied, that is, if \\(\\sup_{x\\in\\mathcal{X}_f}\\frac{f(x)}{g(x)}&lt;\\infty\\). "],
["markov-chain-monte-carlo.html", "7 Markov Chain Monte Carlo", " 7 Markov Chain Monte Carlo "]
]
